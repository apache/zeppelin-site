{




    "/development/contribution/how_to_contribute_code.html": {
      "title": "Contributing to Apache Zeppelin (Code)",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Contributing to Apache Zeppelin ( Code )NOTE : Apache Zeppelin is an Apache2 License Software.Any contributions to Zeppelin (Source code, Documents, Image, Website) means you agree with license all your contributions as Apache2 License.Setting upHere are some tools you will need to build and test Zeppelin.Software Configuration Management ( SCM )Since Zeppelin uses Git for it&amp;#39;s SCM system, you need git client installed in your development machine.Integrated Development Environment ( IDE )You are free to use whatever IDE you prefer, or your favorite command line editor.Build ToolsTo build the code, installOracle Java 8Apache MavenGetting the source codeFirst of all, you need Zeppelin source code. The official location of Zeppelin is https://gitbox.apache.org/repos/asf/zeppelin.git.git accessGet the source code on your development machine using git.git clone git://gitbox.apache.org/repos/asf/zeppelin.git zeppelinYou may also want to develop against a specific branch. For example, for branch-0.5.6git clone -b branch-0.5.6 git://gitbox.apache.org/repos/asf/zeppelin.git zeppelinApache Zeppelin follows Fork &amp;amp; Pull as a source control workflow.If you want to not only build Zeppelin but also make any changes, then you need to fork Zeppelin github mirror repository and make a pull request.Before making a pull request, please take a look Contribution Guidelines.Buildmvn installTo skip testmvn install -DskipTestsTo build with specific spark / hadoop versionmvn install -Dspark.version=x.x.x -Dhadoop.version=x.x.xFor the further Run Zeppelin server in development modeOption 1 - Command LineCopy the conf/zeppelin-site.xml.template to zeppelin-server/src/main/resources/zeppelin-site.xml and change the configurations in this file if requiredRun the following commandcd zeppelin-serverHADOOP_HOME=YOUR_HADOOP_HOME JAVA_HOME=YOUR_JAVA_HOME mvn exec:java -Dexec.mainClass=&amp;quot;org.apache.zeppelin.server.ZeppelinServer&amp;quot; -Dexec.args=&amp;quot;&amp;quot;Option 2 - Daemon ScriptNote: Make sure you first run mvn clean install -DskipTestsin your zeppelin root directory, otherwise your server build will fail to find the required dependencies in the local repro.or use daemon scriptbin/zeppelin-daemon startServer will be run on http://localhost:8080.Option 3 - IDECopy the conf/zeppelin-site.xml.template to zeppelin-server/src/main/resources/zeppelin-site.xml and change the configurations in this file if requiredZeppelinServer.java Main classGenerating Thrift CodeSome portions of the Zeppelin code are generated by Thrift. For most Zeppelin changes, you don&amp;#39;t need to worry about this. But if you modify any of the Thrift IDL files (e.g. zeppelin-interpreter/src/main/thrift/*.thrift), then you also need to regenerate these files and submit their updated version as part of your patch.To regenerate the code, install thrift-0.9.2 and then run the following command to generate thrift code.cd &amp;lt;zeppelin_home&amp;gt;/zeppelin-interpreter/src/main/thrift./genthrift.shRun Selenium testZeppelin has set of integration tests using Selenium. To run these test, first build and run Zeppelin and make sure Zeppelin is running on port 8080. Then you can run test using following commandTEST_SELENIUM=true mvn test -Dtest=[TEST_NAME] -DfailIfNoTests=false -pl &amp;#39;zeppelin-interpreter,zeppelin-zengine,zeppelin-server&amp;#39;For example, to run ParagraphActionIT,TEST_SELENIUM=true mvn test -Dtest=ParagraphActionsIT -DfailIfNoTests=false -pl &amp;#39;zeppelin-interpreter,zeppelin-zengine,zeppelin-server&amp;#39;You&amp;#39;ll need Firefox web browser installed in your development environment.Where to StartYou can find issues for beginner &amp;amp; newbieStay involvedContributors should join the Zeppelin mailing lists.dev@zeppelin.apache.org is for people who want to contribute code to Zeppelin. subscribe, unsubscribe, archivesIf you have any issues, create a ticket in JIRA.",
      "url": " /development/contribution/how_to_contribute_code.html",
      "group": "development/contribution",
      "excerpt": "How can you contribute to Apache Zeppelin project? This document covers from setting up your develop environment to making a pull request on Github."
    }
    ,



    "/development/contribution/how_to_contribute_website.html": {
      "title": "Contributing to Apache Zeppelin (Website)",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Contributing to Apache Zeppelin ( Website )This page will give you an overview of how to build and contribute to the documentation of Apache Zeppelin.The online documentation at zeppelin.apache.org is also generated from the files found here.NOTE : Apache Zeppelin is an Apache2 License Software.Any contributions to Zeppelin (Source code, Documents, Image, Website) means you agree with license all your contributions as Apache2 License.Getting the source codeFirst of all, you need Zeppelin source code. The official location of Zeppelin is https://gitbox.apache.org/repos/asf/zeppelin.git.Documentation website is hosted in &amp;#39;master&amp;#39; branch under /docs/ dir.git accessFirst of all, you need the website source code. The official location of mirror for Zeppelin is https://gitbox.apache.org/repos/asf/zeppelin.git.Get the source code on your development machine using git.git clone git://gitbox.apache.org/repos/asf/zeppelin.gitcd docsApache Zeppelin follows Fork &amp;amp; Pull as a source control workflow.If you want to not only build Zeppelin but also make any changes, then you need to fork Zeppelin github mirror repository and make a pull request.BuildYou&amp;#39;ll need to install some prerequisites to build the code. Please check Build documentation section in docs/README.md.Run website in development modeWhile you&amp;#39;re modifying website, you might want to see preview of it. Please check Run website section in docs/README.md.Then you&amp;#39;ll be able to access it on http://localhost:4000 with your web browser.Making a Pull RequestWhen you are ready, just make a pull-request.Alternative wayYou can directly edit .md files in /docs/ directory at the web interface of github and make pull-request immediately.Stay involvedContributors should join the Zeppelin mailing lists.dev@zeppelin.apache.org is for people who want to contribute code to Zeppelin. subscribe, unsubscribe, archivesIf you have any issues, create a ticket in JIRA.",
      "url": " /development/contribution/how_to_contribute_website.html",
      "group": "development/contribution",
      "excerpt": "How can you contribute to Apache Zeppelin project website? This document covers from building Zeppelin documentation site to making a pull request on Github."
    }
    ,



    "/development/contribution/useful_developer_tools.html": {
      "title": "Useful Developer Tools",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Useful Developer ToolsDeveloping zeppelin-webCheck zeppelin-web: Local Development.ToolsSVM: Scala Version Managersvm would be useful when changing scala version frequently.JDK change script: OSXthis script would be helpful when changing JDK version frequently.function setjdk() {  if [ $# -ne 0 ]; then  # written based on OSX.   # use diffrent base path for other OS  removeFromPath &amp;#39;/System/Library/Frameworks/JavaVM.framework/Home/bin&amp;#39;  if [ -n &amp;quot;${JAVA_HOME+x}&amp;quot; ]; then    removeFromPath $JAVA_HOME  fi  export JAVA_HOME=`/usr/libexec/java_home -v $@`  export PATH=$JAVA_HOME/bin:$PATH  fi}function removeFromPath() {  export PATH=$(echo $PATH | sed -E -e &amp;quot;s;:$1;;&amp;quot; -e &amp;quot;s;$1:?;;&amp;quot;)}you can use this function like setjdk 1.8 / setjdk 1.7Building Submodules Selectively# build `zeppelin-web` onlymvn clean -pl &amp;#39;zeppelin-web&amp;#39; package -DskipTests;# build `zeppelin-server` and its dependencies onlymvn clean package -pl &amp;#39;spark,spark-dependencies,python,markdown,zeppelin-server&amp;#39; --am -DskipTests# build spark related modules with default profiles: scala 2.10 mvn clean package -pl &amp;#39;spark,spark-dependencies,zeppelin-server&amp;#39; --am -DskipTests# build spark related modules with profiles: scala 2.11, spark 2.1 hadoop 2.7 ./dev/change_scala_version.sh 2.11mvn clean package -Pspark-2.1 -Phadoop-2.7 -Pscala-2.11 -pl &amp;#39;spark,spark-dependencies,zeppelin-server&amp;#39; --am -DskipTests# build `zeppelin-server` and `markdown` with dependenciesmvn clean package -pl &amp;#39;markdown,zeppelin-server&amp;#39; --am -DskipTestsRunning Individual Tests# run the `HeliumBundleFactoryTest` test classmvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=HeliumBundleFactoryTestRunning Selenium TestsMake sure that Zeppelin instance is started to execute integration tests (= selenium tests).# run the `SparkParagraphIT` test classTEST_SELENIUM=&amp;quot;true&amp;quot; mvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=SparkParagraphIT# run the `testSqlSpark` test function only in the `SparkParagraphIT` class# but note that, some test might be dependent on the previous testsTEST_SELENIUM=&amp;quot;true&amp;quot; mvn test -pl &amp;#39;zeppelin-server&amp;#39; --am -DfailIfNoTests=false -Dtest=SparkParagraphIT#testSqlSpark",
      "url": " /development/contribution/useful_developer_tools.html",
      "group": "development/contribution",
      "excerpt": ""
    }
    ,



    "/development/helium/overview.html": {
      "title": "Helium",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Helium OverviewWhat is Helium?Helium is a plugin system that can extend Zeppelin a lot. For example, you can write custom display system or install already published one in Helium Online Registry. Currently, Helium supports 4 types of package.Helium Visualization: Adding a new chart typeHelium Spell: Adding new interpreter, display system running on browserHelium Application Helium Interpreter: Adding a new custom interpreterConfigurationZeppelin ships with several builtin helium plugins which is located in $ZEPPELIN_HOME/heliums. If you want to try more types of heliums plugins,you can configure zeppelin.helium.registry to be helium,https://zeppelin.apache.org/helium.json in zeppelin-site.xml. https://zeppelin.apache.org/helium.json will be updated regularly.",
      "url": " /development/helium/overview.html",
      "group": "development/helium",
      "excerpt": ""
    }
    ,



    "/development/helium/writing_application.html": {
      "title": "Writing a new Helium Application",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Writing a new ApplicationWhat is Apache Zeppelin ApplicationApache Zeppelin Application is a package that runs on Interpreter process and displays it&amp;#39;s output inside of the notebook. While application runs on Interpreter process, it&amp;#39;s able to access resources provided by Interpreter through ResourcePool. Output is always rendered by AngularDisplaySystem. Therefore application provides all the possiblities of making interactive graphical application that uses data and processing power of any Interpreter.Make your own ApplicationWriting Application means extending org.apache.zeppelin.helium.Application. You can use your favorite IDE and language while Java class files are packaged into jar. Application class looks like/** * Constructor. Invoked when application is loaded */public Application(ApplicationContext context);/** * Invoked when there&amp;#39;re (possible) updates in required resource set. * i.e. invoked after application load and after paragraph finishes. */public abstract void run(ResourceSet args);/** * Invoked before application unload. * Application is automatically unloaded with paragraph/notebook removal */public abstract void unload();You can check example applications under ./zeppelin-examples directory.Development modeIn the development mode, you can run your Application in your IDE as a normal java application and see the result inside of Zeppelin notebook.org.apache.zeppelin.helium.ZeppelinApplicationDevServer can run Zeppelin Application in development mode.// entry point for development modepublic static void main(String[] args) throws Exception {  // add resources for development mode  LocalResourcePool pool = new LocalResourcePool(&amp;quot;dev&amp;quot;);  pool.put(&amp;quot;date&amp;quot;, new Date());  // run application in devlopment mode with given resource  // in this case, Clock.class.getName() will be the application class name    org.apache.zeppelin.helium.ZeppelinApplicationDevServer devServer = new org.apache.zeppelin.helium.ZeppelinApplicationDevServer(    Clock.class.getName(), pool.getAll());  // start development mode  devServer.start();  devServer.join();}In the Zeppelin notebook, run %dev run will connect to application running in development mode.Package filePackage file is a json file that provides information about the application.Json file contains the following information{  &amp;quot;name&amp;quot; : &amp;quot;[organization].[name]&amp;quot;,  &amp;quot;description&amp;quot; : &amp;quot;Description&amp;quot;,  &amp;quot;artifact&amp;quot; : &amp;quot;groupId:artifactId:version&amp;quot;,  &amp;quot;className&amp;quot; : &amp;quot;your.package.name.YourApplicationClass&amp;quot;,  &amp;quot;resources&amp;quot; : [    [&amp;quot;resource.name&amp;quot;, &amp;quot;:resource.class.name&amp;quot;],    [&amp;quot;alternative.resource.name&amp;quot;, &amp;quot;:alternative.class.name&amp;quot;]  ],  &amp;quot;icon&amp;quot; : &amp;quot;&amp;lt;i class=&amp;#39;icon&amp;#39;&amp;gt;&amp;lt;/i&amp;gt;&amp;quot;}nameName is a string in [group].[name] format.[group] and [name] allow only [A-Za-z0-9_].Group is normally the name of an organization who creates this application.descriptionA short description about the applicationartifactLocation of the jar artifact.&amp;quot;groupId:artifactId:version&amp;quot; will load artifact from maven repository.If jar exists in the local filesystem, absolute/relative can be used.e.g.When artifact exists in Maven repositoryartifact: &amp;quot;org.apache.zeppelin:zeppelin-examples:0.6.0&amp;quot;When artifact exists in the local filesystemartifact: &amp;quot;zeppelin-example/target/zeppelin-example-0.6.0.jar&amp;quot;classNameEntry point. Class that extends org.apache.zeppelin.helium.ApplicationresourcesTwo dimensional array that defines required resources by name or by className. Helium Application launcher will compare resources in the ResourcePool with the information in this field and suggest application only when all required resources are available in the ResourcePool.Resouce name is a string which will be compared with the name of objects in the ResourcePool. className is a string with &amp;quot;:&amp;quot; prepended, which will be compared with className of the objects in the ResourcePool.Application may require two or more resources. Required resources can be listed inside of the json array. For example, if the application requires object &amp;quot;name1&amp;quot;, &amp;quot;name2&amp;quot; and &amp;quot;className1&amp;quot; type of object to run, resources field can beresources: [  [ &amp;quot;name1&amp;quot;, &amp;quot;name2&amp;quot;, &amp;quot;:className1&amp;quot;, ...]]If Application can handle alternative combination of required resources, alternative set can be listed as below.resources: [  [ &amp;quot;name&amp;quot;, &amp;quot;:className&amp;quot;],  [ &amp;quot;altName&amp;quot;, &amp;quot;:altClassName1&amp;quot;],  ...]Easier way to understand this scheme isresources: [   [ &amp;#39;resource&amp;#39; AND &amp;#39;resource&amp;#39; AND ... ] OR   [ &amp;#39;resource&amp;#39; AND &amp;#39;resource&amp;#39; AND ... ] OR   ...]iconIcon to be used on the application button. String in this field will be rendered as a HTML tag.e.g.icon: &amp;quot;&amp;lt;i class=&amp;#39;fa fa-clock-o&amp;#39;&amp;gt;&amp;lt;/i&amp;gt;&amp;quot;",
      "url": " /development/helium/writing_application.html",
      "group": "development/helium",
      "excerpt": "Apache Zeppelin Application is a package that runs on Interpreter process and displays it's output inside of the notebook. Make your own Application in Apache Zeppelin is quite easy."
    }
    ,



    "/development/helium/writing_spell.html": {
      "title": "Writing a new Helium Spell",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Writing a new SpellWhat is Apache Zeppelin SpellSpell is a kind of interpreter that runs on browser not on backend. So, technically it&amp;#39;s the frontend interpreter.It can provide many benefits.Spell is pluggable frontend interpreter. So it can be installed and removed easily using helium registry.Every spell is written in javascript. It means you can use existing javascript libraries whatever you want.Spell runs on browser like display system (%html, %table). In other words, every spell can be used as display system as well.How it worksHelium Spell works like Helium Visualization.Every helium packages are loaded from central (online) registry or local registryYou can see loaded packages in /helium page.When you enable a spell, it&amp;#39;s built from server and sent to clientFinally it will be loaded into browser.How to use spell1. EnablingFind a spell what you want to use in /helium package and click Enable button.2. UsingSpell works like an interpreter. Use the MAGIC value to execute spell in a note. (you might need to refresh after enabling)For example, Use %echo for the Echo Spell.Write a new SpellMaking a new spell is similar to Helium Visualization#write-new-visualization.Add framework dependency called zeppelin-spell into package.jsonWrite code using frameworkPublish your spell to npm1. Create a npm packageCreate a package.json in new directory for spell.You have to add a framework called zeppelin-spell as a dependency to create spell (zeppelin-spell)Also, you can add any dependencies you want to utilise.Here&amp;#39;s an example{  &amp;quot;name&amp;quot;: &amp;quot;zeppelin-echo-spell&amp;quot;,  &amp;quot;description&amp;quot;: &amp;quot;Zeppelin Echo Spell (example)&amp;quot;,  &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;,  &amp;quot;main&amp;quot;: &amp;quot;index&amp;quot;,  &amp;quot;author&amp;quot;: &amp;quot;&amp;quot;,  &amp;quot;license&amp;quot;: &amp;quot;Apache-2.0&amp;quot;,  &amp;quot;dependencies&amp;quot;: {    &amp;quot;zeppelin-spell&amp;quot;: &amp;quot;*&amp;quot;  },  &amp;quot;helium&amp;quot;: {    &amp;quot;icon&amp;quot; : &amp;quot;&amp;lt;i class=&amp;#39;fa fa-repeat&amp;#39;&amp;gt;&amp;lt;/i&amp;gt;&amp;quot;,    &amp;quot;spell&amp;quot;: {      &amp;quot;magic&amp;quot;: &amp;quot;%echo&amp;quot;,      &amp;quot;usage&amp;quot;: &amp;quot;%echo &amp;lt;TEXT&amp;gt;&amp;quot;    }  }}2. Write spell using frameworkHere are some examples you can referEcho SpellMarkdown Spell: Using libraryFlowchart Spell: Using DOMGoogle Translation API Spell: Using API (returning promise)Now, you need to write code to create spell which processing text.import {    SpellBase,    SpellResult,    DefaultDisplayType,} from &amp;#39;zeppelin-spell&amp;#39;;export default class EchoSpell extends SpellBase {    constructor() {        /** pass magic to super class&amp;#39;s constructor parameter */        super(&amp;quot;%echo&amp;quot;);    }    interpret(paragraphText) {        const processed = paragraphText + &amp;#39;!&amp;#39;;        /**         * should return `SpellResult` which including `data` and `type`         * default type is `TEXT` if you don&amp;#39;t specify.           */        return new SpellResult(processed);    }}Here is another example. Let&amp;#39;s say we want to create markdown spell. First of all, we should add a dependency for markdown in package.json// package.json &amp;quot;dependencies&amp;quot;: {    &amp;quot;markdown&amp;quot;: &amp;quot;0.5.0&amp;quot;,    &amp;quot;zeppelin-spell&amp;quot;: &amp;quot;*&amp;quot;  },And here is spell code.import {    SpellBase,    SpellResult,    DefaultDisplayType,} from &amp;#39;zeppelin-spell&amp;#39;;import md from &amp;#39;markdown&amp;#39;;const markdown = md.markdown;export default class MarkdownSpell extends SpellBase {    constructor() {        super(&amp;quot;%markdown&amp;quot;);    }    interpret(paragraphText) {        const parsed = markdown.toHTML(paragraphText);        /**         * specify `DefaultDisplayType.HTML` since `parsed` will contain DOM         * otherwise it will be rendered as `DefaultDisplayType.TEXT` (default)         */        return new SpellResult(parsed, DefaultDisplayType.HTML);    }}You might want to manipulate DOM directly (e.g google d3.js), then refer Flowchart SpellYou might want to return promise not string (e.g API call), then refer Google Translation API Spell3. Create Helium package file for local deploymentYou don&amp;#39;t want to publish your package every time you make a change in your spell. Zeppelin provides local deploy.The only thing you need to do is creating a Helium Package file (JSON) for local deploy.It&amp;#39;s automatically created when you publish to npm repository but in local case, you should make it by yourself.{  &amp;quot;type&amp;quot; : &amp;quot;SPELL&amp;quot;,  &amp;quot;name&amp;quot; : &amp;quot;zeppelin-echo-spell&amp;quot;,  &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;,  &amp;quot;description&amp;quot; : &amp;quot;Return just what receive (example)&amp;quot;,  &amp;quot;artifact&amp;quot; : &amp;quot;./zeppelin-examples/zeppelin-example-spell-echo&amp;quot;,  &amp;quot;license&amp;quot; : &amp;quot;Apache-2.0&amp;quot;,  &amp;quot;spell&amp;quot;: {    &amp;quot;magic&amp;quot;: &amp;quot;%echo&amp;quot;,    &amp;quot;usage&amp;quot;: &amp;quot;%echo &amp;lt;TEXT&amp;gt;&amp;quot;  }}Place this file in your local registry directory (default $ZEPPELIN_HOME/helium).type should be SPELLMake sure that artifact should be same as your spell directory.You can get information about other fields in Helium Visualization#3-create-helium-package-file-and-locally-deploy.4. Run in dev modecd zeppelin-webyarn run dev:heliumYou can browse localhost:9000. Every time refresh your browser, Zeppelin will rebuild your spell and reload changes.5. Publish your spell to the npm repositorySee Publishing npm packages",
      "url": " /development/helium/writing_spell.html",
      "group": "development/helium",
      "excerpt": "Spell is a kind of interpreter that runs on browser not on backend. So, technically it's the frontend interpreter. "
    }
    ,



    "/development/helium/writing_visualization_basic.html": {
      "title": "Writing a new Helium Visualization: basic",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Writing a new VisualizationWhat is Apache Zeppelin VisualizationApache Zeppelin Visualization is a pluggable package that can be loaded/unloaded on runtime through Helium framework in Zeppelin. A Visualization is a javascript npm package and user can use them just like any other built-in visualization in notebook.How it works1. Load Helium package files from registryZeppelin needs to know what Visualization packages are available. Zeppelin will read information of packages from both online and local registry.Registries are configurable through ZEPPELIN_HELIUM_LOCALREGISTRY_DEFAULT env variable or zeppelin.helium.localregistry.default property.2. Enable packagesOnce Zeppelin loads Helium package files from registries, available packages are displayed in Helium menu.Click &amp;#39;enable&amp;#39; button.3. Create and load visualization bundle on the flyOnce a Visualization package is enabled, HeliumBundleFactory creates a js bundle. The js bundle is served by helium/bundle/load rest api endpoint.4. Run visualizationZeppelin shows additional button for loaded Visualizations.User can use just like any other built-in visualizations.Write new Visualization1. Create a npm packageCreate a package.json in your new Visualization directory. You can add any dependencies in package.json, but you must include two dependencies: zeppelin-vis and zeppelin-tabledata.Here&amp;#39;s an example{  &amp;quot;name&amp;quot;: &amp;quot;zeppelin_horizontalbar&amp;quot;,  &amp;quot;description&amp;quot; : &amp;quot;Horizontal Bar chart&amp;quot;,  &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;,  &amp;quot;main&amp;quot;: &amp;quot;horizontalbar&amp;quot;,  &amp;quot;author&amp;quot;: &amp;quot;&amp;quot;,  &amp;quot;license&amp;quot;: &amp;quot;Apache-2.0&amp;quot;,  &amp;quot;dependencies&amp;quot;: {    &amp;quot;zeppelin-tabledata&amp;quot;: &amp;quot;*&amp;quot;,    &amp;quot;zeppelin-vis&amp;quot;: &amp;quot;*&amp;quot;  }}2. Create your own visualizationTo create your own visualization, you need to create a js file and import Visualization class from zeppelin-vis package and extend the class. zeppelin-tabledata package provides some useful transformations, like pivot, you can use in your visualization. (you can create your own transformation, too).Visualization class, there&amp;#39;re several methods that you need to override and implement. Here&amp;#39;s simple visualization that just prints Hello world.import Visualization from &amp;#39;zeppelin-vis&amp;#39;import PassthroughTransformation from &amp;#39;zeppelin-tabledata/passthrough&amp;#39;export default class helloworld extends Visualization {  constructor(targetEl, config) {    super(targetEl, config)    this.passthrough = new PassthroughTransformation(config);  }  render(tableData) {    this.targetEl.html(&amp;#39;Hello world!&amp;#39;)  }  getTransformation() {    return this.passthrough  }}To learn more about Visualization class, check visualization.js.You can check complete visualization package example here.Zeppelin&amp;#39;s built-in visualization uses the same API, so you can check built-in visualizations as additional examples.3. Create Helium package file and locally deployHelium Package file is a json file that provides information about the application.Json file contains the following information{  &amp;quot;type&amp;quot; : &amp;quot;VISUALIZATION&amp;quot;,  &amp;quot;name&amp;quot; : &amp;quot;zeppelin_horizontalbar&amp;quot;,  &amp;quot;description&amp;quot; : &amp;quot;Horizontal Bar chart (example)&amp;quot;,  &amp;quot;license&amp;quot; : &amp;quot;Apache-2.0&amp;quot;,  &amp;quot;artifact&amp;quot; : &amp;quot;./zeppelin-examples/zeppelin-example-horizontalbar&amp;quot;,  &amp;quot;icon&amp;quot; : &amp;quot;&amp;lt;i class=&amp;#39;fa fa-bar-chart rotate90flipX&amp;#39;&amp;gt;&amp;lt;/i&amp;gt;&amp;quot;}Place this file in your local registry directory (default ./helium).typeWhen you&amp;#39;re creating a visualization, &amp;#39;type&amp;#39; should be &amp;#39;VISUALIZATION&amp;#39;. Check these types as well.Helium ApplicationHelium SpellnameName of visualization. Should be unique. Allows [A-Za-z90-9_].descriptionA short description about visualization.artifactLocation of the visualization npm package. Support npm package with version or local filesystem path.e.g.When artifact exists in npm repository&amp;quot;artifact&amp;quot;: &amp;quot;my-visualiztion@1.0.0&amp;quot;When artifact exists in local file system&amp;quot;artifact&amp;quot;: &amp;quot;/path/to/my/visualization&amp;quot;licenseLicense information.e.g.&amp;quot;license&amp;quot;: &amp;quot;Apache-2.0&amp;quot;iconIcon to be used in visualization select button. String in this field will be rendered as a HTML tag.e.g.&amp;quot;icon&amp;quot;: &amp;quot;&amp;lt;i class=&amp;#39;fa fa-coffee&amp;#39;&amp;gt;&amp;lt;/i&amp;gt;&amp;quot;4. Run in dev modePlace your Helium package file in local registry (ZEPPELIN_HOME/helium).Run Zeppelin. And then run zeppelin-web in visualization dev mode.cd zeppelin-webyarn run dev:heliumYou can browse localhost:9000. Everytime refresh your browser, Zeppelin will rebuild your visualization and reload changes.5. Publish your visualizationOnce it&amp;#39;s done, publish your visualization package using npm publish.That&amp;#39;s it. With in an hour, your visualization will be available in Zeppelin&amp;#39;s helium menu.See MoreCheck Helium Visualization: Transformation for more complex examples.",
      "url": " /development/helium/writing_visualization_basic.html",
      "group": "development/helium",
      "excerpt": "Apache Zeppelin Visualization is a pluggable package that can be loaded/unloaded on runtime through Helium framework in Zeppelin. A Visualization is a javascript npm package and user can use them just like any other built-in visualization in a note."
    }
    ,



    "/development/helium/writing_visualization_transformation.html": {
      "title": "Transformations in Zeppelin Visualization",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Transformations for Zeppelin VisualizationOverviewTransformations renders setting which allows users to set columns and transforms table rows according to the configured columns.Zeppelin provides 4 types of transformations.1. PassthroughTransformationPassthroughTransformation is the simple transformation which does not convert original tabledata at all.See passthrough.js2. ColumnselectorTransformationColumnselectorTransformation is uses when you need N axes but do not need aggregation. See columnselector.js3. PivotTransformationPivotTransformation provides group by and aggregation. Every chart using PivotTransformation has 3 axes. Keys, Groups and Values.See pivot.js4. AdvancedTransformationAdvancedTransformation has more detailed options while providing existing features of PivotTransformation and ColumnselectorTransformationmultiple sub chartsconfigurable chart axesparameter widgets: input, checkbox, option, textareaparsing parameters automatically based on their typesexpand / fold axis and parameter panelsmultiple transformation methods while supporting lazy converting re-initialize the whole configuration based on spec hash.SpecAdvancedTransformation requires spec which includes axis and parameter details for charts.Let&amp;#39;s create 2 sub-charts called line and no-group. Each sub chart can have different axis and parameter depending on their requirements.class AwesomeVisualization extends Visualization {  constructor(targetEl, config) {    super(targetEl, config)    const spec = {      charts: {        &amp;#39;line&amp;#39;: {          transform: { method: &amp;#39;object&amp;#39;, },          sharedAxis: false, /** set if you want to share axes between sub charts, default is `false` */          axis: {            &amp;#39;xAxis&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;key&amp;#39;, description: &amp;#39;serial&amp;#39;, },            &amp;#39;yAxis&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;aggregator&amp;#39;, description: &amp;#39;serial&amp;#39;, },            &amp;#39;category&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;group&amp;#39;, description: &amp;#39;categorical&amp;#39;, },          },          parameter: {            &amp;#39;xAxisUnit&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;unit of xAxis&amp;#39;, },            &amp;#39;yAxisUnit&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;unit of yAxis&amp;#39;, },            &amp;#39;lineWidth&amp;#39;: { valueType: &amp;#39;int&amp;#39;, defaultValue: 0, description: &amp;#39;width of line&amp;#39;, },          },        },        &amp;#39;no-group&amp;#39;: {          transform: { method: &amp;#39;object&amp;#39;, },          sharedAxis: false,          axis: {            &amp;#39;xAxis&amp;#39;: { dimension: &amp;#39;single&amp;#39;, axisType: &amp;#39;key&amp;#39;, },            &amp;#39;yAxis&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;value&amp;#39;, },          },          parameter: {            &amp;#39;xAxisUnit&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;unit of xAxis&amp;#39;, },            &amp;#39;yAxisUnit&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;unit of yAxis&amp;#39;, },        },      },    }    this.transformation = new AdvancedTransformation(config, spec)  }  ...  // `render` will be called whenever `axis` or `parameter` is changed   render(data) {    const { chart, parameter, column, transformer, } = data    if (chart === &amp;#39;line&amp;#39;) {      const transformed = transformer()      // draw line chart     } else if (chart === &amp;#39;no-group&amp;#39;) {      const transformed = transformer()      // draw no-group chart     }  }}Spec: axisField NameAvailable Values (type)DescriptiondimensionsingleAxis can contains only 1 columndimensionmultipleAxis can contains multiple columnsaxisTypekeyColumn(s) in this axis will be used as key like in PivotTransformation. These columns will be served in column.keyaxisTypeaggregatorColumn(s) in this axis will be used as value like in PivotTransformation. These columns will be served in column.aggregatoraxisTypegroupColumn(s) in this axis will be used as group like in PivotTransformation. These columns will be served in column.groupaxisType(string)Any string value can be used here. These columns will be served in column.custommaxAxisCount (optional)(int)The max number of columns that this axis can contain. (unlimited if undefined)minAxisCount (optional)(int)The min number of columns that this axis should contain to draw chart. (1 in case of single dimension)description (optional)(string)Description for the axis.Here is an example.axis: {  &amp;#39;xAxis&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;key&amp;#39;,  },  &amp;#39;yAxis&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;aggregator&amp;#39;},  &amp;#39;category&amp;#39;: { dimension: &amp;#39;multiple&amp;#39;, axisType: &amp;#39;group&amp;#39;, maxAxisCount: 2, valueType: &amp;#39;string&amp;#39;, },},Spec: sharedAxisIf you set sharedAxis: false for sub charts, then their axes are persisted in global space (shared). It&amp;#39;s useful for when you creating multiple sub charts sharing their axes but have different parameters. For example, basic-column, stacked-column, percent-columnpie and donutHere is an example.    const spec = {      charts: {        &amp;#39;column&amp;#39;: {          transform: { method: &amp;#39;array&amp;#39;, },          sharedAxis: true,          axis: { ... },          parameter: { ... },        },        &amp;#39;stacked&amp;#39;: {          transform: { method: &amp;#39;array&amp;#39;, },          sharedAxis: true,          axis: { ... }          parameter: { ... },        },Spec: parameterField NameAvailable Values (type)DescriptionvalueTypestringParameter which has string valuevalueTypeintParameter which has int valuevalueTypefloatParameter which has float valuevalueTypebooleanParameter which has boolean value used with checkbox widget usuallyvalueTypeJSONParameter which has JSON value used with textarea widget usually. defaultValue should be &amp;quot;&amp;quot; (empty string). Thisdescription(string)Description of this parameter. This value will be parsed as HTML for pretty outputwidgetinputUse input widget. This is the default widget (if widget is undefined)widgetcheckboxUse checkbox widget.widgettextareaUse textarea widget.widgetoptionUse select + option widget. This parameter should have optionValues field as well.optionValues(Array)Available option values used with the option widgetHere is an example.parameter: {  // string type, input widget  &amp;#39;xAxisUnit&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;unit of xAxis&amp;#39;, },  // boolean type, checkbox widget  &amp;#39;inverted&amp;#39;: { widget: &amp;#39;checkbox&amp;#39;, valueType: &amp;#39;boolean&amp;#39;, defaultValue: false, description: &amp;#39;invert x and y axes&amp;#39;, },  // string type, option widget with `optionValues`  &amp;#39;graphType&amp;#39;: { widget: &amp;#39;option&amp;#39;, valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;line&amp;#39;, description: &amp;#39;graph type&amp;#39;, optionValues: [ &amp;#39;line&amp;#39;, &amp;#39;smoothedLine&amp;#39;, &amp;#39;step&amp;#39;, ], },  // HTML in `description`  &amp;#39;dateFormat&amp;#39;: { valueType: &amp;#39;string&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;format of date (&amp;lt;a href=&amp;quot;https://docs.amcharts.com/3/javascriptcharts/AmGraph#dateFormat&amp;quot;&amp;gt;doc&amp;lt;/a&amp;gt;) (e.g YYYY-MM-DD)&amp;#39;, },  // JSON type, textarea widget  &amp;#39;yAxisGuides&amp;#39;: { widget: &amp;#39;textarea&amp;#39;, valueType: &amp;#39;JSON&amp;#39;, defaultValue: &amp;#39;&amp;#39;, description: &amp;#39;guides of yAxis &amp;#39;, },Spec: transformField NameAvailable Values (type)Descriptionmethodobjectdesigned for rows requiring object manipulationmethodarraydesigned for rows requiring array manipulationmethodarray:2-keydesigned for xyz charts (e.g bubble chart)methoddrill-downdesigned for drill-down chartsmethodrawwill return the original tableData.rowsWhatever you specified as transform.method, the transformer value will be always function for lazy computation. // advanced-transformation.util#getTransformerif (transformSpec.method === &amp;#39;raw&amp;#39;) {  transformer = () =&amp;gt; { return rows; }} else if (transformSpec.method === &amp;#39;array&amp;#39;) {  transformer = () =&amp;gt; {    ...    return { ... }  }}Here is actual usage.class AwesomeVisualization extends Visualization {  constructor(...) { /** setup your spec */ }  ...   // `render` will be called whenever `axis` or `parameter` are changed  render(data) {    const { chart, parameter, column, transformer, } = data    if (chart === &amp;#39;line&amp;#39;) {      const transformed = transformer()      // draw line chart     } else if (chart === &amp;#39;no-group&amp;#39;) {      const transformed = transformer()      // draw no-group chart     }  }  ...}",
      "url": " /development/helium/writing_visualization_transformation.html",
      "group": "development/helium",
      "excerpt": "Description for Transformations"
    }
    ,



    "/development/writing_zeppelin_interpreter.html": {
      "title": "Writing a New Interpreter",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Writing a New InterpreterWhat is Apache Zeppelin InterpreterApache Zeppelin Interpreter is a language backend. For example to use scala code in Zeppelin, you need a scala interpreter.Every Interpreters belongs to an InterpreterGroup.Interpreters in the same InterpreterGroup can reference each other. For example, SparkSqlInterpreter can reference SparkInterpreter to get SparkContext from it while they&amp;#39;re in the same group.InterpreterSetting is configuration of a given InterpreterGroup and a unit of start/stop interpreter.All Interpreters in the same InterpreterSetting are launched in a single, separate JVM process. The Interpreter communicates with Zeppelin engine via Thrift.In &amp;#39;Separate Interpreter(scoped / isolated) for each note&amp;#39; mode which you can see at the Interpreter Setting menu when you create a new interpreter, new interpreter instance will be created per note. But it still runs on the same JVM while they&amp;#39;re in the same InterpreterSettings.Make your own InterpreterCreating a new interpreter is quite simple. Just extend org.apache.zeppelin.interpreter abstract class and implement some methods.For your interpreter project, you need to make interpreter-parent as your parent project and use plugin maven-enforcer-plugin, maven-dependency-plugin and maven-resources-plugin. Here&amp;#39;s one sample pom.xml &amp;lt;project xmlns=&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xsi:schemaLocation=&amp;quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&amp;quot;&amp;gt;    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;    &amp;lt;parent&amp;gt;        &amp;lt;artifactId&amp;gt;interpreter-parent&amp;lt;/artifactId&amp;gt;        &amp;lt;groupId&amp;gt;org.apache.zeppelin&amp;lt;/groupId&amp;gt;        &amp;lt;version&amp;gt;0.9.0-SNAPSHOT&amp;lt;/version&amp;gt;        &amp;lt;relativePath&amp;gt;../interpreter-parent&amp;lt;/relativePath&amp;gt;    &amp;lt;/parent&amp;gt;    ...    &amp;lt;dependencies&amp;gt;        &amp;lt;dependency&amp;gt;            &amp;lt;groupId&amp;gt;org.apache.zeppelin&amp;lt;/groupId&amp;gt;            &amp;lt;artifactId&amp;gt;zeppelin-interpreter&amp;lt;/artifactId&amp;gt;            &amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;            &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;        &amp;lt;/dependency&amp;gt;    &amp;lt;/dependencies&amp;gt;    &amp;lt;build&amp;gt;        &amp;lt;plugins&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-enforcer-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-dependency-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;            &amp;lt;plugin&amp;gt;                &amp;lt;artifactId&amp;gt;maven-resources-plugin&amp;lt;/artifactId&amp;gt;            &amp;lt;/plugin&amp;gt;        &amp;lt;/plugins&amp;gt;    &amp;lt;/build&amp;gt;&amp;lt;/project&amp;gt;You should include org.apache.zeppelin:zeppelin-interpreter:[VERSION] as your interpreter&amp;#39;s dependency in pom.xml. BesAnd you should put your jars under your interpreter directory with a specific directory name. Zeppelin server reads interpreter directories recursively and initializes interpreters including your own interpreter.There are three locations where you can store your interpreter group, name and other information. Zeppelin server tries to find the location below. Next, Zeppelin tries to find interpreter-setting.json in your interpreter jar.{ZEPPELIN_INTERPRETER_DIR}/{YOUR_OWN_INTERPRETER_DIR}/interpreter-setting.jsonHere is an example of interpreter-setting.json on your own interpreter.[  {    &amp;quot;group&amp;quot;: &amp;quot;your-group&amp;quot;,    &amp;quot;name&amp;quot;: &amp;quot;your-name&amp;quot;,    &amp;quot;className&amp;quot;: &amp;quot;your.own.interpreter.class&amp;quot;,    &amp;quot;properties&amp;quot;: {      &amp;quot;properties1&amp;quot;: {        &amp;quot;envName&amp;quot;: null,        &amp;quot;propertyName&amp;quot;: &amp;quot;property.1.name&amp;quot;,        &amp;quot;defaultValue&amp;quot;: &amp;quot;propertyDefaultValue&amp;quot;,        &amp;quot;description&amp;quot;: &amp;quot;Property description&amp;quot;,        &amp;quot;type&amp;quot;: &amp;quot;textarea&amp;quot;      },      &amp;quot;properties2&amp;quot;: {        &amp;quot;envName&amp;quot;: PROPERTIES_2,        &amp;quot;propertyName&amp;quot;: null,        &amp;quot;defaultValue&amp;quot;: &amp;quot;property2DefaultValue&amp;quot;,        &amp;quot;description&amp;quot;: &amp;quot;Property 2 description&amp;quot;,        &amp;quot;type&amp;quot;: &amp;quot;textarea&amp;quot;      }, ...    },    &amp;quot;editor&amp;quot;: {      &amp;quot;language&amp;quot;: &amp;quot;your-syntax-highlight-language&amp;quot;,      &amp;quot;editOnDblClick&amp;quot;: false,      &amp;quot;completionKey&amp;quot;: &amp;quot;TAB&amp;quot;    },    &amp;quot;config&amp;quot;: {      &amp;quot;runOnSelectionChange&amp;quot;: true/false,      &amp;quot;title&amp;quot;: true/false,      &amp;quot;checkEmpty&amp;quot;: true/false    }  },  {    ...  }]Finally, Zeppelin uses static initialization with the following:static {  Interpreter.register(&amp;quot;MyInterpreterName&amp;quot;, MyClassName.class.getName());}Static initialization is deprecated and will be supported until 0.6.0.The name will appear later in the interpreter name option box during the interpreter configuration process.The name of the interpreter is what you later write to identify a paragraph which should be interpreted using this interpreter.%MyInterpreterNamesome interpreter specific code...Editor setting for InterpreterYou can add editor object to interpreter-setting.json file to specify paragraph editor settings.LanguageIf the interpreter uses a specific programming language (like Scala, Python, SQL), it is generally recommended to add a syntax highlighting supported for that to the note paragraph editor.To check out the list of languages supported, see the mode-*.js files under zeppelin-web/bower_components/ace-builds/src-noconflict or from github.com/ajaxorg/ace-builds.If you want to add a new set of syntax highlighting,  Add the mode-*.js file to zeppelin-web/bower.json (when built, zeppelin-web/src/index.html will be changed automatically).Add language field to editor object. Note that if you don&amp;#39;t specify language field, your interpreter will use plain text mode for syntax highlighting. Let&amp;#39;s say you want to set your language to java, then add:&amp;quot;editor&amp;quot;: {  &amp;quot;language&amp;quot;: &amp;quot;java&amp;quot;}Edit on double clickIf your interpreter uses mark-up language such as markdown or HTML, set editOnDblClick to true so that text editor opens on pargraph double click and closes on paragraph run. Otherwise set it to false.&amp;quot;editor&amp;quot;: {  &amp;quot;editOnDblClick&amp;quot;: false}Completion key (Optional)By default, Ctrl+dot(.) brings autocompletion list in the editor.Through completionKey, each interpreter can configure autocompletion key.Currently TAB is only available option.&amp;quot;editor&amp;quot;: {  &amp;quot;completionKey&amp;quot;: &amp;quot;TAB&amp;quot;}Notebook paragraph display title (Optional)The notebook paragraph does not display the title by default.You can have the title of the notebook display the title by config.title=true.&amp;quot;config&amp;quot;: {  &amp;quot;title&amp;quot;: true  # default: false}Notebook run on selection change (Optional)The dynamic form in the notebook triggers execution when the selection is modified.You can make the dynamic form in the notebook not trigger execution after selecting the modification by setting config.runOnSelectionChange=false.&amp;quot;config&amp;quot;: {  &amp;quot;runOnSelectionChange&amp;quot;: false # default: true}Check if the paragraph is empty before running (Optional)The notebook&amp;#39;s paragraph default will not run if it is empty.You can set config.checkEmpty=false, to run even when the paragraph of the notebook is empty.&amp;quot;config&amp;quot;: {  &amp;quot;checkEmpty&amp;quot;: false # default: true}Install your interpreter binaryOnce you have built your interpreter, you can place it under the interpreter directory with all its dependencies.[ZEPPELIN_HOME]/interpreter/[INTERPRETER_NAME]/Configure your interpreterTo configure your interpreter you need to follow these steps:Start Zeppelin by running ./bin/zeppelin-daemon.sh start.In the interpreter page, click the +Create button and configure your interpreter properties.Now you are done and ready to use your interpreter.Note : Interpreters released with zeppelin have a default configuration which is used when there is no conf/zeppelin-site.xml.Use your interpreter0.5.0Inside of a note, %[INTERPRETER_NAME] directive will call your interpreter.Note that the first interpreter configuration in zeppelin.interpreters will be the default one.For example,%myintpval a = &amp;quot;My interpreter&amp;quot;println(a)0.6.0 and laterInside of a note, %[INTERPRETER_GROUP].[INTERPRETER_NAME] directive will call your interpreter.You can omit either [INTERPRETER_GROUP] or [INTERPRETER_NAME]. If you omit [INTERPRETER_NAME], then first available interpreter will be selected in the [INTERPRETER_GROUP].Likewise, if you skip [INTERPRETER_GROUP], then [INTERPRETER_NAME] will be chosen from default interpreter group.For example, if you have two interpreter myintp1 and myintp2 in group mygrp, you can call myintp1 like%mygrp.myintp1codes for myintp1and you can call myintp2 like%mygrp.myintp2codes for myintp2If you omit your interpreter name, it&amp;#39;ll select first available interpreter in the group ( myintp1 ).%mygrpcodes for myintp1You can only omit your interpreter group when your interpreter group is selected as a default group.%myintp2codes for myintp2ExamplesCheckout some interpreters released with Zeppelin by default.sparkmarkdownshelljdbcContributing a new Interpreter to Zeppelin releasesWe welcome contribution to a new interpreter. Please follow these few steps:First, check out the general contribution guide here.Follow the steps in Make your own Interpreter section and Editor setting for Interpreter above.Add your interpreter as in the Configure your interpreter section above; also add it to the example template zeppelin-site.xml.template.Add tests! They are run for all changes and it is important that they are self-contained.Include your interpreter as a module in pom.xml.Add documentation on how to use your interpreter under docs/interpreter/. Follow the Markdown style as this example. Make sure you list config settings and provide working examples on using your interpreter in code boxes in Markdown. Link to images as appropriate (images should go to docs/assets/themes/zeppelin/img/docs-img/). And add a link to your documentation in the navigation menu (docs/_includes/themes/zeppelin/_navigation.html).Most importantly, ensure licenses of the transitive closure of all dependencies are list in license file.Commit your changes and open a Pull Request on the project Mirror on GitHub; check to make sure Travis CI build is passing.",
      "url": " /development/writing_zeppelin_interpreter.html",
      "group": "development",
      "excerpt": "Apache Zeppelin Interpreter is a language backend. Every Interpreters belongs to an InterpreterGroup. Interpreters in the same InterpreterGroup can reference each other."
    }
    ,




    "/interpreter/alluxio.html": {
      "title": "Alluxio Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Alluxio Interpreter for Apache ZeppelinOverviewAlluxio is a memory-centric distributed storage system enabling reliable data sharing at memory-speed across cluster frameworks.Configuration      Name    Class    Description        alluxio.master.hostname    localhost    Alluxio master hostname        alluxio.master.port    19998    Alluxio master port  Enabling Alluxio InterpreterIn a notebook, to enable the Alluxio interpreter, click on the Gear icon and select Alluxio.Using the Alluxio InterpreterIn a paragraph, use %alluxio to select the Alluxio interpreter and then input all commands.%alluxiohelpTip : Use ( Ctrl + . ) for autocompletion.Interpreter CommandsThe Alluxio interpreter accepts the following commands.            Operation      Syntax      Description              cat      cat &amp;quot;path&amp;quot;      Print the content of the file to the console.              chgrp      chgrp &amp;quot;group&amp;quot; &amp;quot;path&amp;quot;      Change the group of the directory or file.              chmod      chmod &amp;quot;permission&amp;quot; &amp;quot;path&amp;quot;      Change the permission of the directory or file.              chown      chown &amp;quot;owner&amp;quot; &amp;quot;path&amp;quot;      Change the owner of the directory or file.              copyFromLocal      copyFromLocal &amp;quot;source path&amp;quot; &amp;quot;remote path&amp;quot;      Copy the specified file specified by &amp;quot;source path&amp;quot; to the path specified by &amp;quot;remote path&amp;quot;.      This command will fail if &amp;quot;remote path&amp;quot; already exists.              copyToLocal      copyToLocal &amp;quot;remote path&amp;quot; &amp;quot;local path&amp;quot;      Copy the specified file from the path specified by &amp;quot;remote path&amp;quot; to a local destination.              count      count &amp;quot;path&amp;quot;      Display the number of folders and files matching the specified prefix in &amp;quot;path&amp;quot;.              du      du &amp;quot;path&amp;quot;      Display the size of a file or a directory specified by the input path.              fileInfo      fileInfo &amp;quot;path&amp;quot;      Print the information of the blocks of a specified file.              free      free &amp;quot;path&amp;quot;      Free a file or all files under a directory from Alluxio. If the file/directory is also      in under storage, it will still be available there.              getCapacityBytes      getCapacityBytes      Get the capacity of the AlluxioFS.              getUsedBytes      getUsedBytes      Get number of bytes used in the AlluxioFS.              load      load &amp;quot;path&amp;quot;      Load the data of a file or a directory from under storage into Alluxio.              loadMetadata      loadMetadata &amp;quot;path&amp;quot;      Load the metadata of a file or a directory from under storage into Alluxio.              location      location &amp;quot;path&amp;quot;      Display a list of hosts that have the file data.              ls      ls &amp;quot;path&amp;quot;      List all the files and directories directly under the given path with information such as      size.              mkdir      mkdir &amp;quot;path1&amp;quot; ... &amp;quot;pathn&amp;quot;      Create directory(ies) under the given paths, along with any necessary parent directories.      Multiple paths separated by spaces or tabs. This command will fail if any of the given paths      already exist.              mount      mount &amp;quot;path&amp;quot; &amp;quot;uri&amp;quot;      Mount the underlying file system path &amp;quot;uri&amp;quot; into the Alluxio namespace as &amp;quot;path&amp;quot;. The &amp;quot;path&amp;quot;      is assumed not to exist and is created by the operation. No data or metadata is loaded from under      storage into Alluxio. After a path is mounted, operations on objects under the mounted path are      mirror to the mounted under storage.              mv      mv &amp;quot;source&amp;quot; &amp;quot;destination&amp;quot;      Move a file or directory specified by &amp;quot;source&amp;quot; to a new location &amp;quot;destination&amp;quot;. This command      will fail if &amp;quot;destination&amp;quot; already exists.              persist      persist &amp;quot;path&amp;quot;      Persist a file or directory currently stored only in Alluxio to the underlying file system.              pin      pin &amp;quot;path&amp;quot;      Pin the given file to avoid evicting it from memory. If the given path is a directory, it      recursively pins all the files contained and any new files created within this directory.              report      report &amp;quot;path&amp;quot;      Report to the master that a file is lost.              rm      rm &amp;quot;path&amp;quot;      Remove a file. This command will fail if the given path is a directory rather than a file.              setTtl      setTtl &amp;quot;time&amp;quot;      Set the TTL (time to live) in milliseconds to a file.              tail      tail &amp;quot;path&amp;quot;      Print the last 1KB of the specified file to the console.              touch      touch &amp;quot;path&amp;quot;      Create a 0-byte file at the specified location.              unmount      unmount &amp;quot;path&amp;quot;      Unmount the underlying file system path mounted in the Alluxio namespace as &amp;quot;path&amp;quot;. Alluxio      objects under &amp;quot;path&amp;quot; are removed from Alluxio, but they still exist in the previously mounted      under storage.              unpin      unpin &amp;quot;path&amp;quot;      Unpin the given file to allow Alluxio to evict this file again. If the given path is a      directory, it recursively unpins all files contained and any new files created within this      directory.              unsetTtl      unsetTtl      Remove the TTL (time to live) setting from a file.      How to test it&amp;#39;s workingBe sure to have configured correctly the Alluxio interpreter, then open a new paragraph and type one of the above commands.Below a simple example to show how to interact with Alluxio interpreter.Following steps are performed:using sh interpreter a new text file is created on local machineusing Alluxio interpreter:is listed the content of the afs (Alluxio File System) rootthe file previously created is copied to afsis listed again the content of the afs root to check the existence of the new copied fileis showed the content of the copied file (using the tail command)the file previously copied to afs is copied to local machine using sh interpreter it&amp;#39;s checked the existence of the new file copied from Alluxio and its content is showed  ",
      "url": " /interpreter/alluxio.html",
      "group": "interpreter",
      "excerpt": "Alluxio is a memory-centric distributed storage system enabling reliable data sharing at memory-speed across cluster frameworks."
    }
    ,



    "/interpreter/beam.html": {
      "title": "Beam interpreter in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Beam interpreter for Apache ZeppelinOverviewApache Beam is an open source unified platform for data processing pipelines. A pipeline can be build using one of the Beam SDKs.The execution of the pipeline is done by different Runners. Currently, Beam supports Apache Flink Runner, Apache Spark Runner, and Google Dataflow Runner.How to useBasically, you can write normal Beam java code where you can determine the Runner. You should write the main method inside a class becuase the interpreter invoke this main to execute the pipeline. Unlike Zeppelin normal pattern, each paragraph is considered as a separate job, there isn&amp;#39;t any relation to any other paragraph.The following is a demonstration of a word count example with data represented in array of stringsBut it can read data from files by replacing Create.of(SENTENCES).withCoder(StringUtf8Coder.of()) with TextIO.Read.from(&amp;quot;path/to/filename.txt&amp;quot;)%beam// most used importsimport org.apache.beam.sdk.coders.StringUtf8Coder;import org.apache.beam.sdk.transforms.Create;import java.io.Serializable;import java.util.Arrays;import java.util.List;import java.util.ArrayList;import org.apache.beam.runners.direct.*;import org.apache.beam.sdk.runners.*;import org.apache.beam.sdk.options.*;import org.apache.beam.runners.flink.*;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;import org.apache.beam.sdk.options.PipelineOptions;public class MinimalWordCount {  static List&amp;lt;String&amp;gt; s = new ArrayList&amp;lt;&amp;gt;();  static final String[] SENTENCES_ARRAY = new String[] {    &amp;quot;Hadoop is the Elephant King!&amp;quot;,    &amp;quot;A yellow and elegant thing.&amp;quot;,    &amp;quot;He never forgets&amp;quot;,    &amp;quot;Useful data, or lets&amp;quot;,    &amp;quot;An extraneous element cling!&amp;quot;,    &amp;quot;A wonderful king is Hadoop.&amp;quot;,    &amp;quot;The elephant plays well with Sqoop.&amp;quot;,    &amp;quot;But what helps him to thrive&amp;quot;,    &amp;quot;Are Impala, and Hive,&amp;quot;,    &amp;quot;And HDFS in the group.&amp;quot;,    &amp;quot;Hadoop is an elegant fellow.&amp;quot;,    &amp;quot;An elephant gentle and mellow.&amp;quot;,    &amp;quot;He never gets mad,&amp;quot;,    &amp;quot;Or does anything bad,&amp;quot;,    &amp;quot;Because, at his core, he is yellow&amp;quot;,    };    static final List&amp;lt;String&amp;gt; SENTENCES = Arrays.asList(SENTENCES_ARRAY);  public static void main(String[] args) {    PipelineOptions options = PipelineOptionsFactory.create().as(PipelineOptions.class);    options.setRunner(FlinkRunner.class);    Pipeline p = Pipeline.create(options);    p.apply(Create.of(SENTENCES).withCoder(StringUtf8Coder.of()))         .apply(&amp;quot;ExtractWords&amp;quot;, ParDo.of(new DoFn&amp;lt;String, String&amp;gt;() {           @ProcessElement           public void processElement(ProcessContext c) {             for (String word : c.element().split(&amp;quot;[^a-zA-Z&amp;#39;]+&amp;quot;)) {               if (!word.isEmpty()) {                 c.output(word);               }             }           }         }))        .apply(Count.&amp;lt;String&amp;gt; perElement())        .apply(&amp;quot;FormatResults&amp;quot;, ParDo.of(new DoFn&amp;lt;KV&amp;lt;String, Long&amp;gt;, String&amp;gt;() {          @ProcessElement          public void processElement(DoFn&amp;lt;KV&amp;lt;String, Long&amp;gt;, String&amp;gt;.ProcessContext arg0)            throws Exception {            s.add(&amp;quot;n&amp;quot; + arg0.element().getKey() + &amp;quot;t&amp;quot; + arg0.element().getValue());            }        }));    p.run();    System.out.println(&amp;quot;%table wordtcount&amp;quot;);    for (int i = 0; i &amp;lt; s.size(); i++) {      System.out.print(s.get(i));    }  }}",
      "url": " /interpreter/beam.html",
      "group": "interpreter",
      "excerpt": "Apache Beam is an open source, unified programming model that you can use to create a data processing pipeline."
    }
    ,



    "/interpreter/bigquery.html": {
      "title": "BigQuery Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;BigQuery Interpreter for Apache ZeppelinOverviewBigQuery is a highly scalable no-ops data warehouse in the Google Cloud Platform. Querying massive datasets can be time consuming and expensive without the right hardware and infrastructure. Google BigQuery solves this problem by enabling super-fast SQL queries against append-only tables using the processing power of Google&amp;#39;s infrastructure. Simply move your data into BigQuery and let us handle the hard work. You can control access to both the project and your data based on your business needs, such as giving others the ability to view or query your data.  Configuration      Name    Default Value    Description        zeppelin.bigquery.project_id          Google Project Id        zeppelin.bigquery.wait_time    5000    Query Timeout in Milliseconds        zeppelin.bigquery.max_no_of_rows    100000    Max result set size        zeppelin.bigquery.sql_dialect        BigQuery SQL dialect (standardSQL or legacySQL). If empty, [query prefix](https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#sql-prefix) like &#39;#standardSQL&#39; can be used.  BigQuery APIZeppelin is built against BigQuery API version v2-rev265-1.21.0 - API JavadocsEnabling the BigQuery InterpreterIn a notebook, to enable the BigQuery interpreter, click the Gear icon and select bigquery.Provide Application Default CredentialsWithin Google Cloud Platform (e.g. Google App Engine, Google Compute Engine),built-in credentials are used by default.Outside of GCP, follow the Google API authentication instructions for Zeppelin Google Cloud StorageUsing the BigQuery InterpreterIn a paragraph, use %bigquery.sql to select the BigQuery interpreter and then input SQL statements against your datasets stored in BigQuery.You can use BigQuery SQL Reference to build your own SQL.For Example, SQL to query for top 10 departure delays across airports using the flights public dataset%bigquery.sqlSELECT departure_airport,count(case when departure_delay&amp;gt;0 then 1 else 0 end) as no_of_delays FROM [bigquery-samples:airline_ontime_data.flights] group by departure_airport order by 2 desc limit 10Another Example, SQL to query for most commonly used java packages from the github data hosted in BigQuery %bigquery.sqlSELECT  package,  COUNT(*) countFROM (  SELECT    REGEXP_EXTRACT(line, r&amp;#39; ([a-z0-9._]*).&amp;#39;) package,    id  FROM (    SELECT      SPLIT(content, &amp;#39;n&amp;#39;) line,      id    FROM      [bigquery-public-data:github_repos.sample_contents]    WHERE      content CONTAINS &amp;#39;import&amp;#39;      AND sample_path LIKE &amp;#39;%.java&amp;#39;    HAVING      LEFT(line, 6)=&amp;#39;import&amp;#39; )  GROUP BY    package,    id )GROUP BY  1ORDER BY  count DESCLIMIT  40Technical descriptionFor in-depth technical details on current implementation please refer to bigquery/README.md.",
      "url": " /interpreter/bigquery.html",
      "group": "interpreter",
      "excerpt": "BigQuery is a highly scalable no-ops data warehouse in the Google Cloud Platform."
    }
    ,



    "/interpreter/cassandra.html": {
      "title": "Cassandra CQL Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Cassandra CQL Interpreter for Apache Zeppelin      Name    Class    Description        %cassandra    CassandraInterpreter    Provides interpreter for Apache Cassandra CQL query language  Enabling Cassandra InterpreterIn a notebook, to enable the Cassandra interpreter, click on the Gear icon and select Cassandra  Using the Cassandra InterpreterIn a paragraph, use %cassandra to select the Cassandra interpreter and then input all commands.To access the interactive help, type HELP;    Interpreter CommandsThe Cassandra interpreter accepts the following commands            Command Type      Command Name      Description              Help command      HELP      Display the interactive help menu              Schema commands      DESCRIBE KEYSPACE, DESCRIBE CLUSTER, DESCRIBE TABLES ...      Custom commands to describe the Cassandra schema              Option commands      @consistency, @fetchSize ...      Inject runtime options to all statements in the paragraph              Prepared statement commands      @prepare, @bind, @remove_prepared      Let you register a prepared command and re-use it later by injecting bound values              Native CQL statements      All CQL-compatible statements (SELECT, INSERT, CREATE, ...)      All CQL statements are executed directly against the Cassandra server      CQL statementsThis interpreter is compatible with any CQL statement supported by Cassandra. Ex:INSERT INTO users(login,name) VALUES(&amp;#39;jdoe&amp;#39;,&amp;#39;John DOE&amp;#39;);SELECT * FROM users WHERE login=&amp;#39;jdoe&amp;#39;;Each statement should be separated by a semi-colon ( ; ) except the special commands below:@prepare@bind@remove_prepare@consistency@serialConsistency@timestamp@fetchSize@requestTimeOutMulti-line statements as well as multiple statements on the same line are also supported as long as they are separated by a semi-colon. Ex:USE spark_demo;SELECT * FROM albums_by_country LIMIT 1; SELECT * FROM countries LIMIT 1;SELECT *FROM artistsWHERE login=&amp;#39;jlennon&amp;#39;;Batch statements are supported and can span multiple lines, as well as DDL (CREATE/ALTER/DROP) statements:BEGIN BATCH    INSERT INTO users(login,name) VALUES(&amp;#39;jdoe&amp;#39;,&amp;#39;John DOE&amp;#39;);    INSERT INTO users_preferences(login,account_type) VALUES(&amp;#39;jdoe&amp;#39;,&amp;#39;BASIC&amp;#39;);APPLY BATCH;CREATE TABLE IF NOT EXISTS test(    key int PRIMARY KEY,    value text);CQL statements are case-insensitive (except for column names and values). This means that the following statements are equivalent and valid:INSERT INTO users(login,name) VALUES(&amp;#39;jdoe&amp;#39;,&amp;#39;John DOE&amp;#39;);Insert into users(login,name) vAlues(&amp;#39;hsue&amp;#39;,&amp;#39;Helen SUE&amp;#39;);The complete list of all CQL statements and versions can be found below:         Cassandra Version     Documentation Link           3.x             &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;          href=&quot;https://docs.datastax.com/en/archived/cql/3.3/cql/cqlIntro.html&quot;&gt;          https://docs.datastax.com/en/archived/cql/3.3/cql/cqlIntro.html                        2.2             &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;          href=&quot;https://docs.datastax.com/en/archived/cql/3.3/cql/cqlIntro.html&quot;&gt;          https://docs.datastax.com/en/archived/cql/3.3/cql/cqlIntro.html                        2.1             &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;          href=&quot;http://docs.datastax.com/en/cql/3.1/cql/cql_intro_c.html&quot;&gt;          http://docs.datastax.com/en/cql/3.1/cql/cqlintroc.html                 Comments in statementsIt is possible to add comments between statements. Single line comments start with the hash sign (#), double slashes (//),  double dash (--). Multi-line comments are enclosed between /** and **/. Ex:# Single line comment style 1INSERT INTO users(login,name) VALUES(&amp;#39;jdoe&amp;#39;,&amp;#39;John DOE&amp;#39;);// Single line comment style 2// Single line comment style 3/** Multi line comments **/Insert into users(login,name) vAlues(&amp;#39;hsue&amp;#39;,&amp;#39;Helen SUE&amp;#39;);Syntax ValidationThe interpreters is shipped with a built-in syntax validator. This validator only checks for basic syntax errors.All CQL-related syntax validation is delegated directly to CassandraMost of the time, syntax errors are due to missing semi-colons between statements or typo errors.Schema commandsTo make schema discovery easier and more interactive, the following commands are supported:         Command     Description           DESCRIBE CLUSTER;     Show the current cluster name and its partitioner           DESCRIBE KEYSPACES;     List all existing keyspaces in the cluster and their configuration (replication factor, durable write ...)           DESCRIBE TABLES;     List all existing keyspaces in the cluster and for each, all the tables name           DESCRIBE TYPES;     List all existing keyspaces in the cluster and for each, all the user-defined types name           DESCRIBE FUNCTIONS;     List all existing keyspaces in the cluster and for each, all the functions name           DESCRIBE AGGREGATES;     List all existing keyspaces in the cluster and for each, all the aggregates name           DESCRIBE MATERIALIZED VIEWS;     List all existing keyspaces in the cluster and for each, all the materialized views name           DESCRIBE KEYSPACE &amp;lt;keyspacename&amp;gt;;     Describe the given keyspace configuration and all its table details (name, columns, ...)           DESCRIBE TABLE (&amp;lt;keyspacename&amp;gt;).&amp;lt;tablename&amp;gt;;             Describe the given table. If the keyspace is not provided, the current logged in keyspace is used.        If there is no logged in keyspace, the default system keyspace is used.        If no table is found, an error message is raised                DESCRIBE TYPE (&amp;lt;keyspacename&amp;gt;).&amp;lt;typename&amp;gt;;             Describe the given type(UDT). If the keyspace is not provided, the current logged in keyspace is used.        If there is no logged in keyspace, the default system keyspace is used.        If no type is found, an error message is raised                DESCRIBE FUNCTION (&amp;lt;keyspacename&amp;gt;).&amp;lt;functionname&amp;gt;;     Describe the given function. If the keyspace is not provided, the current logged in keyspace is used.         If there is no logged in keyspace, the default system keyspace is used.         If no function is found, an error message is raised                DESCRIBE AGGREGATE (&amp;lt;keyspacename&amp;gt;).&amp;lt;aggregatename&amp;gt;;     Describe the given aggregate. If the keyspace is not provided, the current logged in keyspace is used.         If there is no logged in keyspace, the default system keyspace is used.         If no aggregate is found, an error message is raised                DESCRIBE MATERIALIZED VIEW (&amp;lt;keyspacename&amp;gt;).&amp;lt;view_name&amp;gt;;     Describe the given view. If the keyspace is not provided, the current logged in keyspace is used.         If there is no logged in keyspace, the default system keyspace is used.         If no view is found, an error message is raised         The schema objects (cluster, keyspace, table, type, function and aggregate) are displayed in a tabular format.There is a drop-down menu on the top left corner to expand objects details. On the top right menu is shown the Icon legend.  Runtime Execution ParametersSometimes you want to be able to pass runtime query parameters to your statements.Those parameters are not part of the CQL specs and are specific to the interpreter.Below is the list of all parameters:         Parameter     Syntax     Description           Consistency Level     @consistency=value     Apply the given consistency level to all queries in the paragraph           Serial Consistency Level     @serialConsistency=value     Apply the given serial consistency level to all queries in the paragraph           Timestamp     @timestamp=long value             Apply the given timestamp to all queries in the paragraph.        Please note that timestamp value passed directly in CQL statement will override this value                 Fetch Size     @fetchSize=integer value     Apply the given fetch size to all queries in the paragraph           Request Time Out     @requestTimeOut=integer value     Apply the given request timeout in millisecs to all queries in the paragraph    Some parameters only accept restricted values:         Parameter     Possible Values           Consistency Level     ALL, ANY, ONE, TWO, THREE, QUORUM, LOCALONE, LOCALQUORUM, EACHQUORUM           Serial Consistency Level     SERIAL, LOCALSERIAL           Timestamp     Any long value           Fetch Size     Any integer value    Please note that you should not add semi-colon ( ; ) at the end of each parameter statementSome examples:CREATE TABLE IF NOT EXISTS spark_demo.ts(    key int PRIMARY KEY,    value text);TRUNCATE spark_demo.ts;// Timestamp in the past@timestamp=10// Force timestamp directly in the first insertINSERT INTO spark_demo.ts(key,value) VALUES(1,&amp;#39;first insert&amp;#39;) USING TIMESTAMP 100;// Select some data to make the clock turnSELECT * FROM spark_demo.albums LIMIT 100;// Now insert using the timestamp parameter set at the beginning(10)INSERT INTO spark_demo.ts(key,value) VALUES(1,&amp;#39;second insert&amp;#39;);// Check for the result. You should see &amp;#39;first insert&amp;#39;SELECT value FROM spark_demo.ts WHERE key=1;Some remarks about query parameters:many query parameters can be set in the same paragraphif the same query parameter is set many time with different values, the interpreter only take into account the first valueeach query parameter applies to all CQL statements in the same paragraph, unless you override the option using plain CQL text (like forcing timestamp with the USING clause)the order of each query parameter with regard to CQL statement does not matterRuntime Formatting ParametersSometimes you want to be able to format output of your statement. Cassandra interpreter allows to specify different parameters as local properties of the paragraph. Below is the list of all formatting parameters:         Parameter     Syntax     Description           Output Format     outputFormat=value     Controls, should we output data as CQL literals, or in human-readable form. Possible values: cql, human (default: human           Locale     locale=value     Locale for formatting of numbers &amp;amp; time-related values. Could be any locale supported by JVM (default: en_US)           Timezone     timezone=value     Timezone for formatting of time-related values. Could be any timezone supported by JVM (default: UTC)           Float precision     floatPrecision=value     Precision when formatting float values. Any positive integer value, or -1 to show everything           Double precision     doublePrecision=value     Precision when formatting double values. Any positive integer value, or -1 to show everything           Decimal precision     decimalPrecision=value     Precision when formatting decimal values. Any positive integer value, or -1 to show everything           Timestamp Format     timestampFormat=value     Format string for timestamp values. Should be valid DateTimeFormatter pattern           Time Format     timeFormat=value     Format string for time values. Should be valid DateTimeFormatter pattern           Date Format     dateFormat=value     Format string for date values. Should be valid DateTimeFormatter pattern    Some examples:create table if not exists zep.test_format (  id int primary key,  text text,  date date,  timestamp timestamp,  time time,  double double,  float float);insert into zep.test_format(id, text, date, timestamp, time, double, float)  values (1, &amp;#39;text&amp;#39;, &amp;#39;2019-01-29&amp;#39;, &amp;#39;2020-06-16T23:59:59.123Z&amp;#39;, &amp;#39;04:05:00.234&amp;#39;,   10.0153423453425634653463466346543, 20.0303443); %cassandra(outputFormat=human, locale=de_DE, floatPrecision=2, doublePrecision=4, timeFormat=hh:mma, timestampFormat=MM/dd/yy HH:mm, dateFormat=&amp;quot;E, d MMM yy&amp;quot;, timezone=Etc/GMT+2)select id, double, float, text, date, time, timestamp from zep.test_format;will output data formatted according to settings, including German locale:id  double   float  text  date           time     timestamp1   10,0153  20,03  text  Di, 29 Jan 19  04:05AM  06/16/20 21:59while with outputFormat=cql, data is formatted as CQL literals:id double              float       text    date        time                  timestamp1  10.015342345342564  20.030344  &amp;#39;text&amp;#39;  &amp;#39;2019-01-29&amp;#39; &amp;#39;04:05:00.234000000&amp;#39;  &amp;#39;2020-06-17T01:59:59.123+02:00&amp;#39;Support for Prepared StatementsFor performance reason, it is better to prepare statements before-hand and reuse them later by providing bound values.This interpreter provides 3 commands to handle prepared and bound statements:@prepare@bind@remove_preparedExample:@prepare[statement-name]=...@bind[statement-name]=’text’, 1223, ’2015-07-30 12:00:01’, null, true, [‘list_item1’, ’list_item2’]@bind[statement-name-with-no-bound-value]@remove_prepare[statement-name]@prepareYou can use the syntax &amp;quot;@prepare[statement-name]=SELECT...&amp;quot; to create a prepared statement.The statement-name is mandatory because the interpreter prepares the given statement with the Java driver andsaves the generated prepared statement in an internal hash map, using the provided statement-name as search key.Please note that this internal prepared statement map is shared with all notebooks and all paragraphs becausethere is only one instance of the interpreter for CassandraIf the interpreter encounters many @prepare for the same statement-name (key), only the first statement will be taken into account.Example:@prepare[select]=SELECT * FROM spark_demo.albums LIMIT ?@prepare[select]=SELECT * FROM spark_demo.artists LIMIT ?For the above example, the prepared statement is SELECT * FROM spark_demo.albums LIMIT ?.SELECT * FROM spark_demo.artists LIMIT ? is ignored because an entry already exists in the prepared statements map with the key select.In the context of Zeppelin, a notebook can be scheduled to be executed at regular interval,thus it is necessary to avoid re-preparing many time the same statement (considered an anti-pattern).@bindOnce the statement is prepared (possibly in a separated notebook/paragraph). You can bind values to it:@bind[select_first]=10Bound values are not mandatory for the @bind statement. However if you provide bound values, they need to comply to some syntax:String values should be enclosed between simple quotes (&amp;#39;)Date values should be enclosed between simple quotes (&amp;#39;) and respect the formats (full list is in the documentation):yyyy-MM-dd HH:MM:ssyyyy-MM-dd HH:MM:ss.SSSyyyy-mm-dd&amp;#39;T&amp;#39;HH:mm:ss.SSSZnull is parsed as-isboolean (true|false) are parsed as-iscollection values must follow the standard CQL syntax:list: [&amp;#39;listitem1&amp;#39;, &amp;#39;listitem2&amp;#39;, ...]set: {&amp;#39;setitem1&amp;#39;, &amp;#39;setitem2&amp;#39;, …}map: {&amp;#39;key1&amp;#39;: &amp;#39;val1&amp;#39;, &amp;#39;key2&amp;#39;: &amp;#39;val2&amp;#39;, …}tuple values should be enclosed between parenthesis (see Tuple CQL syntax): (&amp;#39;text&amp;#39;, 123, true)udt values should be enclosed between brackets (see UDT CQL syntax): {streename: &amp;#39;Beverly Hills&amp;#39;, number: 104, zipcode: 90020, state: &amp;#39;California&amp;#39;, …}It is possible to use the @bind statement inside a batch:BEGIN BATCH   @bind[insert_user]=&amp;#39;jdoe&amp;#39;,&amp;#39;John DOE&amp;#39;   UPDATE users SET age = 27 WHERE login=&amp;#39;hsue&amp;#39;;APPLY BATCH;@remove_prepareTo avoid for a prepared statement to stay forever in the prepared statement map, you can use the@remove_prepare[statement-name] syntax to remove it.Removing a non-existing prepared statement yields no error.Using Dynamic FormsInstead of hard-coding your CQL queries, it is possible to use [Zeppelin dynamic form] syntax to inject simple value or multiple choices forms.The legacy mustache syntax ( {{ }} ) to bind input text and select form is still supported but is deprecated and will be removed in future releases.LegacyThe syntax for simple parameter is: {{input_Label=default value}}. The default value is mandatory because the first time the paragraph is executed,we launch the CQL query before rendering the form so at least one value should be provided.The syntax for multiple choices parameter is: {{input_Label=value1 | value2 | … | valueN }}. By default the first choice is used for CQL querythe first time the paragraph is executed.Example:#Secondary index on performer styleSELECT name, country, performerFROM spark_demo.performersWHERE name=&amp;#39;${performer=Sheryl Crow|Doof|Fanfarlo|Los Paranoia}&amp;#39;AND styles CONTAINS &amp;#39;${style=Rock}&amp;#39;;In the above example, the first CQL query will be executed for performer=&amp;#39;Sheryl Crow&amp;#39; AND style=&amp;#39;Rock&amp;#39;.For subsequent queries, you can change the value directly using the form.Please note that we enclosed the ${ } block between simple quotes ( &amp;#39; ) because Cassandra expects a String here.We could have also use the ${style=&amp;#39;Rock&amp;#39;} syntax but this time, the value displayed on the form is &amp;#39;Rock&amp;#39; and not Rock.It is also possible to use dynamic forms for prepared statements:@bind[select]==&amp;#39;${performer=Sheryl Crow|Doof|Fanfarlo|Los Paranoia}&amp;#39;, &amp;#39;${style=Rock}&amp;#39;Shared statesIt is possible to execute many paragraphs in parallel. However, at the back-end side, we&amp;#39;re still using synchronous queries.Asynchronous execution is only possible when it is possible to return a Future value in the InterpreterResult.It may be an interesting proposal for the Zeppelin project.Recently, Zeppelin allows you to choose the level of isolation for your interpreters (see [Interpreter Binding Mode] ).Long story short, you have 3 available bindings:shared : same JVM and same Interpreter instance for all notesscoped : same JVM but different Interpreter instances, one for each noteisolated: different JVM running a single Interpreter instance, one JVM for each noteUsing the shared binding, the same com.datastax.driver.core.Session object is used for all notes and paragraphs.Consequently, if you use the USE keyspace_name; statement to log into a keyspace, it will change the keyspace forall current users of the Cassandra interpreter because we only create 1 com.datastax.driver.core.Session objectper instance of Cassandra interpreter.The same remark does apply to the prepared statement hash map, it is shared by all users using the same instance of Cassandra interpreter.When using scoped binding, in the same JVM Zeppelin will create multiple instances of the Cassandra interpreter, thus multiple com.datastax.driver.core.Session objects. Beware of resource and memory usage using this binding ! The isolated mode is the most extreme and will create as many JVM/com.datastax.driver.core.Session object as there are distinct notes.Interpreter ConfigurationTo configure the Cassandra interpreter, go to the Interpreter menu and scroll down to change the parameters.The Cassandra interpreter is using the official Datastax Java Driver for Apache Cassandra® and most of the parameters are used to configure the Java driverBelow are the configuration parameters supported by interpreter and their default values.        Property Name     Description     Default Value           cassandra.cluster     Name of the Cassandra cluster to connect to     Test Cluster           cassandra.compression.protocol     On wire compression. Possible values are: NONE, SNAPPY, LZ4     NONE           cassandra.credentials.username     If security is enable, provide the login     none           cassandra.credentials.password     If security is enable, provide the password     none           cassandra.hosts             Comma separated Cassandra hosts (DNS name or IP address).                Ex: 192.168.0.12,node2,node3           localhost           cassandra.interpreter.parallelism     Number of concurrent paragraphs(queries block) that can be executed     10           cassandra.keyspace             Default keyspace to connect to.                  It is strongly recommended to let the default value          and prefix the table name with the actual keyspace          in all of your queries                  system           cassandra.load.balancing.policy             Load balancing policy. Default = DefaultLoadBalancingPolicy        To Specify your own policy, provide the fully qualify class name (FQCN) of your policy.        At runtime the driver will instantiate the policy using class name.          DEFAULT           cassandra.max.schema.agreement.wait.second     Cassandra max schema agreement wait in second     10           cassandra.pooling.connection.per.host.local     Protocol V3 and above default = 1     1           cassandra.pooling.connection.per.host.remote     Protocol V3 and above default = 1     1           cassandra.pooling.heartbeat.interval.seconds     Cassandra pool heartbeat interval in secs     30           cassandra.pooling.max.request.per.connection     Protocol V3 and above default = 1024     1024           cassandra.pooling.pool.timeout.millisecs     Cassandra pool time out in millisecs     5000           cassandra.protocol.version     Cassandra binary protocol version (V3, V4, ...)     DEFAULT (detected automatically)           cassandra.query.default.consistency           Cassandra query default consistency level            Available values: ONE, TWO, THREE, QUORUM, LOCAL_ONE, LOCAL_QUORUM, EACH_QUORUM, ALL          ONE           cassandra.query.default.fetchSize     Cassandra query default fetch size     5000           cassandra.query.default.serial.consistency           Cassandra query default serial consistency level            Available values: SERIAL, LOCAL_SERIAL          SERIAL           cassandra.reconnection.policy             Cassandra Reconnection Policy.        Default = ExponentialReconnectionPolicy        To Specify your own policy, provide the fully qualify class name (FQCN) of your policy.        At runtime the driver will instantiate the policy using class name.          DEFAULT           cassandra.retry.policy             Cassandra Retry Policy.        Default = DefaultRetryPolicy        To Specify your own policy, provide the fully qualify class name (FQCN) of your policy.        At runtime the driver will instantiate the policy using class name.          DEFAULT           cassandra.socket.connection.timeout.millisecs     Cassandra socket default connection timeout in millisecs     500           cassandra.socket.read.timeout.millisecs     Cassandra socket read timeout in millisecs     12000           cassandra.socket.tcp.no_delay     Cassandra socket TCP no delay     true           cassandra.speculative.execution.policy             Cassandra Speculative Execution Policy.        Default = NoSpeculativeExecutionPolicy        To Specify your own policy, provide the fully qualify class name (FQCN) of your policy.        At runtime the driver will instantiate the policy using class name.          DEFAULT           cassandra.ssl.enabled             Enable support for connecting to the Cassandra configured with SSL.        To connect to Cassandra configured with SSL use true        and provide a truststore file and password with following options.          false           cassandra.ssl.truststore.path             Filepath for the truststore file to use for connection to Cassandra with SSL.                     cassandra.ssl.truststore.password             Password for the truststore file to use for connection to Cassandra with SSL.                     cassandra.format.output     Output format for data - strict CQL (cql), or human-readable (human)     human           cassandra.format.locale     Which locale to use for output (any locale supported by JVM could be specified)     en_US           cassandra.format.timezone     For which timezone format time/date-related types (any timezone supported by JVM could be specified)     UTC           cassandra.format.timestamp     Format string for timestamp columns (any valid DateTimeFormatter pattern could be used)     yyyy-MM-dd&amp;#39;T&amp;#39;HH:mm:ss.SSSXXX           cassandra.format.time     Format string for time columns (any valid DateTimeFormatter pattern could be used)     HH:mm:ss.SSS           cassandra.format.date     Format string for date columns (any valid DateTimeFormatter pattern could be used)     yyyy-MM-dd           cassandra.format.float_precision     Precision when formatting values of float type     5           cassandra.format.double_precision     Precision when formatting values of double type     12           cassandra.format.decimal_precision     Precision when formatting values of decimal type     -1 (show everything)    Besides these parameters, it&amp;#39;s also possible to set other driver parameters by adding them into interpreter configuration.  The configuration key should have full form with datastax-java-driver prefix, as described in documentation.  For example, to specify 5 seconds request timeout, you can use datastax-java-driver.basic.request.timeout with value of 5 seconds.  Full list of available configuration options is available in documentation.  Additional options may override the options that are specified by the interpreter&amp;#39;s configuration parameters.Change Log4.0 (Zeppelin 0.10.1) :Refactor to use unified Java driver 4.7 (ZEPPELIN-4378:changes in configuration were necessary, as new driver has different architecture, and configuration optionsinterpreter got support for DSE-specific data types, and other extensionssupport for @retryPolicy is removed, as only single retry policy is shipped with driverallow to specify any configuration option of Java driverdropped support for Cassandra 1.2 &amp;amp; 2.0, that isn&amp;#39;t supported by driver anymoreadded support for formatting options, both interpreter &amp;amp; cell level3.1 (Zeppelin 0.10.1) :Upgrade Java driver to 3.7.2 (ZEPPELIN-4331;3.0 (Zeppelin 0.10.1) :Update documentationUpdate interactive documentationAdd support for binary protocol V4Implement new @requestTimeOut runtime optionUpgrade Java driver version to 3.0.1Allow interpreter to add dynamic forms programmatically when using FormType.SIMPLEAllow dynamic form using default Zeppelin syntaxFixing typo on FallThroughPolicyLook for data in AngularObjectRegistry before creating dynamic formAdd missing support for ALTER statements2.0 (Zeppelin 0.10.1) :Update help menu and add changelogAdd Support for User Defined Functions, User Defined Aggregates and Materialized ViewsUpgrade Java driver version to 3.0.0-rc11.0 (Zeppelin 0.5.5-incubating) :Initial versionBugs &amp;amp; ContactsIf you encounter a bug for this interpreter, please create a JIRA ticket.Zeppelin Dynamic FormInterpreter Binding Mode",
      "url": " /interpreter/cassandra.html",
      "group": "interpreter",
      "excerpt": "Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance."
    }
    ,



    "/interpreter/elasticsearch.html": {
      "title": "Elasticsearch Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Elasticsearch Interpreter for Apache ZeppelinOverviewElasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.Configuration      Property    Default    Description        elasticsearch.cluster.name    elasticsearch    Cluster name        elasticsearch.host    localhost    Host of a node in the cluster        elasticsearch.port    9300    Connection port ( Important: it depends on the client type, transport or http)        elasticsearch.client.type    transport    The type of client for Elasticsearch (transport or http)( Important: the port depends on this value)        elasticsearch.basicauth.username        Username for a basic authentication (http)        elasticsearch.basicauth.password        Password for a basic authentication (http)        elasticsearch.result.size    10    The size of the result set of a search query    Note #1 : You can add more properties to configure the Elasticsearch client.Note #2 : If you use Shield, you can add a property named shield.user with a value containing the name and the password ( format: username:password ). For more details about Shield configuration, consult the Shield reference guide. Do not forget, to copy the shield client jar in the interpreter directory (ZEPPELIN_HOME/interpreters/elasticsearch).Enabling the Elasticsearch InterpreterIn a notebook, to enable the Elasticsearch interpreter, click the Gear icon and select Elasticsearch.Using the Elasticsearch InterpreterIn a paragraph, use %elasticsearch to select the Elasticsearch interpreter and then input all commands. To get the list of available commands, use help.%elasticsearchhelpElasticsearch interpreter:General format: &amp;lt;command&amp;gt; /&amp;lt;indices&amp;gt;/&amp;lt;types&amp;gt;/&amp;lt;id&amp;gt; &amp;lt;option&amp;gt; &amp;lt;JSON&amp;gt;  - indices: list of indices separated by commas (depends on the command)  - types: list of document types separated by commas (depends on the command)Commands:  - search /indices/types &amp;lt;query&amp;gt;    . indices and types can be omitted (at least, you have to provide &amp;#39;/&amp;#39;)    . a query is either a JSON-formatted query, nor a lucene query  - size &amp;lt;value&amp;gt;    . defines the size of the result set (default value is in the config)    . if used, this command must be declared before a search command  - count /indices/types &amp;lt;query&amp;gt;    . same comments as for the search  - get /index/type/id  - delete /index/type/id  - index /index/type/id &amp;lt;json-formatted document&amp;gt;    . the id can be omitted, elasticsearch will generate oneTip : Use ( Ctrl + . ) for autocompletion.GetWith the get command, you can find a document by id. The result is a JSON document.%elasticsearchget /index/type/idExample:SearchWith the search command, you can send a search query to Elasticsearch. There are two formats of query:You can provide a JSON-formatted query, that is exactly what you provide when you use the REST API of Elasticsearch.See Elasticsearch search API reference document for more details about the content of the search queries.You can also provide the content of a query_string.This is a shortcut to a query like that: { &amp;quot;query&amp;quot;: { &amp;quot;query_string&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;__HERE YOUR QUERY__&amp;quot;, &amp;quot;analyze_wildcard&amp;quot;: true } } }See Elasticsearch query string syntax for more details about the content of such a query.%elasticsearchsearch /index1,index2,.../type1,type2,...  &amp;lt;JSON document containing the query or query_string elements&amp;gt;If you want to modify the size of the result set, you can add a line that is setting the size, before your search command.%elasticsearchsize 50search /index1,index2,.../type1,type2,...  &amp;lt;JSON document containing the query or query_string elements&amp;gt;A search query can also contain aggregations. If there is at least one aggregation, the result of the first aggregation is shown, otherwise, you get the search hits.Examples:With a JSON query:%elasticsearchsearch / { &amp;quot;query&amp;quot;: { &amp;quot;match_all&amp;quot;: { } } }%elasticsearchsearch /logs { &amp;quot;query&amp;quot;: { &amp;quot;query_string&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;request.method:GET AND status:200&amp;quot; } } }%elasticsearchsearch /logs { &amp;quot;aggs&amp;quot;: {&amp;quot;content_length_stats&amp;quot;: {  &amp;quot;extended_stats&amp;quot;: {    &amp;quot;field&amp;quot;: &amp;quot;content_length&amp;quot;  }}} }With query_string elements:%elasticsearchsearch /logs request.method:GET AND status:200%elasticsearchsearch /logs (404 AND (POST OR DELETE))Important : a document in Elasticsearch is a JSON document, so it is hierarchical, not flat as a row in a SQL table.For the Elastic interpreter, the result of a search query is flattened.Suppose we have a JSON document:{  &amp;quot;date&amp;quot;: &amp;quot;2015-12-08T21:03:13.588Z&amp;quot;,  &amp;quot;request&amp;quot;: {    &amp;quot;method&amp;quot;: &amp;quot;GET&amp;quot;,    &amp;quot;url&amp;quot;: &amp;quot;/zeppelin/4cd001cd-c517-4fa9-b8e5-a06b8f4056c4&amp;quot;,    &amp;quot;headers&amp;quot;: [ &amp;quot;Accept: *.*&amp;quot;, &amp;quot;Host: apache.org&amp;quot;]  },  &amp;quot;status&amp;quot;: &amp;quot;403&amp;quot;,  &amp;quot;content_length&amp;quot;: 1234}The data will be flattened like this:content_lengthdaterequest.headers[0]request.headers[1]request.methodrequest.urlstatus12342015-12-08T21:03:13.588ZAccept: *.*Host: apache.orgGET/zeppelin/4cd001cd-c517-4fa9-b8e5-a06b8f4056c4403Examples:With a table containing the results:You can also use a predefined diagram:With a JSON query:With a JSON query containing a fields parameter (for filtering the fields in the response): in this case, all the fields values in the response are arrays, so, after flattening the result, the format of all the field names is field_name[x]With a query string:With a query containing a multi-value metric aggregation:With a query containing a multi-bucket aggregation:CountWith the count command, you can count documents available in some indices and types. You can also provide a query.%elasticsearchcount /index1,index2,.../type1,type2,... &amp;lt;JSON document containing the query OR a query string&amp;gt;Examples:Without query:With a query:IndexWith the index command, you can insert/update a document in Elasticsearch.%elasticsearchindex /index/type/id &amp;lt;JSON document&amp;gt;%elasticsearchindex /index/type &amp;lt;JSON document&amp;gt;DeleteWith the delete command, you can delete a document.%elasticsearchdelete /index/type/idApply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your queries. You can use both the text input and select form parameterization features.%elasticsearchsize ${limit=10}search /index/type { &amp;quot;query&amp;quot;: { &amp;quot;match_all&amp;quot;: { } } }",
      "url": " /interpreter/elasticsearch.html",
      "group": "interpreter",
      "excerpt": "Elasticsearch is a highly scalable open-source full-text search and analytics engine."
    }
    ,



    "/interpreter/flink.html": {
      "title": "Flink Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Flink interpreter for Apache ZeppelinOverviewApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.In Zeppelin 0.9, we refactor the Flink interpreter in Zeppelin to support the latest version of Flink. Only Flink 1.10+ is supported, old versions of flink won&amp;#39;t work.Apache Flink is supported in Zeppelin with the Flink interpreter group which consists of the five interpreters listed below.      Name    Class    Description        %flink    FlinkInterpreter    Creates ExecutionEnvironment/StreamExecutionEnvironment/BatchTableEnvironment/StreamTableEnvironment and provides a Scala environment        %flink.pyflink    PyFlinkInterpreter    Provides a python environment        %flink.ipyflink    IPyFlinkInterpreter    Provides an ipython environment        %flink.ssql    FlinkStreamSqlInterpreter    Provides a stream sql environment        %flink.bsql    FlinkBatchSqlInterpreter    Provides a batch sql environment  Main Features      Feature    Description        Support multiple versions of Flink    You can run different versions of Flink in one Zeppelin instance        Support multiple versions of Scala    You can run different Scala versions of Flink in on Zeppelin instance        Support multiple languages    Scala, Python, SQL are supported, besides that you can also collaborate across languages, e.g. you can write Scala UDF and use it in PyFlink        Support multiple execution modes    Local | Remote | Yarn | Yarn Application        Support Hive    Hive catalog is supported        Interactive development    Interactive development user experience increase your productivity        Enhancement on Flink SQL    * Support both streaming sql and batch sql in one notebook * Support sql comment (single line comment/multiple line comment) * Support advanced configuration (jobName, parallelism) * Support multiple insert statements         Multi-tenancy    Multiple user can work in one Zeppelin instance without affecting each other.        Rest API Support    You can not only submit Flink job via Zeppelin notebook UI, but also can do that via its rest api (You can use Zeppelin as Flink job server).  Play Flink in Zeppelin dockerFor beginner, we would suggest you to play Flink in Zeppelin docker. First you need to download Flink, because there&amp;#39;s no Flink binary distribution shipped with Zeppelin. e.g. Here we download Flink 1.12.2 to/mnt/disk1/flink-1.12.2,and we mount it to Zeppelin docker container and run the following command to start Zeppelin docker.docker run -u $(id -u) -p 8080:8080 -p 8081:8081 --rm -v /mnt/disk1/flink-1.12.2:/opt/flink -e FLINK_HOME=/opt/flink  --name zeppelin apache/zeppelin:0.10.0After running the above command, you can open http://localhost:8080 to play Flink in Zeppelin. We only verify the flink local mode in Zeppelin docker, other modes may not due to network issues.-p 8081:8081 is to expose Flink web ui, so that you can access Flink web ui via http://localhost:8081.Here&amp;#39;s screenshot of running note Flink Tutorial/5. Streaming Data AnalyticsYou can also mount notebook folder to replace the built-in zeppelin tutorial notebook. e.g. Here&amp;#39;s a repo of Flink sql cookbook on Zeppelin: https://github.com/zjffdu/flink-sql-cookbook-on-zeppelin/You can clone this repo and mount it to docker,docker run -u $(id -u) -p 8080:8080 --rm -v /mnt/disk1/flink-sql-cookbook-on-zeppelin:/notebook -v /mnt/disk1/flink-1.12.2:/opt/flink -e FLINK_HOME=/opt/flink  -e ZEPPELIN_NOTEBOOK_DIR=&amp;#39;/notebook&amp;#39; --name zeppelin apache/zeppelin:0.10.0PrerequisitesDownload Flink 1.10 or afterwards (Scala 2.11 &amp;amp; 2.12 are both supported)Flink on Zeppelin ArchitectureThe above diagram is the architecture of Flink on Zeppelin. Flink interpreter on the left side is actually a Flink client which is responsible for compiling and managing Flink job lifecycle, such as submit, cancel job, monitoring job progress and so on. The Flink cluster on the right side is the place where executing Flink job. It could be a MiniCluster (local mode), Standalone cluster (remote mode), Yarn session cluster (yarn mode) or Yarn application session cluster (yarn-application mode)There are 2 important components in Flink interpreter: Scala shell &amp;amp; Python shellScala shell is the entry point of Flink interpreter, it would create all the entry points of Flink program, such as ExecutionEnvironment，StreamExecutionEnvironment and TableEnvironment. Scala shell is responsible for compiling and running Scala code and sql.Python shell is the entry point of PyFlink, it is responsible for compiling and running Python code.ConfigurationThe Flink interpreter can be configured with properties provided by Zeppelin (as following table).You can also add and set other Flink properties which are not listed in the table. For a list of additional properties, refer to Flink Available Properties.      Property    Default    Description        FLINK_HOME        Location of Flink installation. It is must be specified, otherwise you can not use Flink in Zeppelin        HADOOP_CONF_DIR        Location of hadoop conf, this is must be set if running in yarn mode        HIVE_CONF_DIR        Location of hive conf, this is must be set if you want to connect to hive metastore        flink.execution.mode    local    Execution mode of Flink, e.g. local | remote | yarn | yarn-application        flink.execution.remote.host        Host name of running JobManager. Only used for remote mode        flink.execution.remote.port        Port of running JobManager. Only used for remote mode        jobmanager.memory.process.size    1024m    Total memory size of JobManager, e.g. 1024m. It is official Flink property        taskmanager.memory.process.size    1024m    Total memory size of TaskManager, e.g. 1024m. It is official Flink property        taskmanager.numberOfTaskSlots    1    Number of slot per TaskManager        local.number-taskmanager    4    Total number of TaskManagers in local mode        yarn.application.name    Zeppelin Flink Session    Yarn app name        yarn.application.queue    default    queue name of yarn app        zeppelin.flink.uiWebUrl        User specified Flink JobManager url, it could be used in remote mode where Flink cluster is already started, or could be used as url template, e.g. https://knox-server:8443/gateway/cluster-topo/yarn/proxy/{{applicationId}}/ where {{applicationId}} is placeholder of yarn app id        zeppelin.flink.run.asLoginUser    true    Whether run Flink job as the Zeppelin login user, it is only applied when running Flink job in hadoop yarn cluster and shiro is enabled         flink.udf.jars        Flink udf jars (comma separated), Zeppelin will register udf in these jars automatically for user. These udf jars could be either local files or hdfs files if you have hadoop installed. The udf name is the class name.        flink.udf.jars.packages        Packages (comma separated) that would be searched for the udf defined in flink.udf.jars. Specifying this can reduce the number of classes to scan, otherwise all the classes in udf jar will be scanned.        flink.execution.jars        Additional user jars (comma separated), these jars could be either local files or hdfs files if you have hadoop installed. It can be used to specify Flink connector jars or udf jars (no udf class auto-registration like flink.udf.jars)        flink.execution.packages        Additional user packages (comma separated), e.g. org.apache.flink:flink-json:1.10.0        zeppelin.flink.concurrentBatchSql.max    10    Max concurrent sql of Batch Sql (%flink.bsql)        zeppelin.flink.concurrentStreamSql.max    10    Max concurrent sql of Stream Sql (%flink.ssql)        zeppelin.pyflink.python    python    Python binary executable for PyFlink        table.exec.resource.default-parallelism    1    Default parallelism for Flink sql job        zeppelin.flink.scala.color    true    Whether display Scala shell output in colorful format      zeppelin.flink.enableHive    false    Whether enable hive        zeppelin.flink.hive.version    2.3.4    Hive version that you would like to connect        zeppelin.flink.module.enableHive    false    Whether enable hive module, hive udf take precedence over Flink udf if hive module is enabled.        zeppelin.flink.maxResult    1000    max number of row returned by sql interpreter        zeppelin.flink.job.check_interval    1000    Check interval (in milliseconds) to check Flink job progress        flink.interpreter.close.shutdown_cluster    true    Whether shutdown Flink cluster when closing interpreter        zeppelin.interpreter.close.cancel_job    true    Whether cancel Flink job when closing interpreter  Interpreter Binding ModeThe default interpreter binding mode is globally shared. That means all notes share the same Flink interpreter which means they share the same Flink cluster.In practice, we would recommend you to use isolated per note which means each note has own Flink interpreter without affecting each other (Each one has his own Flink cluster). Execution ModeFlink in Zeppelin supports 4 execution modes (flink.execution.mode):LocalRemoteYarnYarn ApplicationLocal ModeRunning Flink in local mode will start a MiniCluster in local JVM. By default, the local MiniCluster use port 8081, so make sure this port is available in your machine,otherwise you can configure rest.port to specify another port. You can also specify local.number-taskmanager and flink.tm.slot to customize the number of TM and number of slots per TM.Because by default it is only 4 TM with 1 slot in this MiniCluster which may not be enough for some cases.Remote ModeRunning Flink in remote mode will connect to an existing Flink cluster which could be standalone cluster or yarn session cluster. Besides specifying flink.execution.mode to be remote, you also need to specifyflink.execution.remote.host and flink.execution.remote.port to point to Flink job manager&amp;#39;s rest api address.Yarn ModeIn order to run Flink in Yarn mode, you need to make the following settings:Set flink.execution.mode to be yarnSet HADOOP_CONF_DIR in Flink&amp;#39;s interpreter setting or zeppelin-env.sh.Make sure hadoop command is on your PATH. Because internally Flink will call command hadoop classpath and load all the hadoop related jars in the Flink interpreter processIn this mode, Zeppelin would launch a Flink yarn session cluster for you and destroy it when you shutdown your Flink interpreter.Yarn Application ModeIn the above yarn mode, there will be a separated Flink interpreter process on the Zeppelin server host. However, this may run out of resources when there are too many interpreter processes.So in practise, we would recommend you to use yarn application mode if you are using Flink 1.11 or afterwards (yarn application mode is only supported after Flink 1.11). In this mode Flink interpreter runs in the JobManager which is in yarn container.In order to run Flink in yarn application mode, you need to make the following settings:Set flink.execution.mode to be yarn-applicationSet HADOOP_CONF_DIR in Flink&amp;#39;s interpreter setting or zeppelin-env.sh.Make sure hadoop command is on your PATH. Because internally flink will call command hadoop classpath and load all the hadoop related jars in Flink interpreter processFlink ScalaScala is the default language of Flink on Zeppelin（%flink), and it is also the entry point of Flink interpreter. Underneath Flink interpreter will create Scala shell which would create several built-in variables, including ExecutionEnvironment，StreamExecutionEnvironment and so on. So don&amp;#39;t create these Flink environment variables again, otherwise you might hit weird issues. The Scala code you write in Zeppelin will be submitted to this Scala shell.Here are the builtin variables created in Flink Scala shell.senv (StreamExecutionEnvironment),benv (ExecutionEnvironment)stenv (StreamTableEnvironment for blink planner)btenv (BatchTableEnvironment for blink planner)stenv_2 (StreamTableEnvironment for flink planner)btenv_2 (BatchTableEnvironment for flink planner)z  (ZeppelinContext)Blink/Flink PlannerThere are 2 planners supported by Flink SQL: flink &amp;amp; blink.If you want to use DataSet api, and convert it to Flink table then please use flink planner (btenv_2 and stenv_2).In other cases, we would always recommend you to use blink planner. This is also what Flink batch/streaming sql interpreter use (%flink.bsql &amp;amp; %flink.ssql)Check this page for the difference between flink planner and blink planner.Stream WordCount ExampleYou can write whatever Scala code in Zeppelin. e.g. in the following example, we write a classical streaming wordcount example.Code CompletionYou can type tab for code completion.ZeppelinContextZeppelinContext provides some additional functions and utilities.See Zeppelin-Context for more details. For Flink interpreter, you can use z to display Flink Dataset/Table. e.g. you can use z.show to display DataSet, Batch Table, Stream Table.z.show(DataSet)z.show(Batch Table)z.show(Stream Table)Flink SQLIn Zeppelin, there are 2 kinds of Flink sql interpreter you can use%flink.ssqlStreaming Sql interpreter which launch Flink streaming job via StreamTableEnvironment%flink.bsqlBatch Sql interpreter which launch Flink batch job via BatchTableEnvironmentFlink Sql interpreter in Zeppelin is equal to Flink Sql-client + many other enhancement features.Enhancement SQL FeaturesSupport batch SQL and streaming sql together.In Flink Sql-client, either you run streaming sql or run batch sql in one session. You can not run them together. But in Zeppelin, you can do that. %flink.ssql is used for running streaming sql, while %flink.bsql is used for running batch sql. Batch/Streaming Flink jobs run in the same Flink session cluster.Support multiple statementsYou can write multiple sql statements in one paragraph, each sql statement is separated by semicolon. Comment support2 kinds of sql comments are supported in Zeppelin:Single line comment start with --Multiple line comment around with /* */Job parallelism settingYou can set the sql parallelism via paragraph local property: parallelismSupport multiple insertSometimes you have multiple insert statements which read the same source, but write to different sinks. By default, each insert statement would launch a separated Flink job, but you can set paragraph local property: runAsOne to be true to run them in one single Flink job.Set job nameYou can set Flink job name for insert statement via setting paragraph local property: jobName. To be noticed, you can only set job name for insert statement. Select statement is not supported yet. And this kind of setting only works for single insert statement. It doesn&amp;#39;t work for multiple insert we mentioned above.Streaming Data VisualizationZeppelin can visualize the select sql result of Flink streaming job. Overall it supports 3 modes:SingleUpdateAppendSingle ModeSingle mode is for the case when the result of sql statement is always one row, such as the following example. The output format is HTML, and you can specify paragraph local property template for the final output content template. You can use {i} as placeholder for the ith column of result.Update ModeUpdate mode is suitable for the case when the output is more than one rows, and will always be updated continuously. Here’s one example where we use group by.Append ModeAppend mode is suitable for the scenario where output data is always appended. E.g. the following example which use tumble window.PyFlinkPyFlink is Python entry point of  Flink on Zeppelin, internally Flink interpreter will create Python shell whichwould create Flink&amp;#39;s environment variables (including ExecutionEnvironment, StreamExecutionEnvironment and so on).To be noticed, the java environment behind Pyflink is created in Scala shell.That means underneath Scala shell and Python shell share the same environment.These are variables created in Python shell.s_env    (StreamExecutionEnvironment),b_env     (ExecutionEnvironment)st_env   (StreamTableEnvironment for blink planner)bt_env   (BatchTableEnvironment for blink planner)st_env_2   (StreamTableEnvironment for flink planner)bt_env_2   (BatchTableEnvironment for flink planner)Configure PyFlinkThere are 3 things you need to configure to make Pyflink work in Zeppelin.Install pyflinke.g. ( pip install apache-flink==1.11.1 ).If you need to use Pyflink udf, then you to install pyflink on all the task manager nodes. That means if you are using yarn, then all the yarn nodes need to install pyflink.Copy python folder under ${FLINK_HOME}/opt to ${FLINK_HOME/lib.Set zeppelin.pyflink.python as the python executable path. By default, it is the python in PATH. In case you have multiple versions of python installed, you need to configure zeppelin.pyflink.python as the python version you want to use.How to use PyFlinkThere are 2 ways to use PyFlink in Zeppelin%flink.pyflink%flink.ipyflink%flink.pyflink is much simple and easy,  you don&amp;#39;t need to do anything except the above setting,but its function is also limited. We suggest you to use %flink.ipyflink which provides almost the same user experience like jupyter.Configure IPyFlinkIf you don&amp;#39;t have anaconda installed, then you need to install the following 3 libraries.pip install jupyterpip install grpciopip install protobufIf you have anaconda installed, then you only need to install following 2 libraries.pip install grpciopip install protobufZeppelinContext is also available in PyFlink, you can use it almost the same as in Flink Scala.Check the Python doc for more features of IPython.Third party dependenciesIt is very common to have third party dependencies when you write Flink job in whatever languages (Scala, Python, Sql).It is very easy to add dependencies in IDE (e.g. add dependency in pom.xml),but how can you do that in Zeppelin ? Mainly there are 2 settings you can use to add third party dependenciesflink.execution.packagesflink.execution.jarsflink.execution.packagesThis is the recommended way of adding dependencies. Its implementation is the same as addingdependencies in pom.xml. Underneath it would download all the packages and its transitive dependenciesfrom maven repository, then put them on the classpath. Here&amp;#39;s one example of how to add kafka connector of Flink 1.10 via inline configuration.%flink.confflink.execution.packages  org.apache.flink:flink-connector-kafka_2.11:1.10.0,org.apache.flink:flink-connector-kafka-base_2.11:1.10.0,org.apache.flink:flink-json:1.10.0The format is artifactGroup:artifactId:version, if you have multiple packages,then separate them with comma. flink.execution.packages requires internet accessible.So if you can not access internet, you need to use flink.execution.jars instead.flink.execution.jarsIf your Zeppelin machine can not access internet or your dependencies are not deployed to maven repository,then you can use flink.execution.jars to specify the jar files you depend on (each jar file is separated with comma)Here&amp;#39;s one example of how to add kafka dependencies(including kafka connector and its transitive dependencies) via flink.execution.jars%flink.confflink.execution.jars /usr/lib/flink-kafka/target/flink-kafka-1.0-SNAPSHOT.jarFlink UDFThere are 4 ways you can define UDF in Zeppelin.Write Scala UDFWrite PyFlink UDFCreate UDF via SQLConfigure udf jar via flink.udf.jarsScala UDF%flinkclass ScalaUpper extends ScalarFunction {  def eval(str: String) = str.toUpperCase}btenv.registerFunction(&amp;quot;scala_upper&amp;quot;, new ScalaUpper())It is very straightforward to define scala udf almost the same as what you do in IDE.After creating udf class, you need to register it via btenv.You can also register it via stenv which share the same Catalog with btenv.Python UDF%flink.pyflinkclass PythonUpper(ScalarFunction):  def eval(self, s):    return s.upper()bt_env.register_function(&amp;quot;python_upper&amp;quot;, udf(PythonUpper(), DataTypes.STRING(), DataTypes.STRING()))It is also very straightforward to define Python udf almost the same as what you do in IDE. After creating udf class, you need to register it via bt_env. You can also register it via st_env which share the same Catalog with bt_env.UDF via SQLSome simple udf can be written in Zeppelin. But if the udf logic is very complicated, then it is better to write it in IDE, then register it in Zeppelin as following%flink.ssqlCREATE FUNCTION myupper AS &amp;#39;org.apache.zeppelin.flink.udf.JavaUpper&amp;#39;;But this kind of approach requires the udf jar must be on CLASSPATH, so you need to configure flink.execution.jars to include this udf jar on CLASSPATH, such as following:%flink.confflink.execution.jars /usr/lib/flink-udf-1.0-SNAPSHOT.jarflink.udf.jarsThe above 3 approaches all have some limitations:It is suitable to write simple Scala udf or Python udf in Zeppelin, but not suitable to write very complicated udf in Zeppelin. Because notebook doesn&amp;#39;t provide advanced features compared to IDE, such as package management, code navigation and etc.It is not easy to share the udf between notes or users, you have to run the paragraph of defining udf in each flink interpreter.So when you have many udfs or udf logic is very complicated and you don&amp;#39;t want to register them by yourself every time, then you can use flink.udf.jarsStep 1. Create a udf project in your IDE, write your udf there.Step 2. Set flink.udf.jars to point to the udf jar you build from your udf projectFor example,%flink.confflink.execution.jars /usr/lib/flink-udf-1.0-SNAPSHOT.jarZeppelin would scan this jar, find out all the udf classes and then register them automatically for you. The udf name is the class name. For example, here&amp;#39;s the output of show functions after specifing the above udf jars in flink.udf.jarsBy default, Zeppelin would scan all the classes in this jar, so it would be pretty slow if your jar is very big specially when your udf jar has other dependencies. So in this case we would recommend you to specify flink.udf.jars.packages to specify the package to scan,this can reduce the number of classes to scan and make the udf detection much faster.How to use HiveIn order to use Hive in Flink, you have to make the following settings.Set zeppelin.flink.enableHive to be trueSet zeppelin.flink.hive.version to be the hive version you are using.Set HIVE_CONF_DIR to be the location where hive-site.xml is located. Make sure hive metastore is started and you have configured hive.metastore.uris in hive-site.xmlCopy the following dependencies to the lib folder of flink installation.flink-connector-hive_2.11–*.jarflink-hadoop-compatibility_2.11–*.jarhive-exec-2.x.jar (for hive 1.x, you need to copy hive-exec-1.x.jar, hive-metastore-1.x.jar, libfb303–0.9.2.jar and libthrift-0.9.2.jar)Paragraph local propertiesIn the section of Streaming Data Visualization, we demonstrate the different visualization type via paragraph local properties: type. In this section, we will list and explain all the supported local properties in Flink interpreter.      Property    Default    Description        type        Used in %flink.ssql to specify the streaming visualization type (single, update, append)        refreshInterval    3000    Used in `%flink.ssql` to specify frontend refresh interval for streaming data visualization.        template    {0}    Used in `%flink.ssql` to specify html template for `single` type of streaming data visualization, And you can use `{i}` as placeholder for the {i}th column of the result.        parallelism        Used in %flink.ssql &amp; %flink.bsql to specify the flink sql job parallelism        maxParallelism        Used in %flink.ssql &amp; %flink.bsql to specify the flink sql job max parallelism in case you want to change parallelism later. For more details, refer this [link](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/parallel.html#setting-the-maximum-parallelism)         savepointDir        If you specify it, then when you cancel your flink job in Zeppelin, it would also do savepoint and store state in this directory. And when you resume your job, it would resume from this savepoint.        execution.savepoint.path        When you resume your job, it would resume from this savepoint path.        resumeFromSavepoint        Resume flink job from savepoint if you specify savepointDir.        resumeFromLatestCheckpoint        Resume flink job from latest checkpoint if you enable checkpoint.        runAsOne    false    All the insert into sql will run in a single flink job if this is true.  Tutorial NotesZeppelin is shipped with several Flink tutorial notes which may be helpful for you. You can check for more features in the tutorial notes.CommunityJoin our community to discuss with others.",
      "url": " /interpreter/flink.html",
      "group": "interpreter",
      "excerpt": "Apache Flink is an open source platform for distributed stream and batch data processing."
    }
    ,



    "/interpreter/geode.html": {
      "title": "Geode/Gemfire OQL Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Geode/Gemfire OQL Interpreter for Apache ZeppelinOverview      Name    Class    Description        %geode.oql    GeodeOqlInterpreter    Provides OQL environment for Apache Geode  This interpreter supports the Geode Object Query Language (OQL).With the OQL-based querying language:You can query on any arbitrary objectYou can navigate object collectionsYou can invoke methods and access the behavior of objectsData mapping is supportedYou are not required to declare types. Since you do not need type definitions, you can work across multiple languagesYou are not constrained by a schemaThis Video Tutorial illustrates some of the features provided by the Geode Interpreter.Create InterpreterBy default Zeppelin creates one Geode/OQL instance. You can remove it or create more instances.Multiple Geode instances can be created, each configured to the same or different backend Geode cluster. But over time a  Notebook can have only one Geode interpreter instance bound. That means you cannot connect to different Geode clusters in the same Notebook. This is a known Zeppelin limitation.To create new Geode instance open the Interpreter section and click the +Create button. Pick a Name of your choice and from the Interpreter drop-down select geode.Then follow the configuration instructions and Save the new instance.Note: The Name of the instance is used only to distinguish the instances while binding them to the Notebook. The Name is irrelevant inside the Notebook. In the Notebook you must use %geode.oql tag.Bind to NotebookIn the Notebook click on the settings icon in the top right corner. The select/deselect the interpreters to be bound with the Notebook.ConfigurationYou can modify the configuration of the Geode from the Interpreter section. The Geode interpreter expresses the following properties:      Property Name    Description    Default Value        geode.locator.host    The Geode Locator Host    localhost        geode.locator.port    The Geode Locator Port    10334        geode.max.result    Max number of OQL result to display to prevent the browser overload    1000  How to useTip 1: Use (CTRL + .) for OQL auto-completion.Tip 2: Always start the paragraphs with the full %geode.oql prefix tag! The short notation: %geode would still be able run the OQL queries but the syntax highlighting and the auto-completions will be disabled.Create / Destroy RegionsThe OQL specification does not support  Geode Regions mutation operations. To create/destroy regions one should use the GFSH shell tool instead. In the following it is assumed that the GFSH is colocated with Zeppelin server.%shsource /etc/geode/conf/geode-env.shgfsh &amp;lt;&amp;lt; EOF connect --locator=ambari.localdomain[10334] destroy region --name=/regionEmployee destroy region --name=/regionCompany create region --name=regionEmployee --type=REPLICATE create region --name=regionCompany --type=REPLICATE exit;EOFAbove snippet re-creates two regions: regionEmployee and regionCompany. Note that you have to explicitly specify the locator host and port. The values should match those you have used in the Geode Interpreter configuration. Comprehensive list of GFSH Commands by Functional Area.Basic OQL%geode.oqlSELECT count(*) FROM /regionEmployeeOQL IN and SET filters:%geode.oqlSELECT * FROM /regionEmployeeWHERE companyId IN SET(2) OR lastName IN SET(&amp;#39;Tzolov13&amp;#39;, &amp;#39;Tzolov73&amp;#39;)OQL JOIN operations%geode.oqlSELECT e.employeeId, e.firstName, e.lastName, c.id as companyId, c.companyName, c.addressFROM /regionEmployee e, /regionCompany cWHERE e.companyId = c.idBy default the QOL responses contain only the region entry values. To access the keys, query the EntrySet instead:%geode.oqlSELECT e.key, e.value.companyId, e.value.emailFROM /regionEmployee.entrySet eFollowing query will return the EntrySet value as a Blob:%geode.oqlSELECT e.key, e.value FROM /regionEmployee.entrySet eNote: You can have multiple queries in the same paragraph but only the result from the first is displayed. [1], [2].GFSH Commands From The ShellUse the Shell Interpreter (%sh) to run OQL commands form the command line:%shsource /etc/geode/conf/geode-env.shgfsh -e &amp;quot;connect&amp;quot; -e &amp;quot;list members&amp;quot;Apply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your OQL queries. You can use both the text input and select form parameterization features%geode.oqlSELECT * FROM /regionEmployee e WHERE e.employeeId &amp;gt; ${Id}Auto-completionThe Geode Interpreter provides a basic auto-completion functionality. On (Ctrl+.) it list the most relevant suggestions in a pop-up window.Geode REST APITo list the defined regions you can use the Geode REST API:http://&amp;lt;geode server hostname&amp;gt;phd1.localdomain:8484/gemfire-api/v1/{  &amp;quot;regions&amp;quot; : [{    &amp;quot;name&amp;quot; : &amp;quot;regionEmployee&amp;quot;,    &amp;quot;type&amp;quot; : &amp;quot;REPLICATE&amp;quot;,    &amp;quot;key-constraint&amp;quot; : null,    &amp;quot;value-constraint&amp;quot; : null  }, {    &amp;quot;name&amp;quot; : &amp;quot;regionCompany&amp;quot;,    &amp;quot;type&amp;quot; : &amp;quot;REPLICATE&amp;quot;,    &amp;quot;key-constraint&amp;quot; : null,    &amp;quot;value-constraint&amp;quot; : null  }]}To enable Geode REST API with JSON support add the following properties to geode.server.properties.file and restart:http-service-port=8484start-dev-rest-api=true",
      "url": " /interpreter/geode.html",
      "group": "interpreter",
      "excerpt": "Apache Geode (incubating) provides a database-like consistency model, reliable transaction processing and a shared-nothing architecture to maintain very low latency performance with high concurrency processing."
    }
    ,



    "/interpreter/groovy.html": {
      "title": "Apache Groovy Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Groovy Interpreter for Apache ZeppelinSamples%groovy//get a parameter defined as z.angularBind(&amp;#39;ngSearchParam&amp;#39;, value, &amp;#39;paragraph_id&amp;#39;)//g is a context object for groovy to avoid mix with z objectdef param = g.angular(&amp;#39;ngSearchParam&amp;#39;)//send request https://www.googleapis.com/customsearch/v1?q=ngSearchParam_valuedef r = HTTP.get(  //assume you defined the groovy interpreter property  //   `search_baseurl`=&amp;#39;https://www.googleapis.com/customsearch/v1&amp;#39;  //in groovy object o.getProperty(&amp;#39;A&amp;#39;) == o.&amp;#39;A&amp;#39; == o.A == o[&amp;#39;A&amp;#39;]  url : g.search_baseurl,  query: [ q: param ],  headers: [    &amp;#39;Accept&amp;#39;:&amp;#39;application/json&amp;#39;,    //&amp;#39;Authorization:&amp;#39; : g.getProperty(&amp;#39;search_auth&amp;#39;),  ],  ssl : g.getProperty(&amp;#39;search_ssl&amp;#39;) // assume groovy interpreter property search_ssl = HTTP.getNaiveSSLContext())//check response codeif( r.response.code==200 ) {  g.html().with{     //g.html() renders %angular to output and returns groovy.xml.MarkupBuilder    h2(&amp;quot;the response ${r.response.code}&amp;quot;)    span( r.response.body )    h2(&amp;quot;headers&amp;quot;)    pre( r.response.headers.join(&amp;#39;n&amp;#39;) )  }} else {  //just to show that it&amp;#39;s possible to use println with multiline groovy string to render output  println(&amp;quot;&amp;quot;&amp;quot;%angular    &amp;lt;script&amp;gt; alert (&amp;quot;code=${r.response.code} n msg=${r.response.message}&amp;quot;) &amp;lt;/script&amp;gt;  &amp;quot;&amp;quot;&amp;quot;)}%groovy//renders a table with headers a, b, c  and two rowsg.table(  [    [&amp;#39;a&amp;#39;,&amp;#39;b&amp;#39;,&amp;#39;c&amp;#39;],    [&amp;#39;a1&amp;#39;,&amp;#39;b1&amp;#39;,&amp;#39;c1&amp;#39;],    [&amp;#39;a2&amp;#39;,&amp;#39;b2&amp;#39;,&amp;#39;c2&amp;#39;],  ])the g objectg.angular(String name)Returns angular object by name. Look up notebook scope first and then global scope.g.angularBind(String name, Object value)Assign a new value into angular object namejava.util.Properties g.getProperties()returns all properties defined for this interpreterString g.getProperty(&amp;#39;PROPERTY_NAME&amp;#39;) g.PROPERTY_NAMEg.&amp;#39;PROPERTY_NAME&amp;#39;g[&amp;#39;PROPERTY_NAME&amp;#39;]g.getProperties().getProperty(&amp;#39;PROPERTY_NAME&amp;#39;)All above the accessor to named property defined in groovy interpreter.In this case with name PROPERTY_NAMEgroovy.xml.MarkupBuilder g.html()Starts or continues rendering of %angular to output and returns groovy.xml.MarkupBuilderMarkupBuilder is usefull to generate html (xml)void g.table(obj)starts or continues rendering table rows.obj:  List(rows) of List(columns) where first line is a header g.input(name, value )Creates text input with value specified. The parameter value is optional.g.select(name, default, Map&amp;lt;Object, String&amp;gt; options)Creates select input with defined options. The parameter default is optional.g.select(&amp;#39;sex&amp;#39;, &amp;#39;m&amp;#39;, [&amp;#39;m&amp;#39;:&amp;#39;man&amp;#39;, &amp;#39;w&amp;#39;:&amp;#39;woman&amp;#39;])g.checkbox(name, Collection checked, Map&amp;lt;Object, String&amp;gt; options)Creates checkbox input.g.get(name, default)Returns interpreter-based variable. Visibility depends on interpreter scope. The parameter default is optional.g.put(name, value)Stores new value into interpreter-based variable. Visibility depends on interpreter scope.",
      "url": " /interpreter/groovy.html",
      "group": "interpreter",
      "excerpt": "Apache Groovy is a powerful, optionally typed and dynamic language, with static-typing and static compilation capabilities, for the Java platform aimed at improving developer productivity thanks to a concise, familiar and easy to learn syntax."
    }
    ,



    "/interpreter/hazelcastjet.html": {
      "title": "Hazelcast Jet interpreter in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Hazelcast Jet interpreter for Apache ZeppelinOverviewHazelcast Jet is an open source application embeddable, distributed computing engine for In-Memory Streaming and Fast Batch Processing built on top of Hazelcast In-Memory Data Grid (IMDG). With Hazelcast IMDG providing storage functionality, Hazelcast Jet performs parallel execution to enable data-intensive applications to operate in near real-time.Why Hazelcast Jet?There are plenty of solutions which can solve some of these issues, so why choose Hazelcast Jet?When speed and simplicity is important.Hazelcast Jet gives you all the infrastructure you need to build a distributed data processing pipeline within one 10Mb Java JAR: processing, storage and clustering.As it is built on top of Hazelcast IMDG, Hazelcast Jet comes with in-memory operational storage that’s available out-of-the box. This storage is partitioned, distributed and replicated across the Hazelcast Jet cluster for capacity and resiliency. It can be used as an input data buffer, to publish the results of a Hazelcast Jet computation, to connect multiple Hazelcast Jet jobs or as a lookup cache for data enrichment.How to use the Hazelcast Jet interpreterBasically, you can write normal java code. You should write the main method inside a class because the interpreter invoke this main to execute the code. Unlike Zeppelin normal pattern, each paragraph is considered as a separate job, there isn&amp;#39;t any relation to any other paragraph. For example, a variable defined in one paragraph cannot be used in another one as each paragraph is a self contained java main class that is executed and the output returned to Zeppelin.The following is a demonstration of a word count example with the result represented as an Hazelcast IMDG IMap sink and displayed leveraging Zeppelin&amp;#39;s built in visualization using the utility method JavaInterpreterUtils.displayTableFromSimpleMap.%hazelcastjetimport com.hazelcast.jet.Jet;import com.hazelcast.jet.JetInstance;import com.hazelcast.jet.core.DAG;import com.hazelcast.jet.pipeline.Pipeline;import com.hazelcast.jet.pipeline.Sinks;import com.hazelcast.jet.pipeline.Sources;import java.util.List;import java.util.Map;import java.util.stream.Collectors;import org.apache.zeppelin.java.JavaInterpreterUtils;import static com.hazelcast.jet.Traversers.traverseArray;import static com.hazelcast.jet.aggregate.AggregateOperations.counting;import static com.hazelcast.jet.function.DistributedFunctions.wholeItem;public class DisplayTableFromSimpleMapExample {    public static void main(String[] args) {        // Create the specification of the computation pipeline. Note        // it&amp;#39;s a pure POJO: no instance of Jet needed to create it.        Pipeline p = Pipeline.create();        p.drawFrom(Sources.&amp;lt;String&amp;gt;list(&amp;quot;text&amp;quot;))                .flatMap(word -&amp;gt;                        traverseArray(word.toLowerCase().split(&amp;quot;W+&amp;quot;)))                .filter(word -&amp;gt; !word.isEmpty())                .groupingKey(wholeItem())                .aggregate(counting())                .drainTo(Sinks.map(&amp;quot;counts&amp;quot;));        // Start Jet, populate the input list        JetInstance jet = Jet.newJetInstance();        try {            List&amp;lt;String&amp;gt; text = jet.getList(&amp;quot;text&amp;quot;);            text.add(&amp;quot;hello world hello hello world&amp;quot;);            text.add(&amp;quot;world world hello world&amp;quot;);            // Perform the computation            jet.newJob(p).join();            // Diplay the results with Zeppelin %table            Map&amp;lt;String, Long&amp;gt; counts = jet.getMap(&amp;quot;counts&amp;quot;);            System.out.println(JavaInterpreterUtils.displayTableFromSimpleMap(&amp;quot;Word&amp;quot;,&amp;quot;Count&amp;quot;, counts));        } finally {            Jet.shutdownAll();        }    }}The following is a demonstration where the Hazelcast DAG (directed acyclic graph) is displayed as a graph leveraging Zeppelin&amp;#39;s built in visualization using the utility method HazelcastJetInterpreterUtils.displayNetworkFromDAG.This is particularly useful to understand how the high level Pipeline is then converted to the Jet’s low-level Core API. %hazelcastjetimport com.hazelcast.jet.pipeline.Pipeline;import com.hazelcast.jet.pipeline.Sinks;import com.hazelcast.jet.pipeline.Sources;import org.apache.zeppelin.hazelcastjet.HazelcastJetInterpreterUtils;import static com.hazelcast.jet.Traversers.traverseArray;import static com.hazelcast.jet.aggregate.AggregateOperations.counting;import static com.hazelcast.jet.function.DistributedFunctions.wholeItem;public class DisplayNetworkFromDAGExample {    public static void main(String[] args) {        // Create the specification of the computation pipeline. Note        // it&amp;#39;s a pure POJO: no instance of Jet needed to create it.        Pipeline p = Pipeline.create();        p.drawFrom(Sources.&amp;lt;String&amp;gt;list(&amp;quot;text&amp;quot;))                .flatMap(word -&amp;gt;                        traverseArray(word.toLowerCase().split(&amp;quot;W+&amp;quot;))).setName(&amp;quot;flat traversing&amp;quot;)                .filter(word -&amp;gt; !word.isEmpty())                .groupingKey(wholeItem())                .aggregate(counting())                .drainTo(Sinks.map(&amp;quot;counts&amp;quot;));        // Diplay the results with Zeppelin %network        System.out.println(HazelcastJetInterpreterUtils.displayNetworkFromDAG(p.toDag()));    }}Note- By clicking on a node of the graph, the node type is displayed (either Source, Sink or Transform). This is also visually represented with colors (Sources and Sinks are blue, Transforms are orange).- By clicking on an edge of the graph, the following details are shown: routing (UNICAST, PARTITIONED, ISOLATED, BROADCAST), distributed (true or false), priority (int).",
      "url": " /interpreter/hazelcastjet.html",
      "group": "interpreter",
      "excerpt": "Build and execture Hazelcast Jet computation jobs."
    }
    ,



    "/interpreter/hbase.html": {
      "title": "HBase Shell Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;HBase Shell Interpreter for Apache ZeppelinOverviewHBase Shell is a JRuby IRB client for Apache HBase. This interpreter provides all capabilities of Apache HBase shell within Apache Zeppelin. The interpreter assumes that Apache HBase client software has been installed and it can connect to the Apache HBase cluster from the machine on where Apache Zeppelin is installed.To get start with HBase, please see HBase Quickstart.HBase release supportedBy default, Zeppelin is built against HBase 1.0.x releases. To work with HBase 1.1.x releases, use the following build command:# HBase 1.1.4mvn clean package -DskipTests -Phadoop-2.6 -Dhadoop.version=2.6.0 -P build-distr -Dhbase.hbase.version=1.1.4 -Dhbase.hadoop.version=2.6.0To work with HBase 1.2.0+, use the following build command:# HBase 1.2.0mvn clean package -DskipTests -Phadoop-2.6 -Dhadoop.version=2.6.0 -P build-distr -Dhbase.hbase.version=1.2.0 -Dhbase.hadoop.version=2.6.0Configuration      Property    Default    Description        hbase.home    /usr/lib/hbase    Installation directory of HBase, defaults to HBASE_HOME in environment        hbase.ruby.sources    lib/ruby    Path to Ruby scripts relative to &#39;hbase.home&#39;        zeppelin.hbase.test.mode    false    Disable checks for unit and manual tests  If you want to connect to HBase running on a cluster, you&amp;#39;ll need to follow the next step.Export HBASE_HOMEIn conf/zeppelin-env.sh, export HBASE_HOME environment variable with your HBase installation path. This ensures hbase-site.xml can be loaded.For exampleexport HBASE_HOME=/usr/lib/hbaseor, when running with CDHexport HBASE_HOME=&amp;quot;/opt/cloudera/parcels/CDH/lib/hbase&amp;quot;You can optionally export HBASE_CONF_DIR instead of HBASE_HOME should you have custom HBase configurations.Enabling the HBase Shell InterpreterIn a notebook, to enable the HBase Shell interpreter, click the Gear icon and select HBase Shell.Using the HBase Shell InterpreterIn a paragraph, use %hbase to select the HBase Shell interpreter and then input all commands. To get the list of available commands, use help.%hbasehelpFor example, to create a table%hbasecreate &amp;#39;test&amp;#39;, &amp;#39;cf&amp;#39;And then to put data into that table%hbaseput &amp;#39;test&amp;#39;, &amp;#39;row1&amp;#39;, &amp;#39;cf:a&amp;#39;, &amp;#39;value1&amp;#39;For more information on all commands available, refer to HBase shell commands.",
      "url": " /interpreter/hbase.html",
      "group": "interpreter",
      "excerpt": "HBase Shell is a JRuby IRB client for Apache HBase. This interpreter provides all capabilities of Apache HBase shell within Apache Zeppelin."
    }
    ,



    "/interpreter/hdfs.html": {
      "title": "HDFS File System Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;HDFS File System Interpreter for Apache ZeppelinOverviewHadoop File System is a distributed, fault tolerant file system part of the hadoop project and is often used as storage for distributed processing engines like Hadoop MapReduce and Apache Spark or underlying file systems like Alluxio.Configuration      Property    Default    Description        hdfs.url    http://localhost:50070/webhdfs/v1/    The URL for WebHDFS        hdfs.user    hdfs    The WebHDFS user        hdfs.maxlength    1000    Maximum number of lines of results fetched  This interpreter connects to HDFS using the HTTP WebHDFS interface.It supports the basic shell file commands applied to HDFS, it currently only supports browsing.You can use ls [PATH] and ls -l [PATH] to list a directory. If the path is missing, then the current directory is listed.  ls  supports a -h flag for human readable file sizes.You can use cd [PATH] to change your current directory by giving a relative or an absolute path.You can invoke pwd to see your current directory.Tip : Use ( Ctrl + . ) for autocompletion.Create InterpreterIn a notebook, to enable the HDFS interpreter, click the Gear icon and select HDFS.WebHDFS REST APIYou can confirm that you&amp;#39;re able to access the WebHDFS API by running a curl command against the WebHDFS end point provided to the interpreter.Here is an example:$&amp;gt; curl &amp;quot;http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&amp;quot;",
      "url": " /interpreter/hdfs.html",
      "group": "interpreter",
      "excerpt": "Hadoop File System is a distributed, fault tolerant file system part of the hadoop project and is often used as storage for distributed processing engines like Hadoop MapReduce and Apache Spark or underlying file systems like Alluxio."
    }
    ,



    "/interpreter/hive.html": {
      "title": "Hive Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Hive Interpreter for Apache ZeppelinImportant NoticeHive Interpreter will be deprecated and merged into JDBC Interpreter. You can use Hive Interpreter by using JDBC Interpreter with same functionality. See the example below of settings and dependencies.Properties      Property    Value        hive.driver    org.apache.hive.jdbc.HiveDriver        hive.url    jdbc:hive2://localhost:10000        hive.user    hiveUser        hive.password    hivePassword  Dependencies      Artifact    Exclude        org.apache.hive:hive-jdbc:0.14.0            org.apache.hadoop:hadoop-common:2.6.0      Configuration      Property    Default    Description        default.driver    org.apache.hive.jdbc.HiveDriver    Class path of JDBC driver        default.url    jdbc:hive2://localhost:10000    Url for connection        default.user        ( Optional ) Username of the connection        default.password        ( Optional ) Password of the connection        default.xxx        ( Optional ) Other properties used by the driver        ${prefix}.driver        Driver class path of %hive(${prefix})         ${prefix}.url        Url of %hive(${prefix})         ${prefix}.user        ( Optional ) Username of the connection of %hive(${prefix})         ${prefix}.password        ( Optional ) Password of the connection of %hive(${prefix})         ${prefix}.xxx        ( Optional ) Other properties used by the driver of %hive(${prefix})   This interpreter provides multiple configuration with ${prefix}. User can set a multiple connection properties by this prefix. It can be used like %hive(${prefix}).OverviewThe Apache Hive ™ data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. At the same time this language also allows traditional map/reduce programmers to plug in their custom mappers and reducers when it is inconvenient or inefficient to express this logic in HiveQL.How to useBasically, you can use%hiveselect * from my_table;or%hive(etl)-- &amp;#39;etl&amp;#39; is a ${prefix}select * from my_table;You can also run multiple queries up to 10 by default. Changing these settings is not implemented yet.Apply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your queries. You can use both the text input and select form parameterization features.%hiveSELECT ${group_by}, count(*) as countFROM retail_demo.order_lineitems_pxfGROUP BY ${group_by=product_id,product_id|product_name|customer_id|store_id}ORDER BY count ${order=DESC,DESC|ASC}LIMIT ${limit=10};",
      "url": " /interpreter/hive.html",
      "group": "interpreter",
      "excerpt": "Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. At the same time this..."
    }
    ,



    "/interpreter/ignite.html": {
      "title": "Ignite Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Ignite Interpreter for Apache ZeppelinOverviewApache Ignite In-Memory Data Fabric is a high-performance, integrated and distributed in-memory platform for computing and transacting on large-scale data sets in real-time, orders of magnitude faster than possible with traditional disk-based or flash technologies.You can use Zeppelin to retrieve distributed data from cache using Ignite SQL interpreter. Moreover, Ignite interpreter allows you to execute any Scala code in cases when SQL doesn&amp;#39;t fit to your requirements. For example, you can populate data into your caches or execute distributed computations.Installing and Running Ignite exampleIn order to use Ignite interpreters, you may install Apache Ignite in some simple steps:Ignite provides examples only with source or binary release. Download Ignite source release or binary release whatever you want. But you must download Ignite as the same version of Zeppelin&amp;#39;s. If it is not, you can&amp;#39;t use scala code on Zeppelin. The supported Ignite version is specified in Supported Interpreter table for each Zeppelin release. If you&amp;#39;re using Zeppelin master branch, please see ignite.version in path/to/your-Zeppelin/ignite/pom.xml.Examples are shipped as a separate Maven project, so to start running you simply need to import provided &amp;lt;dest_dir&amp;gt;/apache-ignite-fabric-{version}-bin/examples/pom.xml file into your favourite IDE, such as Eclipse.In case of Eclipse, Eclipse -&amp;gt; File -&amp;gt; Import -&amp;gt; Existing Maven ProjectsSet examples directory path to Eclipse and select the pom.xml.Then start org.apache.ignite.examples.ExampleNodeStartup (or whatever you want) to run at least one or more ignite node. When you run example code, you may notice that the number of node is increase one by one.Tip. If you want to run Ignite examples on the cli not IDE, you can export executable Jar file from IDE. Then run it by using below command.nohup java -jar &amp;lt;/path/to/your Jar file name&amp;gt;Configuring Ignite InterpreterAt the &amp;quot;Interpreters&amp;quot; menu, you may edit Ignite interpreter or create new one. Zeppelin provides these properties for Ignite.      Property Name    value    Description        ignite.addresses    127.0.0.1:47500..47509    Coma separated list of Ignite cluster hosts. See Ignite Cluster Configuration section for more details.        ignite.clientMode    true    You can connect to the Ignite cluster as client or server node. See Ignite Clients vs. Servers section for details. Use true or false values in order to connect in client or server mode respectively.        ignite.config.url        Configuration URL. Overrides all other settings.        ignite.jdbc.url    jdbc:ignite:cfg://default-ignite-jdbc.xml    Ignite JDBC connection URL.        ignite.peerClassLoadingEnabled    true    Enables peer-class-loading. See Zero Deployment section for details. Use true or false values in order to enable or disable P2P class loading respectively.  How to useAfter configuring Ignite interpreter, create your own notebook. Then you can bind interpreters like below image.For more interpreter binding information see here.Ignite SQL interpreterIn order to execute SQL query, use %ignite.ignitesql prefix. Supposing you are running org.apache.ignite.examples.streaming.wordcount.StreamWords, then you can use &amp;quot;words&amp;quot; cache( Of course you have to specify this cache name to the Ignite interpreter setting section ignite.jdbc.url of Zeppelin ).For example, you can select top 10 words in the words cache using the following query%ignite.ignitesqlselect _val, count(_val) as cnt from String group by _val order by cnt desc limit 10As long as your Ignite version and Zeppelin Ignite version is same, you can also use scala code. Please check the Zeppelin Ignite version before you download your own Ignite.%igniteimport org.apache.ignite._import org.apache.ignite.cache.affinity._import org.apache.ignite.cache.query._import org.apache.ignite.configuration._import scala.collection.JavaConversions._val cache: IgniteCache[AffinityUuid, String] = ignite.cache(&amp;quot;words&amp;quot;)val qry = new SqlFieldsQuery(&amp;quot;select avg(cnt), min(cnt), max(cnt) from (select count(_val) as cnt from String group by _val)&amp;quot;, true)val res = cache.query(qry).getAll()collectionAsScalaIterable(res).foreach(println _)Apache Ignite also provides a guide docs for Zeppelin &amp;quot;Ignite with Apache Zeppelin&amp;quot;",
      "url": " /interpreter/ignite.html",
      "group": "interpreter",
      "excerpt": "Apache Ignite in-memory Data Fabric is a high-performance, integrated and distributed in-memory platform for computing and transacting on large-scale data sets in real-time, orders of magnitude faster than possible with traditional disk-based or flash technologies."
    }
    ,



    "/interpreter/influxdb.html": {
      "title": "InfluxDB Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;InfluxDB Interpreter for Apache ZeppelinOverviewInfluxDB  is an open-source time series database (TSDB) developed by InfluxData. It is written in Go and optimized for fast, high-availability storage and retrieval of time series data in fields such as operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics.This interpreter allows to perform queries in Flux Language in Zeppelin Notebook.NotesThis interpreter is compatible with InfluxDB 1.8+ and InfluxDB 2.0+ (v2 API, Flux language)Code complete and syntax highlighting is not supported for nowExample notebookConfiguration      Property    Default    Value        influxdb.url    http://localhost:9999    InfluxDB API connection url        influxdb.org    my-org    organization name, Organizations are supported in InfluxDB 2.0+, use &quot;-&quot; as org for InfluxDB 1.8        influxdb.token    my-token    authorization token for InfluxDB API, token are supported in InfluxDB 2.0+, for InfluxDB 1.8 use &#39;username:password&#39; as a token.        influxdb.logLevel    NONE    InfluxDB client library verbosity level (for debugging purpose)  Example configurationOverviewHow to useBasically, you can use%influxdbfrom(bucket: &amp;quot;my-bucket&amp;quot;)  |&amp;gt; range(start: -1h)  |&amp;gt; filter(fn: (r) =&amp;gt; r._measurement == &amp;quot;cpu&amp;quot;)  |&amp;gt; filter(fn: (r) =&amp;gt; r.cpu == &amp;quot;cpu-total&amp;quot;)  |&amp;gt; pivot(rowKey:[&amp;quot;_time&amp;quot;], columnKey: [&amp;quot;_field&amp;quot;], valueColumn: &amp;quot;_value&amp;quot;)In this example we use data collected by  [[inputs.cpu]] Telegraf input plugin. The result of Flux command can contain more one or more tables. In the case of multiple tables, each table is rendered as a separate %table structure. This example uses pivot function to collect values from multiple tables into single table. How to run InfluxDB 2.0 using dockerdocker pull quay.io/influxdb/influxdb:nightlydocker run --name influxdb -p 9999:9999 quay.io/influxdb/influxdb:nightly## Post onBoarding request, to setup initial user (my-user@my-password), org (my-org) and bucketSetup (my-bucket)&amp;quot;curl -i -X POST http://localhost:9999/api/v2/setup -H &amp;#39;accept: application/json&amp;#39;     -d &amp;#39;{            &amp;quot;username&amp;quot;: &amp;quot;my-user&amp;quot;,            &amp;quot;password&amp;quot;: &amp;quot;my-password&amp;quot;,            &amp;quot;org&amp;quot;: &amp;quot;my-org&amp;quot;,            &amp;quot;bucket&amp;quot;: &amp;quot;my-bucket&amp;quot;,            &amp;quot;token&amp;quot;: &amp;quot;my-token&amp;quot;        }&amp;#39;",
      "url": " /interpreter/influxdb.html",
      "group": "interpreter",
      "excerpt": "InfluxDB is an open-source time series database designed to handle high write and query loads."
    }
    ,



    "/interpreter/java.html": {
      "title": "Java interpreter in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Java interpreter for Apache ZeppelinHow to useBasically, you can write normal java code. You should write the main method inside a class because the interpreter invoke this main to execute the code. Unlike Zeppelin normal pattern, each paragraph is considered as a separate job, there isn&amp;#39;t any relation to any other paragraph. For example, a variable defined in one paragraph cannot be used in another one as each paragraph is a self contained java main class that is executed and the output returned to Zeppelin.The following is a demonstration of a word count example with data represented as a java Map and displayed leveraging Zeppelin&amp;#39;s built in visualization using the utility method JavaInterpreterUtils.displayTableFromSimpleMap.%javaimport java.util.HashMap;import java.util.Map;import org.apache.zeppelin.java.JavaInterpreterUtils;public class HelloWorld {    public static void main(String[] args) {        Map&amp;lt;String, Long&amp;gt; counts = new HashMap&amp;lt;&amp;gt;();        counts.put(&amp;quot;hello&amp;quot;,4L);        counts.put(&amp;quot;world&amp;quot;,5L);        System.out.println(JavaInterpreterUtils.displayTableFromSimpleMap(&amp;quot;Word&amp;quot;,&amp;quot;Count&amp;quot;, counts));    }}",
      "url": " /interpreter/java.html",
      "group": "interpreter",
      "excerpt": "Run Java code and any distributed java computation library by importing the dependencies in the interpreter configuration."
    }
    ,



    "/interpreter/jdbc.html": {
      "title": "Generic JDBC Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Generic JDBC Interpreter for Apache ZeppelinOverviewJDBC interpreter lets you create a JDBC connection to any data sources seamlessly.Inserts, Updates, and Upserts are applied immediately after running each statement.By now, it has been tested with:                    Postgresql -      JDBC Driver              Mysql -      JDBC Driver              MariaDB -      JDBC Driver              Redshift -      JDBC Driver              Apache Hive -       JDBC Driver              Presto/Trino -       JDBC Driver              Impala -       JDBC Driver              Apache Phoenix itself is a JDBC driver              Apache Drill -       JDBC Driver              Apache Tajo -       JDBC Driver      If you are using other databases not in the above list, please feel free to share your use case. It would be helpful to improve the functionality of JDBC interpreter.Create a new JDBC InterpreterFirst, click + Create button at the top-right corner in the interpreter setting page.Fill Interpreter name field with whatever you want to use as the alias(e.g. mysql, mysql2, hive, redshift, and etc..). Please note that this alias will be used as %interpreter_name to call the interpreter in the paragraph. Then select jdbc as an Interpreter group. The default driver of JDBC interpreter is set as PostgreSQL. It means Zeppelin includes PostgreSQL driver jar in itself.So you don&amp;#39;t need to add any dependencies(e.g. the artifact name or path for PostgreSQL driver jar) for PostgreSQL connection.The JDBC interpreter properties are defined by default like below.      Name    Default Value    Description        common.max_count    1000    The maximun number of SQL result to display        default.driver    org.postgresql.Driver    JDBC Driver Name        default.password        The JDBC user password        default.url    jdbc:postgresql://localhost:5432/    The URL for JDBC        default.user    gpadmin    The JDBC user name        default.precode        Some SQL which executes every time after initialization of the interpreter (see Binding mode)        default.statementPrecode        SQL code which executed before the SQL from paragraph, in the same database session (database connection)        default.completer.schemaFilters        Сomma separated schema (schema = catalog = database) filters to get metadata for completions. Supports &#39;%&#39; symbol is equivalent to any set of characters. (ex. prod_v_%,public%,info)        default.completer.ttlInSeconds    120    Time to live sql completer in seconds (-1 to update everytime, 0 to disable update)  If you want to connect other databases such as Mysql, Redshift and Hive, you need to edit the property values.You can also use Credential for JDBC authentication.If default.user and default.password properties are deleted(using X button) for database connection in the interpreter setting page,the JDBC interpreter will get the account information from Credential.The below example is for Mysql connection.The last step is Dependency Setting. Since Zeppelin only includes PostgreSQL driver jar by default, you need to add each driver&amp;#39;s maven coordinates or JDBC driver&amp;#39;s jar file path for the other databases.That&amp;#39;s it. You can find more JDBC connection setting examples(Mysql, MariaDB, Redshift, Apache Hive, Presto/Trino, Impala, Apache Phoenix, and Apache Tajo) in this section.JDBC Interpreter Datasource Pool ConfigurationThe Jdbc interpreter uses the connection pool technology, and supports users to do some personal configuration of the connection pool. For example, we can configure default.validationQuery=&amp;#39;select 1&amp;#39; and default.testOnBorrow=true in the Interpreter configuration to avoid the &amp;quot;Invalid SessionHandle&amp;quot; runtime error caused by Session timeout when connecting to HiveServer2 through JDBC interpreter.The Jdbc Interpreter supports the following database connection pool configurations:      Property Name    Default    Description        testOnBorrow    false    The indication of whether objects will be validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and we will attempt to borrow another.        testOnCreate    false    The indication of whether objects will be validated after creation. If the object fails to validate, the borrow attempt that triggered the object creation will fail.        testOnReturn    false    The indication of whether objects will be validated before being returned to the pool.        testWhileIdle    false    The indication of whether objects will be validated by the idle object evictor (if any). If an object fails to validate, it will be dropped from the pool.        timeBetweenEvictionRunsMillis    -1L    The number of milliseconds to sleep between runs of the idle object evictor thread. When non-positive, no idle object evictor thread will be run.        maxWaitMillis    -1L    The maximum number of milliseconds that the pool will wait (when there are no available connections) for a connection to be returned before throwing an exception, or -1 to wait indefinitely.        maxIdle    8    The maximum number of connections that can remain idle in the pool, without extra ones being released, or negative for no limit.        minIdle    0    The minimum number of connections that can remain idle in the pool, without extra ones being created, or zero to create none.        maxTotal    -1    The maximum number of active connections that can be allocated from this pool at the same time, or negative for no limit.        validationQuery    show database    The SQL query that will be used to validate connections from this pool before returning them to the caller. If specified, this query MUST be an SQL SELECT statement that returns at least one row. If not specified, connections will be validation by calling the isValid() method.  More propertiesThere are more JDBC interpreter properties you can specify like below.      Property Name    Description        common.max_result    Max number of SQL result to display to prevent the browser overload. This is  common properties for all connections        zeppelin.jdbc.auth.type    Types of authentications&#39; methods supported are SIMPLE, and KERBEROS        zeppelin.jdbc.principal    The principal name to load from the keytab        zeppelin.jdbc.keytab.location    The path to the keytab file          zeppelin.jdbc.auth.kerberos.proxy.enable      When auth type is Kerberos, enable/disable Kerberos proxy with the login user to get the connection. Default value is true.        default.jceks.file    jceks store path (e.g: jceks://file/tmp/zeppelin.jceks)        default.jceks.credentialKey    jceks credential key        zeppelin.jdbc.interpolation    Enables ZeppelinContext variable interpolation into paragraph text. Default value is false.        zeppelin.jdbc.maxConnLifetime    Maximum of connection lifetime in milliseconds. A value of zero or less means the connection has an infinite lifetime.  You can also add more properties by using this method.For example, if a connection needs a schema parameter, it would have to add the property as follows:      name    value        default.schema    schema_name  How to useRun the paragraph with JDBC interpreterTo test whether your databases and Zeppelin are successfully connected or not, type %jdbc_interpreter_name(e.g. %mysql) at the top of the paragraph and run show databases.%jdbc_interpreter_nameshow databasesIf the paragraph is FINISHED without any errors, a new paragraph will be automatically added after the previous one with %jdbc_interpreter_name.So you don&amp;#39;t need to type this prefix in every paragraphs&amp;#39; header.Multiple SQL statementsYou can write multiple sql statements in one paragraph, just separate them with semi-colon. e.g%jdbc_interpreter_nameUSE zeppelin_demo;CREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20),       species VARCHAR(20), sex CHAR(1), birth DATE, death DATE);SQL Comment2 kinds of SQL comments are supported:Single line comment start with --Multiple line comment around with /*  ... */%jdbc_interpreter_name-- single line commentshow tables;/* multiple    line    comment */select * from test_1;Apply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your queries. You can use both the text input and select form parametrization features.Run SQL ContinuouslyBy default, sql statements in one paragraph are executed only once. But you can run it continuously by specifying local property refreshInterval (unit: milli-seconds),So that the sql statements are executed every interval of refreshInterval milli-seconds. This is useful when your data in database is updated continuously by external system,and you can build dynamic dashboard in Zeppelin via this approach.e.g. Here we query the mysql which is updated continuously by other external system.Usage precodeYou can set precode for each data source. Code runs once while opening the connection.PropertiesAn example settings of interpreter for the two data sources, each of which has its precode parameter.      Property Name    Value        default.driver    org.postgresql.Driver        default.password    1        default.url    jdbc:postgresql://localhost:5432/        default.user    postgres        default.precode    set search_path=&#39;test_path&#39;        default.driver    com.mysql.jdbc.Driver        default.password    1        default.url    jdbc:mysql://localhost:3306/        default.user    root        default.precode    set @v=12  UsageTest of execution precode for each data source.%jdbcshow search_pathReturns value of search_path which is set in the default jdbc (use postgresql) interpreter&amp;#39;s default.precode.%mysqlselect @vReturns value of v which is set in the mysql interpreter&amp;#39;s default.precode.ExamplesHere are some examples you can refer to. Including the below connectors, you can connect every databases as long as it can be configured with it&amp;#39;s JDBC driver.PostgresProperties      Name    Value        default.driver    org.postgresql.Driver        default.url    jdbc:postgresql://localhost:5432/        default.user    mysql_user        default.password    mysql_password  Postgres JDBC Driver DocsDependencies      Artifact    Excludes        org.postgresql:postgresql:9.4.1211      Maven Repository: org.postgresql:postgresqlMysqlProperties      Name    Value        default.driver    com.mysql.jdbc.Driver        default.url    jdbc:mysql://localhost:3306/        default.user    mysql_user        default.password    mysql_password  Mysql JDBC Driver DocsDependencies      Artifact    Excludes        mysql:mysql-connector-java:5.1.38      Maven Repository: mysql:mysql-connector-javaMariaDBProperties      Name    Value        default.driver    org.mariadb.jdbc.Driver        default.url    jdbc:mariadb://localhost:3306        default.user    mariadb_user        default.password    mariadb_password  MariaDB JDBC Driver DocsDependencies      Artifact    Excludes        org.mariadb.jdbc:mariadb-java-client:1.5.4      Maven Repository: org.mariadb.jdbc:mariadb-java-clientRedshiftProperties      Name    Value        default.driver    com.amazon.redshift.jdbc42.Driver        default.url    jdbc:redshift://your-redshift-instance-address.redshift.amazonaws.com:5439/your-database        default.user    redshift_user        default.password    redshift_password  AWS Redshift JDBC Driver DocsDependencies      Artifact    Excludes        com.amazonaws:aws-java-sdk-redshift:1.11.51      Maven Repository: com.amazonaws:aws-java-sdk-redshiftApache HiveZeppelin just connect to hiveserver2 to run hive sql via hive jdbc. There are 2 cases of connecting with Hive:Connect to Hive without KERBEROSConnect to Hive with KERBEROSEach case requires different settings.Connect to Hive without KERBEROSIn this scenario, you need to make the following settings at least. By default, hive job run as user of default.user.Refer impersonation if you want hive job run as the Zeppelin login user when authentication is enabled.      Name    Value        default.driver    org.apache.hive.jdbc.HiveDriver        default.url    jdbc:hive2://localhost:10000        default.user    hive_user        Artifact    Excludes        org.apache.hive:hive-jdbc:2.3.4      Connect to Hive with KERBEROSIn this scenario, you need to make the following settings at least. By default, hive job run as user of client principal (zeppelin.jdbc.principal).Refer impersonation if you want hive job run as the Zeppelin login user when authentication is enabled.      Name    Value        default.driver    org.apache.hive.jdbc.HiveDriver        default.url    jdbc:hive2://emr-header-1:10000/default;principal={hive_server2_principal}        zeppelin.jdbc.auth.type    KERBEROS        zeppelin.jdbc.keytab.location    keytab of client        zeppelin.jdbc.principal    principal of client        Artifact    Excludes        org.apache.hive:hive-jdbc:2.3.4            org.apache.hive:hive-exec:2.3.4      Maven Repository : org.apache.hive:hive-jdbcImpersonationWhen Zeppelin server is running with authentication enabled, then the interpreter can utilize Hive&amp;#39;s user proxy feature i.e. send extra parameter for creating and running a session (&amp;quot;hive.server2.proxy.user=&amp;quot;: &amp;quot;${loggedInUser}&amp;quot;). This is particularly useful when multiple users are sharing a notebook.To enable this set following:default.proxy.user.property as hive.server2.proxy.userSee User Impersonation in interpreter for more information.Sample configuration      Name    Value        hive.driver    org.apache.hive.jdbc.HiveDriver        hive.url    jdbc:hive2://hive-server-host:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2        hive.proxy.user.property    hive.server2.proxy.user        zeppelin.jdbc.auth.type    SIMPLE  Presto/TrinoProperties      Name    Value        default.driver    io.prestosql.jdbc.PrestoDriver        default.url    jdbc:presto://presto-server:9090/hive        default.user    presto_user  Trino JDBC Driver Docs Presto JDBC Driver DocsDependencies      Artifact    Excludes        io.prestosql:presto-jdbc:350      ImpalaProperties      Name    Value        default.driver    org.apache.hive.jdbc.HiveDriver        default.url    jdbc:hive2://emr-header-1.cluster-47080:21050/;auth=noSasl  Dependencies      Artifact    Excludes        org.apache.hive:hive-jdbc:2.3.4      Impala JDBC Driver DocsDependencies      Artifact    Excludes        io.prestosql:presto-jdbc:350      Apache PhoenixPhoenix supports thick and thin connection types:Thick client is faster, but must connect directly to ZooKeeper and HBase RegionServers.Thin client has fewer dependencies and connects through a Phoenix Query Server instance.Use the appropriate default.driver, default.url, and the dependency artifact for your connection type.Thick client connectionProperties      Name    Value        default.driver    org.apache.phoenix.jdbc.PhoenixDriver        default.url    jdbc:phoenix:localhost:2181:/hbase-unsecure        default.user    phoenix_user        default.password    phoenix_password  Dependencies      Artifact    Excludes        org.apache.phoenix:phoenix-core:4.4.0-HBase-1.0      Maven Repository: org.apache.phoenix:phoenix-coreThin client connectionProperties      Name    Value        default.driver    org.apache.phoenix.queryserver.client.Driver        default.url    jdbc:phoenix:thin:url=http://localhost:8765;serialization=PROTOBUF        default.user    phoenix_user        default.password    phoenix_password  DependenciesBefore Adding one of the below dependencies, check the Phoenix version first.      Artifact    Excludes    Description        org.apache.phoenix:phoenix-server-client:4.7.0-HBase-1.1        For Phoenix 4.7        org.apache.phoenix:phoenix-queryserver-client:4.8.0-HBase-1.2        For Phoenix 4.8+  Maven Repository: org.apache.phoenix:phoenix-queryserver-clientApache TajoProperties      Name    Value        default.driver    org.apache.tajo.jdbc.TajoDriver        default.url    jdbc:tajo://localhost:26002/default  Apache Tajo JDBC Driver DocsDependencies      Artifact    Excludes        org.apache.tajo:tajo-jdbc:0.11.0      Maven Repository: org.apache.tajo:tajo-jdbcObject InterpolationThe JDBC interpreter also supports interpolation of ZeppelinContext objects into the paragraph text.The following example shows one use of this facility:In Scala cell:z.put(&amp;quot;country_code&amp;quot;, &amp;quot;KR&amp;quot;)    // ...In later JDBC cell:%jdbc_interpreter_nameselect * from patents_list where priority_country = &amp;#39;{country_code}&amp;#39; and filing_date like &amp;#39;2015-%&amp;#39;Object interpolation is disabled by default, and can be enabled for all instances of the JDBC interpreter by setting the value of the property zeppelin.jdbc.interpolation to true (see More Properties above). More details of this feature can be found in the Spark interpreter documentation under Zeppelin-ContextBug reportingIf you find a bug using JDBC interpreter, please create a JIRA ticket.",
      "url": " /interpreter/jdbc.html",
      "group": "interpreter",
      "excerpt": "Generic JDBC Interpreter lets you create a JDBC connection to any data source. You can use Postgres, MySql, MariaDB, Redshift, Apache Hive, Presto/Trino, Impala, Apache Phoenix, Apache Drill and Apache Tajo using JDBC interpreter."
    }
    ,



    "/interpreter/jupyter.html": {
      "title": "Jupyter Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Jupyter Interpreter for Apache ZeppelinOverviewProject Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.Zeppelin&amp;#39;s Jupyter interpreter is a bridge/adapter between Zeppelin interpreter and Jupyter kernel. You can use any of jupyter kernel as long as you installed the necessary dependencies.ConfigurationTo run any Jupyter kernel in Zeppelin you first need to install the following prerequisite:pip install jupyter-clientpip install grpciopip install protobufThen you need install the jupyter kernel you want to use. In the following sections, we will talk about how to use the following 3 jupyter kernels in Zeppelin:ipythonirjuliaJupyter Python kernelIn order to use Jupyter Python kernel in Zeppelin, you need to install ipykernel first. pip install ipykernelThen you can run python code in Jupyter interpreter like following. %jupyter(kernel=python)%matplotlib inlineimport matplotlib.pyplot as pltplt.plot([1, 2, 3])Jupyter R kernelIn order to use IRKernel, you need to first install IRkernel package in R.install.packages(&amp;#39;IRkernel&amp;#39;)IRkernel::installspec()  # to register the kernel in the current R installationThen you can run r code in Jupyter interpreter like following. %jupyter(kernel=ir)library(ggplot2)ggplot(mpg, aes(x = displ, y = hwy)) +  geom_point()Jupyter Julia kernelIn order to use Julia in Zeppelin, you first need to install IJulia firstusing PkgPkg.add(&amp;quot;IJulia&amp;quot;)Then you can run julia code in Jupyter interpreter like following. %jupyter(kernel=julia-1.3)using PkgPkg.add(&amp;quot;Plots&amp;quot;)using Plotsplotly() # Choose the Plotly.jl backend for web interactivityplot(rand(5,5),linewidth=2,title=&amp;quot;My Plot&amp;quot;)Pkg.add(&amp;quot;PyPlot&amp;quot;) # Install a different backendpyplot() # Switch to using the PyPlot.jl backendplot(rand(5,5),linewidth=2,title=&amp;quot;My Plot&amp;quot;)Use any other kernelFor any other jupyter kernel, you can follow the below steps to use it in Zeppelin.Install the specified jupyter kernel. you can find all the available jupyter kernels here Find its kernel name by run the following commandbashjupyter kernelspec listRun the kernel as following%jupyter(kernel=kernel_name)code",
      "url": " /interpreter/jupyter.html",
      "group": "interpreter",
      "excerpt": "Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages."
    }
    ,



    "/interpreter/kotlin.html": {
      "title": "Kotlin interpreter in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Kotlin interpreter for Apache ZeppelinOverviewKotlin is a cross-platform, statically typed, general-purpose programming language with type inference.It is designed to interoperate fully with Java, and the JVM version of its standard library depends on the Java Class Library, but type inference allows its syntax to be more concise.Configuration                Name        Default        Description                        zeppelin.kotlin.maxResult        1000        Max n                    zeppelin.kotlin.shortenTypes        true        Display shortened types instead of full, e.g. Int vs kotlin.Int        Example%kotlin fun square(n: Int): Int = n * nKotlin ContextKotlin context is accessible via kc object bound to the interpreter. It holds vars and functions fields that return all user-defined variables and functions present in the interpreter.You can also print variables or functions by calling kc.showVars() or kc.showFunctions().Examplefun square(n: Int): Int = n * nval greeter = { s: String -&amp;gt; println(&amp;quot;Hello $s!&amp;quot;) }val l = listOf(&amp;quot;Drive&amp;quot;, &amp;quot;to&amp;quot;, &amp;quot;develop&amp;quot;)kc.showVars()kc.showFunctions()Output:l: List&amp;lt;String&amp;gt; = [Drive, to, develop]greeter: (String) -&amp;gt; Unit = (kotlin.String) -&amp;gt; kotlin.Unitfun square(Int): Int",
      "url": " /interpreter/kotlin.html",
      "group": "interpreter",
      "excerpt": "Kotlin is a cross-platform, statically typed, general-purpose programming language with type inference."
    }
    ,



    "/interpreter/ksql.html": {
      "title": "KSQL Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;KSQL Interpreter for Apache ZeppelinOverviewKSQL is the streaming SQL engine for Apache Kafka®. It provides an easy-to-use yet powerful interactive SQL interface for stream processing on Kafka,Configuration            Property      Default      Description                  ksql.url      http://localhost:8080      The KSQL Endpoint base URL      N.b. The interpreter supports all the KSQL properties, i.e. ksql.streams.auto.offset.reset.The full list of KSQL parameters is here.Using the KSQL InterpreterIn a paragraph, use %ksql and start your SQL query in order to start to interact with KSQL.Following some examples:%ksqlPRINT &amp;#39;orders&amp;#39;;%ksqlCREATE STREAM ORDERS WITH  (VALUE_FORMAT=&amp;#39;AVRO&amp;#39;,   KAFKA_TOPIC =&amp;#39;orders&amp;#39;);%ksqlSELECT *FROM ORDERSLIMIT 10",
      "url": " /interpreter/ksql.html",
      "group": "interpreter",
      "excerpt": "SQL is the streaming SQL engine for Apache Kafka and provides an easy-to-use yet powerful interactive SQL interface for stream processing on Kafka."
    }
    ,



    "/interpreter/kylin.html": {
      "title": "Apache Kylin Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Apache Kylin Interpreter for Apache ZeppelinOverviewApache Kylin is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. The interpreter assumes that Apache Kylin has been installed and you can connect to Apache Kylin from the machine Apache Zeppelin is installed.To get start with Apache Kylin, please see Apache Kylin Quickstart.Configuration      Name    Default    Description        kylin.api.url     http://localhost:7070/kylin/api/query    kylin query POST API  The format can be like http://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/kylin/api/query        kylin.api.user    ADMIN    kylin user        kylin.api.password    KYLIN    kylin password        kylin.query.project    learn_kylin    String, Project to perform query. Could update at notebook level        kylin.query.ispartial    true    true|false  (@Deprecated since Apache Kylin V1.5)  Whether accept a partial result or not, default be “false”. Set to “false” for production use.        kylin.query.limit    5000    int, Query limit  If limit is set in sql, perPage will be ignored.        kylin.query.offset    0    int, Query offset  If offset is set in sql, curIndex will be ignored.  Using the Apache Kylin InterpreterIn a paragraph, use %kylin(project_name) to select the kylin interpreter, project name and then input sql. If no project name defined, will use the default project name from the above configuration.%kylin(learn_project)select count(*) from kylin_sales group by part_dt",
      "url": " /interpreter/kylin.html",
      "group": "interpreter",
      "excerpt": "Apache Kylin™ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. ."
    }
    ,



    "/interpreter/lens.html": {
      "title": "Lens Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Lens Interpreter for Apache ZeppelinOverviewApache Lens provides an Unified Analytics interface. Lens aims to cut the Data Analytics silos by providing a single view of data across multiple tiered data stores and optimal execution environment for the analytical query. It seamlessly integrates Hadoop with traditional data warehouses to appear like one.Installing and Running LensIn order to use Lens interpreters, you may install Apache Lens in some simple steps:Download Lens for latest version from the ASF. Or the older release can be found in the Archives.Before running Lens, you have to set HIVEHOME and HADOOPHOME. If you want to get more information about this, please refer to here. Lens also provides Pseudo Distributed mode. Lens pseudo-distributed setup is done by using docker. Hive server and hadoop daemons are run as separate processes in lens pseudo-distributed setup.Now, you can start lens server (or stop)../bin/lens-ctl start # (or stop)Configuring Lens InterpreterAt the &amp;quot;Interpreters&amp;quot; menu, you can edit Lens interpreter or create new one. Zeppelin provides these properties for Lens.      Property Name    value    Description        lens.client.dbname    default    The database schema name        lens.query.enable.persistent.resultset    false    Whether to enable persistent resultset for queries. When enabled, server will fetch results from driver, custom format them if any and store in a configured location. The file name of query output is queryhandle-id, with configured extensions        lens.server.base.url    http://hostname:port/lensapi    The base url for the lens server. you have to edit &quot;hostname&quot; and &quot;port&quot; that you may use(ex. http://0.0.0.0:9999/lensapi)          lens.session.cluster.user     default    Hadoop cluster username        zeppelin.lens.maxResult    1000    Max number of rows to display        zeppelin.lens.maxThreads    10    If concurrency is true then how many threads?        zeppelin.lens.run.concurrent    true    Run concurrent Lens Sessions        xxx    yyy    anything else from [Configuring lens server](https://lens.apache.org/admin/config-server.html)  Interpreter Binding for Zeppelin NotebookAfter configuring Lens interpreter, create your own notebook, then you can bind interpreters like below image.For more interpreter binding information see here.How to useYou can analyze your data by using OLAP Cube QL which is a high level SQL like language to query and describe data sets organized in data cubes.You may experience OLAP Cube like this Video tutorial.As you can see in this video, they are using Lens Client Shell(./bin/lens-cli.sh). All of these functions also can be used on Zeppelin by using Lens interpreter. Create and Use (Switch) Databases.create database newDbuse newDb Create Storage.create storage your/path/to/lens/client/examples/resources/db-storage.xml Create Dimensions, Show fields and join-chains of them.create dimension your/path/to/lens/client/examples/resources/customer.xmldimension show fields customerdimension show joinchains customer Create Caches, Show fields and join-chains of them.create cube your/path/to/lens/client/examples/resources/sales-cube.xmlcube show fields salescube show joinchains sales Create Dimtables and Fact.create dimtable your/path/to/lens/client/examples/resources/customer_table.xmlcreate fact your/path/to/lens/client/examples/resources/sales-raw-fact.xml Add partitions to Dimtable and Fact.dimtable add single-partition --dimtable_name customer_table --storage_name local --path your/path/to/lens/client/examples/resources/customer-local-part.xmlfact add partitions --fact_name sales_raw_fact --storage_name local --path your/path/to/lens/client/examples/resources/sales-raw-local-parts.xml Now, you can run queries on cubes.query execute cube select customer_city_name, product_details.description, product_details.category, product_details.color, store_sales from sales where time_range_in(delivery_time, &amp;#39;2015-04-11-00&amp;#39;, &amp;#39;2015-04-13-00&amp;#39;)These are just examples that provided in advance by Lens. If you want to explore whole tutorials of Lens, see the tutorial video.Lens UI ServiceLens also provides web UI service. Once the server starts up, you can open the service on http://serverhost:19999/index.html and browse. You may also check the structure that you made and use query easily here.",
      "url": " /interpreter/lens.html",
      "group": "interpreter",
      "excerpt": "Apache Lens provides an Unified Analytics interface. Lens aims to cut the Data Analytics silos by providing a single view of data across multiple tiered data stores and optimal execution environment for the analytical query. It seamlessly integrates Hadoop with..."
    }
    ,



    "/interpreter/livy.html": {
      "title": "Livy Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Livy Interpreter for Apache ZeppelinOverviewLivy is an open source REST interface for interacting with Spark from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in YARN.Interactive Scala, Python and R shellsBatch submissions in Scala, Java, PythonMulti users can share the same server (impersonation support)Can be used for submitting jobs from anywhere with RESTDoes not require any code change to your programsRequirementsAdditional requirements for the Livy interpreter are:Spark 1.3 or above.Livy server.ConfigurationWe added some common configurations for spark, and you can set any configuration you want.You can find all Spark configurations in here.And instead of starting property with spark. it should be replaced with livy.spark..Example: spark.driver.memory to livy.spark.driver.memory      Property    Default    Description        zeppelin.livy.url    http://localhost:8998    URL where livy server is running        zeppelin.livy.spark.sql.maxResult    1000    Max number of Spark SQL result to display.        zeppelin.livy.spark.sql.field.truncate    true    Whether to truncate field values longer than 20 characters or not        zeppelin.livy.session.create_timeout    120    Timeout in seconds for session creation        zeppelin.livy.displayAppInfo    true    Whether to display app info        zeppelin.livy.pull_status.interval.millis    1000    The interval for checking paragraph execution status        livy.spark.driver.cores        Driver cores. ex) 1, 2.          livy.spark.driver.memory        Driver memory. ex) 512m, 32g.          livy.spark.executor.instances        Executor instances. ex) 1, 4.          livy.spark.executor.cores        Num cores per executor. ex) 1, 4.        livy.spark.executor.memory        Executor memory per worker instance. ex) 512m, 32g.        livy.spark.dynamicAllocation.enabled        Use dynamic resource allocation. ex) True, False.        livy.spark.dynamicAllocation.cachedExecutorIdleTimeout        Remove an executor which has cached data blocks.        livy.spark.dynamicAllocation.minExecutors        Lower bound for the number of executors.        livy.spark.dynamicAllocation.initialExecutors        Initial number of executors to run.        livy.spark.dynamicAllocation.maxExecutors        Upper bound for the number of executors.            livy.spark.jars.packages            Adding extra libraries to livy interpreter          zeppelin.livy.ssl.trustStore        client trustStore file. Used when livy ssl is enabled        zeppelin.livy.ssl.trustStorePassword        password for trustStore file. Used when livy ssl is enabled        zeppelin.livy.ssl.trustStoreType    JKS    type of truststore. Either JKS or PKCS12.        zeppelin.livy.ssl.keyStore        client keyStore file. Needed if Livy requires two way SSL authentication.        zeppelin.livy.ssl.keyStorePassword        password for keyStore file.        zeppelin.livy.ssl.keyStoreType    JKS    type of keystore. Either JKS or PKCS12.        zeppelin.livy.ssl.keyPassword        password for key in the keyStore file. Defaults to zeppelin.livy.ssl.keyStorePassword.               zeppelin.livy.http.headers    key_1: value_1; key_2: value_2    custom http headers when calling livy rest api. Each http header is separated by `;`, and each header is one key value pair where key value is separated by `:`        zeppelin.livy.tableWithUTFCharacters    false    If database contains UTF characters then set this as true.  We remove livy.spark.master in zeppelin-0.7. Because we sugguest user to use livy 0.3 in zeppelin-0.7. And livy 0.3 don&amp;#39;t allow to specify livy.spark.master, it enfornce yarn-cluster mode.Adding External librariesYou can load dynamic library to livy interpreter by set livy.spark.jars.packages property to comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. The format for the coordinates should be groupId:artifactId:version.Example      Property    Example    Description          livy.spark.jars.packages      io.spray:spray-json_2.10:1.3.1      Adding extra libraries to livy interpreter      How to useBasically, you can usespark%livy.sparksc.versionpyspark%livy.pysparkprint &amp;quot;1&amp;quot;sparkR%livy.sparkrhello &amp;lt;- function( name ) {    sprintf( &amp;quot;Hello, %s&amp;quot;, name );}hello(&amp;quot;livy&amp;quot;)ImpersonationWhen Zeppelin server is running with authentication enabled,then this interpreter utilizes Livy’s user impersonation featurei.e. sends extra parameter for creating and running a session (&amp;quot;proxyUser&amp;quot;: &amp;quot;${loggedInUser}&amp;quot;).This is particularly useful when multi users are sharing a Notebook server.Apply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form. Form templates is only avalible for livy sql interpreter.%livy.sqlselect * from products where ${product_id=1}And creating dynamic formst programmatically is not feasible in livy interpreter, because ZeppelinContext is not available in livy interpreter.Shared SparkContextStarting from livy 0.5 which is supported by Zeppelin 0.8.0, SparkContext is shared between scala, python, r and sql.That means you can query the table via %livy.sql when this table is registered in %livy.spark, %livy.pyspark, $livy.sparkr.FAQLivy debugging: If you see any of these in error consoleConnect to livyhost:8998 [livyhost/127.0.0.1, livyhost/0:0:0:0:0:0:0:1] failed: Connection refusedLooks like the livy server is not up yet or the config is wrongException: Session not found, Livy server would have restarted, or lost session.The session would have timed out, you may need to restart the interpreter.Blacklisted configuration values in session config: spark.masterEdit conf/spark-blacklist.conf file in livy server and comment out #spark.master line.If you choose to work on livy in apps/spark/java directory in https://github.com/cloudera/hue,copy spark-user-configurable-options.template to spark-user-configurable-options.conf file in livy server and comment out #spark.master.",
      "url": " /interpreter/livy.html",
      "group": "interpreter",
      "excerpt": "Livy is an open source REST interface for interacting with Spark from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in YARN."
    }
    ,



    "/interpreter/mahout.html": {
      "title": "Mahout Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Apache Mahout Interpreter for Apache ZeppelinInstallationApache Mahout is a collection of packages that enable machine learning and matrix algebra on underlying engines such as Apache Flink or Apache Spark.  A convenience script for creating and configuring two Mahout enabled interpreters exists.  The %sparkMahout and %flinkMahout interpreters do not exist by default but can be easily created using this script.  Easy InstallationTo quickly and easily get up and running using Apache Mahout, run the following command from the top-level directory of the Zeppelin install:python scripts/mahout/add_mahout.pyThis will create the %sparkMahout and %flinkMahout interpreters, and restart Zeppelin.Advanced InstallationThe add_mahout.py script contains several command line arguments for advanced users.      Argument    Description    Example        --zeppelin_home    This is the path to the Zeppelin installation.  This flag is not needed if the script is run from the top-level installation directory or from the zeppelin/scripts/mahout directory.    /path/to/zeppelin        --mahout_home    If the user has already installed Mahout, this flag can set the path to MAHOUT_HOME.  If this is set, downloading Mahout will be skipped.    /path/to/mahout_home        --restart_later    Restarting is necessary for updates to take effect. By default the script will restart Zeppelin for you. Restart will be skipped if this flag is set.    NA        --force_download    This flag will force the script to re-download the binary even if it already exists.  This is useful for previously failed downloads.    NA          --overwrite_existing      This flag will force the script to overwrite existing %sparkMahout and %flinkMahout interpreters. Useful when you want to just start over.      NA    NOTE 1: Apache Mahout at this time only supports Spark 1.5 and Spark 1.6 and Scala 2.10.  If the user is using another version of Spark (e.g. 2.0), the %sparkMahout will likely not work.  The %flinkMahout interpreter will still work and the user is encouraged to develop with that engine as the code can be ported via copy and paste, as is evidenced by the tutorial notebook.NOTE 2: If using Apache Flink in cluster mode, the following libraries will also need to be coppied to ${FLINK_HOME}/lib- mahout-math-0.12.2.jar- mahout-math-scala2.10-0.12.2.jar- mahout-flink2.10-0.12.2.jar- mahout-hdfs-0.12.2.jar- com.google.guava:guava:14.0.1OverviewThe Apache Mahout™ project&amp;#39;s goal is to build an environment for quickly creating scalable performant machine learning applications.Apache Mahout software provides three major features:A simple and extensible programming environment and framework for building scalable algorithmsA wide variety of premade algorithms for Scala + Apache Spark, H2O, Apache FlinkSamsara, a vector math experimentation environment with R-like syntax which works at scaleIn other words:Apache Mahout provides a unified API for quickly creating machine learning algorithms on a variety of engines.How to useWhen starting a session with Apache Mahout, depending on which engine you are using (Spark or Flink), a few imports must be made and a Distributed Context must be declared.  Copy and paste the following code and run once to get started.Flink%flinkMahoutimport org.apache.flink.api.scala._import org.apache.mahout.math.drm._import org.apache.mahout.math.drm.RLikeDrmOps._import org.apache.mahout.flinkbindings._import org.apache.mahout.math._import scalabindings._import RLikeOps._implicit val ctx = new FlinkDistributedContext(benv)Spark%sparkMahoutimport org.apache.mahout.math._import org.apache.mahout.math.scalabindings._import org.apache.mahout.math.drm._import org.apache.mahout.math.scalabindings.RLikeOps._import org.apache.mahout.math.drm.RLikeDrmOps._import org.apache.mahout.sparkbindings._implicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)Same Code, Different EnginesAfter importing and setting up the distributed context, the Mahout R-Like DSL is consistent across engines.  The following code will run in both %flinkMahout and %sparkMahoutval drmData = drmParallelize(dense(  (2, 2, 10.5, 10, 29.509541),  // Apple Cinnamon Cheerios  (1, 2, 12,   12, 18.042851),  // Cap&amp;#39;n&amp;#39;Crunch  (1, 1, 12,   13, 22.736446),  // Cocoa Puffs  (2, 1, 11,   13, 32.207582),  // Froot Loops  (1, 2, 12,   11, 21.871292),  // Honey Graham Ohs  (2, 1, 16,   8,  36.187559),  // Wheaties Honey Gold  (6, 2, 17,   1,  50.764999),  // Cheerios  (3, 2, 13,   7,  40.400208),  // Clusters  (3, 3, 13,   4,  45.811716)), numPartitions = 2)drmData.collect(::, 0 until 4)val drmX = drmData(::, 0 until 4)val y = drmData.collect(::, 4)val drmXtX = drmX.t %*% drmXval drmXty = drmX.t %*% yval XtX = drmXtX.collectval Xty = drmXty.collect(::, 0)val beta = solve(XtX, Xty)Leveraging Resource Pools and R for VisualizationResource Pools are a powerful Zeppelin feature that lets us share information between interpreters. A fun trick is to take the output of our work in Mahout and analyze it in other languages.Setting up a Resource Pool in FlinkIn Spark based interpreters resource pools are accessed via the ZeppelinContext API.  To put and get things from the resource pool one can be done simpleval myVal = 1z.put(&amp;quot;foo&amp;quot;, myVal)val myFetchedVal = z.get(&amp;quot;foo&amp;quot;)To add this functionality to a Flink based interpreter we declare the follwoing%flinkMahoutimport org.apache.zeppelin.interpreter.InterpreterContextval z = InterpreterContext.get().getResourcePool()Now we can access the resource pool in a consistent manner from the %flinkMahout interpreter.Passing a variable from Mahout to R and PlottingIn this simple example, we use Mahout (on Flink or Spark, the code is the same) to create a random matrix and then take the Sin of each element. We then randomly sample the matrix and create a tab separated string. Finally we pass that string to R where it is read as a .tsv file, and a DataFrame is created and plotted using native R plotting libraries.val mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)val drmRand = drmParallelize(mxRnd)val drmSin = drmRand.mapBlock() {case (keys, block) =&amp;gt;    val blockB = block.like()  for (i &amp;lt;- 0 until block.nrow) {    blockB(i, 0) = block(i, 0)    blockB(i, 1) = Math.sin((block(i, 0) * 8))  }  keys -&amp;gt; blockB}z.put(&amp;quot;sinDrm&amp;quot;, org.apache.mahout.math.drm.drmSampleToTSV(drmSin, 0.85))And then in an R paragraph...%spark.r {&amp;quot;imageWidth&amp;quot;: &amp;quot;400px&amp;quot;}library(&amp;quot;ggplot2&amp;quot;)sinStr = z.get(&amp;quot;flinkSinDrm&amp;quot;)data &amp;lt;- read.table(text= sinStr, sep=&amp;quot;t&amp;quot;, header=FALSE)plot(data,  col=&amp;quot;red&amp;quot;)",
      "url": " /interpreter/mahout.html",
      "group": "interpreter",
      "excerpt": "Apache Mahout provides a unified API (the R-Like Scala DSL) for quickly creating machine learning algorithms on a variety of engines."
    }
    ,



    "/interpreter/markdown.html": {
      "title": "Markdown Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Markdown Interpreter for Apache ZeppelinOverviewMarkdown is a plain text formatting syntax designed so that it can be converted to HTML.Apache Zeppelin uses flexmark, pegdown and markdown4j as markdown parsers.In Zeppelin notebook, you can use %md in the beginning of a paragraph to invoke the Markdown interpreter and generate static html from Markdown plain text.In Zeppelin, Markdown interpreter is enabled by default and uses the pegdown parser.ExampleThe following example demonstrates the basic usage of Markdown in a Zeppelin notebook.Mathematical expressionMarkdown interpreter leverages %html display system internally. That means you can mix mathematical expressions with markdown syntax. For more information, please see Mathematical Expression section.Configuration      Name    Default Value    Description        markdown.parser.type    flexmark    Markdown Parser Type.  Available values: flexmark, pegdown, markdown4j.  Flexmark parser (Default Markdown Parser)CommonMark/Markdown Java parser with source level AST.flexmark parser provides YUML and Websequence extensions also.Pegdown Parserpegdown parser provides github flavored markdown. Although still one of the most popular Markdown parsing libraries for the JVM, pegdown has reached its end of life.The project is essentially unmaintained with tickets piling up and crucial bugs not being fixed.pegdown&amp;#39;s parsing performance isn&amp;#39;t great. But keep this parser for the backward compatibility.Markdown4j ParserSince pegdown parser is more accurate and provides much more markdown syntax markdown4j option might be removed later. But keep this parser for the backward compatibility.",
      "url": " /interpreter/markdown.html",
      "group": "interpreter",
      "excerpt": "Markdown is a plain text formatting syntax designed so that it can be converted to HTML. Apache Zeppelin uses markdown4j."
    }
    ,



    "/interpreter/mongodb.html": {
      "title": "MongoDB Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;MongoDB interpreter for Apache ZeppelinOverviewMongoDB is a general purpose, document-based, distributed database built for modern application developers and for the cloud era.This interpreter use mongo shell to execute scriptsUse mongo-shell JavaScript to analyze data as you need.Installing AND ConfigurationFirst, you need to install mongo shell with Zeppelin in the same machine.If you use mac with brew, follow this instructions.brew tap mongodb/brewbrew install mongodb/brew/mongodb-community-shellOr you can follow this mongo shellSecond, create mongodb interpreter in Zeppelin.      Name    Default Value    Description        mongo.shell.path    mongo    MongoDB shell local path.  Use which mongo to get local path in linux or mac.         mongo.shell.command.table.limit     1000     Limit of documents displayed in a table.  Use table function when get data from mongodb         mongo.shell.command.timeout     60000     MongoDB shell command timeout in millisecond         mongo.server.host     localhost     MongoDB server host to connect to        mongo.server.port    27017    MongoDB server port to connect to        mongo.server.database    test    MongoDB database name         mongo.server.authentdatabase          MongoDB database name for authentication        mongo.server.username        Username for authentication        mongo.server.password        Password for authentication        mongo.interpreter.concurrency.max    10    Max count of scheduler concurrency  ExamplesThe following example demonstrates the basic usage of MongoDB in a Zeppelin notebook.Or you can monitor stats of mongodb collections.",
      "url": " /interpreter/mongodb.html",
      "group": "interpreter",
      "excerpt": "MongoDB is a general purpose, document-based, distributed database built for modern application developers and for the cloud era."
    }
    ,



    "/interpreter/neo4j.html": {
      "title": "Neo4j Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Neo4j Interpreter for Apache ZeppelinOverviewNeo4j is a native graph database, designed to store and process graphs from bottom to top.Supported VersionThe Neo4j Interpreter supports all Neo4j versions since v3 via the official Neo4j Java DriverConfiguration      Property    Default    Description        neo4j.url    bolt://localhost:7687    The Neo4j&#39;s BOLT url.        neo4j.database        The neo4j target database, if empty use the dafault db.        neo4j.multi.statement    true    Enables the multi statement management, if true it computes multiple queries separated by semicolon.        neo4j.auth.type    BASIC    The Neo4j&#39;s authentication type (NONE, BASIC).        neo4j.auth.user    neo4j    The Neo4j user name.        neo4j.auth.password    neo4j    The Neo4j user password.        neo4j.max.concurrency    50    Max concurrency call from Zeppelin to Neo4j server.    Enabling the Neo4j InterpreterIn a notebook, to enable the Neo4j interpreter, click the Gear icon and select Neo4j.Using the Neo4j InterpreterIn a paragraph, use %neo4j to select the Neo4j interpreter and then input the Cypher commands.For list of Cypher commands please refer to the official Cyper Refcard%neo4j//Sample the TrumpWorld datasetWITH&amp;#39;https://docs.google.com/spreadsheets/u/1/d/1Z5Vo5pbvxKJ5XpfALZXvCzW26Cl4we3OaN73K9Ae5Ss/export?format=csv&amp;amp;gid=1996904412&amp;#39; AS urlLOAD CSV WITH HEADERS FROM url AS rowRETURN row.`Entity A`, row.`Entity A Type`, row.`Entity B`, row.`Entity B Type`, row.Connection, row.`Source(s)`LIMIT 10The Neo4j interpreter leverages the Network display system allowing to visualize the them directly from the paragraph.Write your Cypher queries and navigate your graphThis query:%neo4jMATCH (vp:Person {name:&amp;quot;VLADIMIR PUTIN&amp;quot;}), (dt:Person {name:&amp;quot;DONALD J. TRUMP&amp;quot;})MATCH path = allShortestPaths( (vp)-[*]-(dt) )RETURN pathproduces the following result_Apply Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your queries. This query:%neo4jMATCH (o:Organization)-[r]-()RETURN o.name, count(*), collect(distinct type(r)) AS typesORDER BY count(*) DESCLIMIT ${Show top=10}produces the following result:",
      "url": " /interpreter/neo4j.html",
      "group": "interpreter",
      "excerpt": "Neo4j is a native graph database, designed to store and process graphs from bottom to top."
    }
    ,



    "/interpreter/pig.html": {
      "title": "Pig Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Pig Interpreter for Apache ZeppelinOverviewApache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.Supported interpreter type%pig.script (default Pig interpreter, so you can use %pig)%pig.script is like the Pig grunt shell. Anything you can run in Pig grunt shell can be run in %pig.script interpreter, it is used for running Pig script where you don’t need to visualize the data, it is suitable for data munging. %pig.query%pig.query is a little different compared with %pig.script. It is used for exploratory data analysis via Pig latin where you can leverage Zeppelin’s visualization ability. There&amp;#39;re 2 minor differences in the last statement between %pig.script and %pig.queryNo pig alias in the last statement in %pig.query (read the examples below).The last statement must be in single line in %pig.queryHow to useHow to setup Pig execution modes.Local ModeSet zeppelin.pig.execType as local.MapReduce ModeSet zeppelin.pig.execType as mapreduce. HADOOP_CONF_DIR needs to be specified in ZEPPELIN_HOME/conf/zeppelin-env.sh.Tez Local ModeOnly Tez 0.7 is supported. Set zeppelin.pig.execType as tez_local.Tez ModeOnly Tez 0.7 is supported. Set zeppelin.pig.execType as tez. HADOOP_CONF_DIR and TEZ_CONF_DIR needs to be specified in ZEPPELIN_HOME/conf/zeppelin-env.sh.Spark Local ModeOnly Spark 1.6.x is supported, by default it is Spark 1.6.3. Set zeppelin.pig.execType as spark_local.Spark ModeOnly Spark 1.6.x is supported, by default it is Spark 1.6.3. Set zeppelin.pig.execType as spark. For now, only yarn-client mode is supported. To enable it, you need to set property SPARK_MASTER to yarn-client and set SPARK_JAR to the spark assembly jar.How to choose custom Spark VersionBy default, Pig Interpreter would use Spark 1.6.3 built with scala 2.10, if you want to use another spark version or scala version, you need to rebuild Zeppelin by specifying the custom Spark version via -Dpig.spark.version= and scala version via -Dpig.scala.version= in the maven build command.How to configure interpreterAt the Interpreters menu, you have to create a new Pig interpreter. Pig interpreter has below properties by default.And you can set any Pig properties here which will be passed to Pig engine. (like tez.queue.name &amp;amp; mapred.job.queue.name).Besides, we use paragraph title as job name if it exists, else use the last line of Pig script. So you can use that to find app running in YARN RM UI.            Property        Default        Description                zeppelin.pig.execType        mapreduce        Execution mode for pig runtime. local | mapreduce | tez_local | tez | spark_local | spark                 zeppelin.pig.includeJobStats        false        whether display jobStats info in %pig.script                zeppelin.pig.maxResult        1000        max row number displayed in %pig.query                tez.queue.name        default        queue name for tez engine                mapred.job.queue.name        default        queue name for mapreduce engine                SPARK_MASTER        local        local | yarn-client                SPARK_JAR                The spark assembly jar, both jar in local or hdfs is supported. Put it on hdfs could have        performance benefit      Examplepig%pigbankText = load &amp;#39;bank.csv&amp;#39; using PigStorage(&amp;#39;;&amp;#39;);bank = foreach bankText generate $0 as age, $1 as job, $2 as marital, $3 as education, $5 as balance; bank = filter bank by age != &amp;#39;&amp;quot;age&amp;quot;&amp;#39;;bank = foreach bank generate (int)age, REPLACE(job,&amp;#39;&amp;quot;&amp;#39;,&amp;#39;&amp;#39;) as job, REPLACE(marital, &amp;#39;&amp;quot;&amp;#39;, &amp;#39;&amp;#39;) as marital, (int)(REPLACE(balance, &amp;#39;&amp;quot;&amp;#39;, &amp;#39;&amp;#39;)) as balance;store bank into &amp;#39;clean_bank.csv&amp;#39; using PigStorage(&amp;#39;;&amp;#39;); -- this statement is optional, it just show you that most of time %pig.script is used for data munging before querying the data. pig.queryGet the number of each age where age is less than 30%pig.querybank_data = filter bank by age &amp;lt; 30;b = group bank_data by age;foreach b generate group, COUNT($1);The same as above, but use dynamic text form so that use can specify the variable maxAge in textbox. (See screenshot below). Dynamic form is a very cool feature of Zeppelin, you can refer this link) for details.%pig.querybank_data = filter bank by age &amp;lt; ${maxAge=40};b = group bank_data by age;foreach b generate group, COUNT($1) as count;Get the number of each age for specific marital type, also use dynamic form here. User can choose the marital type in the dropdown list (see screenshot below).%pig.querybank_data = filter bank by marital==&amp;#39;${marital=single,single|divorced|married}&amp;#39;;b = group bank_data by age;foreach b generate group, COUNT($1) as count;The above examples are in the Pig tutorial note in Zeppelin, you can check that for details. Here&amp;#39;s the screenshot.Data is shared between %pig and %pig.query, so that you can do some common work in %pig, and do different kinds of query based on the data of %pig. Besides, we recommend you to specify alias explicitly so that the visualization can display the column name correctly. In the above example 2 and 3 of %pig.query, we name COUNT($1) as count. If you don&amp;#39;t do this, then we will name it using position. E.g. in the above first example of %pig.query, we will use col_1 in chart to represent COUNT($1).",
      "url": " /interpreter/pig.html",
      "group": "manual",
      "excerpt": "Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs."
    }
    ,



    "/interpreter/postgresql.html": {
      "title": "PostgreSQL, Apache HAWQ (incubating) Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--        Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);     you may not use this file except in compliance with the License.        You may obtain a copy of the License at             http://www.apache.org/licenses/LICENSE-2.0              Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an &quot;AS IS&quot; BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.        See the License for the specific language governing permissions and     limitations under the License.      --&gt;     PostgreSQL, Apache HAWQ (incubating) Interpreter for Apache Zeppelin                Important NoticePostgresql interpreter is deprecated and merged into JDBC Interpreter. You can use it with JDBC Interpreter as same functionality. See Postgresql setting example for more detailed information.",
      "url": " /interpreter/postgresql.html",
      "group": "interpreter",
      "excerpt": "Apache Zeppelin supports PostgreSQL, Apache HAWQ(incubating) and Greenplum SQL data processing engines."
    }
    ,



    "/interpreter/python.html": {
      "title": "Python 2 &amp; 3 Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Python 2 &amp;amp; 3 Interpreter for Apache ZeppelinOverviewZeppelin supports python language which is very popular in data analytics and machine learning.      Name    Class    Description        %python    PythonInterpreter    Vanilla python interpreter, with least dependencies, only python environment installed is required, %python will use IPython if its prerequisites are met        %python.ipython    IPythonInterpreter    Provide more fancy python runtime via IPython, almost the same experience like Jupyter. It requires more things, but is the recommended interpreter for using python in Zeppelin, see below for more details        %python.sql    PythonInterpreterPandasSql    Provide sql capability to query data in Pandas DataFrame via pandasql, it can access dataframes in %python  Main Features      Feature    Description        Support vanilla Python and IPython    Vanilla Python only requires python install, IPython provides almost the same user experience like Jupyter, like inline plotting, code completion, magic methods and etc.        Built-in ZeppelinContext Support    You can use ZeppelinContext to visualize pandas dataframe        Support SQL on Pandas dataframe    You can use Sql to query dataframe which is defined in Python        Run Python in yarn cluster with customized Python runtime    You can run Python in yarn cluster with customized Python runtime without affecting each other  Play Python in Zeppelin dockerFor beginner, we would suggest you to play Python in Zeppelin docker first.In the Zeppelin docker image, we have already installedminiconda and lots of useful python librariesincluding IPython&amp;#39;s prerequisites, so %python would use IPython.Without any extra configuration, you can run most of tutorial notes under folder Python Tutorial directly.docker run -u $(id -u) -p 8080:8080 --rm --name zeppelin apache/zeppelin:0.10.0After running the above command, you can open http://localhost:8080 to play Python in Zeppelin. Configuration      Property    Default    Description        zeppelin.python    python    Path of the installed Python binary (could be python2 or python3).    You should set this property explicitly if python is not in your $PATH(example: /usr/bin/python).            zeppelin.python.maxResult    1000    Max number of dataframe rows to display.        zeppelin.python.useIPython    true    When this property is true, %python would be delegated to %python.ipython if IPython is available, otherwise    IPython is only used in %python.ipython.            zeppelin.yarn.dist.archives        Used for ipython in yarn mode. It is a general zeppelin interpreter configuration, not python specific. For Python interpreter it is used         to specify the conda env archive file which could be on local filesystem or on hadoop compatible file system.        zeppelin.interpreter.conda.env.name        Used for ipython in yarn mode. conda environment name, aka the folder name in the working directory of interpreter yarn container.  Vanilla Python Interpreter (%python)The vanilla python interpreter provides basic python interpreter feature, only python installed is required.Matplotlib integrationThe vanilla python interpreter can display matplotlib figures inline automatically using the matplotlib:%pythonimport matplotlib.pyplot as pltplt.plot([1, 2, 3])The output of this command will by default be converted to HTML by implicitly making use of the %html magic. Additional configuration can be achieved using the builtin z.configure_mpl() method. For example, z.configure_mpl(width=400, height=300, fmt=&amp;#39;svg&amp;#39;)plt.plot([1, 2, 3])Will produce a 400x300 image in SVG format, which by default are normally 600x400 and PNG respectively. In the future, another option called angular can be used to make it possible to update a plot produced from one paragraph directly from another (the output will be %angular instead of %html). However, this feature is already available in the pyspark interpreter. More details can be found in the included &amp;quot;Zeppelin Tutorial: Python - matplotlib basic&amp;quot; tutorial notebook. If Zeppelin cannot find the matplotlib backend files (which should usually be found in $ZEPPELIN_HOME/interpreter/lib/python) in your PYTHONPATH, then the backend will automatically be set to agg, and the (otherwise deprecated) instructions below can be used for more limited inline plotting.If you are unable to load the inline backend, use z.show(plt):%pythonimport matplotlib.pyplot as pltplt.figure()(.. ..)z.show(plt)plt.close()The z.show() function can take optional parameters to adapt graph dimensions (width and height) as well as output format (png or optionally svg).%pythonz.show(plt, width=&amp;#39;50px&amp;#39;)z.show(plt, height=&amp;#39;150px&amp;#39;, fmt=&amp;#39;svg&amp;#39;)IPython Interpreter (%python.ipython) (recommended)IPython is more powerful than the vanilla python interpreter with extra functionality. This is what we recommend you to use instead of vanilla python interpreter. You can use IPython with Python2 or Python3 which depends on which python you set in zeppelin.python.PrerequisitesFor non-anaconda environment, You need to install the following packagespip install jupyterpip install grpciopip install protobufFor anaconda environment (zeppelin.python points to the python under anaconda)pip install grpciopip install protobufZeppelin will check the above prerequisites when using %python, if IPython prerequisites are met, %python would use IPython interpreter, otherwise it would use vanilla Python interpreter in %python.In addition to all the basic functions of the vanilla python interpreter, you can use all the IPython advanced features as you use it in Jupyter Notebook.Take a look at tutorial note Python Tutorial/1. IPython Basic and  Python Tutorial/2. IPython Visualization Tutorial for how to use IPython in Zeppelin.Use IPython magic%python.ipython#python helprange?#timeit%timeit range(100)Use matplotlib%python.ipython%matplotlib inlineimport matplotlib.pyplot as pltprint(&amp;quot;hello world&amp;quot;)data=[1,2,3,4]plt.figure()plt.plot(data)Run shell command%python.ipython!pip install pandasColored text outputMore types of visualizatione.g. You can use hvplot in the same way as in Jupyter, Take a look at tutorial note Python Tutorial/2. IPython Visualization Tutorial for more visualization examples.Better code completionType tab can give you all the completion candidates just like in Jupyter.Pandas IntegrationApache Zeppelin Table Display System provides built-in data visualization capabilities. Python interpreter leverages it to visualize Pandas DataFrames via z.show() API.For example:By default, z.show only display 1000 rows, you can configure zeppelin.python.maxResult to adjust the max number of rows.SQL over Pandas DataFramesThere is a convenience %python.sql interpreter that matches Apache Spark experience in Zeppelin and enables usage of SQL language to query Pandas DataFrames and visualization of results through built-in Table Display System.%python.sql can access dataframes defined in %python.PrerequisitesPandas pip install pandasPandaSQL pip install -U pandasqlHere&amp;#39;s one example:first paragraph%pythonimport pandas as pdrates = pd.read_csv(&amp;quot;bank.csv&amp;quot;, sep=&amp;quot;;&amp;quot;)next paragraph%python.sqlSELECT * FROM rates WHERE age &amp;lt; 40Using Zeppelin Dynamic FormsYou can leverage Zeppelin Dynamic Form inside your Python code.Example : %python### Input formprint(z.input(&amp;quot;f1&amp;quot;,&amp;quot;defaultValue&amp;quot;))### Select formprint(z.select(&amp;quot;f2&amp;quot;,[(&amp;quot;o1&amp;quot;,&amp;quot;1&amp;quot;),(&amp;quot;o2&amp;quot;,&amp;quot;2&amp;quot;)],&amp;quot;o1&amp;quot;))### Checkbox formprint(&amp;quot;&amp;quot;.join(z.checkbox(&amp;quot;f3&amp;quot;, [(&amp;quot;o1&amp;quot;,&amp;quot;1&amp;quot;), (&amp;quot;o2&amp;quot;,&amp;quot;2&amp;quot;)],[&amp;quot;o1&amp;quot;])))ZeppelinContext APIPython interpreter create a variable z which represent ZeppelinContext for you. User can use it to do more fancy and complex things in Zeppelin.      API    Description        z.put(key, value)    Put object value with identifier key to distributed resource pool of Zeppelin,     so that it can be used by other interpreters        z.get(key)    Get object with identifier key from distributed resource pool of Zeppelin        z.remove(key)    Remove object with identifier key from distributed resource pool of Zeppelin        z.getAsDataFrame(key)    Get object with identifier key from distributed resource pool of Zeppelin and converted into pandas dataframe.    The object in the distributed resource pool must be table type, e.g. jdbc interpreter result.            z.angular(name, noteId = None, paragraphId = None)    Get the angular object with identifier name        z.angularBind(name, value, noteId = None, paragraphId = None)    Bind value to angular object with identifier name        z.angularUnbind(name, noteId = None)    Unbind value from angular object with identifier name        z.show(p)    Show python object p in Zeppelin, if it is pandas dataframe, it would be displayed in Zeppelin&#39;s table format,     others will be converted to string          z.textbox(name, defaultValue=&quot;&quot;)    Create dynamic form Textbox name with defaultValue        z.select(name, options, defaultValue=&quot;&quot;)    Create dynamic form Select name with options and defaultValue. options should be a list of Tuple(first element is key,     the second element is the displayed value) e.g. z.select(&quot;f2&quot;,[(&quot;o1&quot;,&quot;1&quot;),(&quot;o2&quot;,&quot;2&quot;)],&quot;o1&quot;)        z.checkbox(name, options, defaultChecked=[])    Create dynamic form Checkbox `name` with options and defaultChecked. options should be a list of Tuple(first element is key,     the second element is the displayed value) e.g. z.checkbox(&quot;f3&quot;, [(&quot;o1&quot;,&quot;1&quot;), (&quot;o2&quot;,&quot;2&quot;)],[&quot;o1&quot;])        z.noteTextbox(name, defaultValue=&quot;&quot;)    Create note level dynamic form Textbox        z.noteSelect(name, options, defaultValue=&quot;&quot;)    Create note level dynamic form Select        z.noteCheckbox(name, options, defaultChecked=[])    Create note level dynamic form Checkbox        z.run(paragraphId)    Run paragraph        z.run(noteId, paragraphId)    Run paragraph        z.runNote(noteId)    Run the whole note  Run Python interpreter in yarn clusterZeppelin supports to run interpreter in yarn cluster which means the python interpreter can run in a yarn container.This can achieve better multi-tenant for python interpreter especially when you already have a hadoop yarn cluster.But there&amp;#39;s one critical problem to run python in yarn cluster: how to manage the python environment in yarn container. Because hadoop yarn cluster is a distributed cluster environmentwhich is composed of many nodes, and your python interpreter can start in any node. It is not practical to manage python environment in each node beforehand.So in order to run python in yarn cluster, we would suggest you to use conda to manage your python environment, and Zeppelin can ship yourconda environment to yarn container, so that each python interpreter can have its own python environment without affecting each other.Python interpreter in yarn cluster only works for IPython, so make sure IPython&amp;#39;s prerequisites are met. So make sure including the following packages in Step 1.pythonjupytergrpcioprotobufStep 1We would suggest you to use conda-pack to create archive of conda environment, and ship it to yarn container. Otherwise python interpreterwill use the python executable file in PATH of yarn container.Here&amp;#39;s one example of yaml file which could be used to create a conda environment with python 3 and some useful python libraries.Create yaml file for conda environment, write the following content into file python_3_env.ymlname: python_3_envchannels:  - conda-forge  - defaultsdependencies:  - python=3.7   - jupyter  - grpcio  - protobuf  - pycodestyle  - numpy  - pandas  - scipy  - pandasql    - panel  - pyyaml  - seaborn  - plotnine  - hvplot  - intake  - intake-parquet  - intake-xarray  - altair  - vega_datasets  - pyarrowCreate conda environment via this yml file using either conda or mambaconda env create -f python_3_env.ymlmamba env create -f python_3_envPack the conda environment using condaconda pack -n python_3_envStep 2Specify the following properties to enable yarn mode for python interpreter.%python.confzeppelin.interpreter.launcher yarnzeppelin.yarn.dist.archives /home/hadoop/python_3_env.tar.gz#environmentzeppelin.interpreter.conda.env.name environmentSetting zeppelin.interpreter.launcher as yarn will launch python interpreter in yarn cluster.zeppelin.yarn.dist.archives is the python conda environment tar which is created in step 1.This tar will be shipped to yarn container and untar in the working directory of yarn container.environment in /home/hadoop/python_3.tar.gz#environment is the folder name after untar.This folder name should be the same as zeppelin.interpreter.conda.env.name. Usually we name it as environment here.Python environments (used for vanilla python interpreter in non-yarn mode)DefaultBy default, PythonInterpreter will use python command defined in zeppelin.python property to run python process.The interpreter can use all modules already installed (with pip, easy_install...)CondaConda is an package management system and environment management system for python.%python.conda interpreter lets you change between environments.Usageget the Conda Information: %python.conda infolist the Conda environments: %python.conda env listcreate a conda enviornment: %python.conda create --name [ENV NAME]activate an environment (python interpreter will be restarted): %python.conda activate [ENV NAME]deactivate%python.conda deactivateget installed package list inside the current environment%python.conda listinstall package%python.conda install [PACKAGE NAME]uninstall package%python.conda uninstall [PACKAGE NAME]Docker%python.docker interpreter allows PythonInterpreter creates python process in a specified docker container.Usageactivate an environment%python.docker activate [Repository]%python.docker activate [Repository:Tag]%python.docker activate [Image Id]deactivate%python.docker deactivateHere is an example# activate latest tensorflow image as a python environment%python.docker activate gcr.io/tensorflow/tensorflow:latestCommunityJoin our community to discuss with others.",
      "url": " /interpreter/python.html",
      "group": "interpreter",
      "excerpt": "Python is a programming language that lets you work quickly and integrate systems more effectively."
    }
    ,



    "/interpreter/r.html": {
      "title": "R Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;R Interpreter for Apache ZeppelinOverviewR is a free software environment for statistical computing and graphics.To run R code and visualize plots in Apache Zeppelin, you will need R on your zeppelin server node (or your dev laptop).For Centos: yum install R R-devel libcurl-devel openssl-develFor Ubuntu: apt-get install r-baseValidate your installation with a simple R command:R -e &amp;quot;print(1+1)&amp;quot;To enjoy plots, install additional libraries with:devtools with R -e &amp;quot;install.packages(&amp;#39;devtools&amp;#39;, repos = &amp;#39;http://cran.us.r-project.org&amp;#39;)&amp;quot;knitr with R -e &amp;quot;install.packages(&amp;#39;knitr&amp;#39;, repos = &amp;#39;http://cran.us.r-project.org&amp;#39;)&amp;quot;ggplot2 withR -e &amp;quot;install.packages(&amp;#39;ggplot2&amp;#39;, repos = &amp;#39;http://cran.us.r-project.org&amp;#39;)&amp;quot;Other visualization libraries: R -e &amp;quot;install.packages(c(&amp;#39;devtools&amp;#39;,&amp;#39;mplot&amp;#39;, &amp;#39;googleVis&amp;#39;), repos = &amp;#39;http://cran.us.r-project.org&amp;#39;); require(devtools); install_github(&amp;#39;ramnathv/rCharts&amp;#39;)&amp;quot;We recommend you to also install the following optional R libraries for happy data analytics:glmnetpROCdata.tablecaretsqldfwordcloudSupported InterpretersZeppelin supports R language in 3 interpreters      Name    Class    Description        %r.r    RInterpreter    Vanilla r interpreter, with least dependencies, only R environment and knitr are required.    It is always recommended to use the fully qualified interpreter name %r.r, because %r is ambiguous,     it could mean %spark.r when current note&#39;s default interpreter is %spark and %r.r when the default interpreter is %r        %r.ir    IRInterpreter    Provide more fancy R runtime via [IRKernel](https://github.com/IRkernel/IRkernel), almost the same experience like using R in Jupyter. It requires more things, but is the recommended interpreter for using R in Zeppelin.        %r.shiny    ShinyInterpreter    Run Shiny app in Zeppelin  If you want to use R with Spark, it is almost the same via %spark.r, %spark.ir &amp;amp; %spark.shiny . You can refer Spark interpreter docs for more details.Configuration      Property    Default    Description        zeppelin.R.cmd    R    Path of the installed R binary. You should set this property explicitly if R is not in your $PATH(example: /usr/bin/R).            zeppelin.R.knitr    true    Whether to use knitr or not. It is recommended to install [knitr](https://yihui.org/knitr/)        zeppelin.R.image.width    100%    Image width of R plotting        zeppelin.R.shiny.iframe_width    100%    IFrame width of Shiny App        zeppelin.R.shiny.iframe_height    500px    IFrame height of Shiny App        zeppelin.R.shiny.portRange    :    Shiny app would launch a web app at some port, this property is to specify the portRange via format &#39;start&#39;:&#39;end&#39;, e.g. &#39;5000:5001&#39;. By default it is &#39;:&#39; which means any port.        zeppelin.R.maxResult    1000    Max number of dataframe rows to display when using z.show  Play R in Zeppelin dockerFor beginner, we would suggest you to play R in Zeppelin docker first. In the Zeppelin docker image, we have already installed R and lots of useful R libraries including IRKernel&amp;#39;s prerequisites, so %r.ir is available.Without any extra configuration, you can run most of tutorial notes under folder R Tutorial directly.docker run -u $(id -u) -p 8080:8080 -p:6789:6789 --rm --name zeppelin apache/zeppelin:0.10.0After running the above command, you can open http://localhost:8080 to play R in Zeppelin.The port 6789 exposed in the above command is for R shiny app. You need to make the following 2 interpreter properties to enable shiny app accessible as iframe in Zeppelin docker container. zeppelin.R.shiny.portRange to be 6789:6789Set ZEPPELIN_LOCAL_IP to be 0.0.0.0Interpreter binding modeThe default interpreter binding mode is globally shared. That means all notes share the same R interpreter. So we would recommend you to ues isolated per note which means each note has own R interpreter without affecting each other. But it may run out of your machine resource if too many Rinterpreters are created. You can run R in yarn mode to avoid this problem. How to use R InterpreterThere are two different implementations of R interpreters: %r.r and %r.ir.Vanilla R Interpreter(%r.r) behaves like an ordinary REPL and use SparkR to communicate between R process and JVM process. It requires knitr to be installed.IRKernel R Interpreter(%r.ir) behaves like using IRKernel in Jupyter notebook. It is based on jupyter interpreter. Besides jupyter interpreter&amp;#39;s prerequisites, IRkernel needs to be installed as well. Take a look at the tutorial note R Tutorial/1. R Basics for how to write R code in Zeppelin.R basic expressionsR basic expressions are supported in both %r.r and %r.ir.R base plottingR base plotting is supported in both %r.r and %r.ir.Other plottingBesides R base plotting, you can use other visualization libraries in both %r.r and %r.ir, e.g. ggplot and googleVis z.showz.show() is only available in %r.ir to visualize R dataframe, e.g.By default, z.show would only display 1000 rows, you can specify the maxRows via z.show(df, maxRows=2000)Make Shiny App in ZeppelinShiny is an R package that makes it easy to build interactive web applications (apps) straight from R.%r.shiny is used for developing R shiny app in Zeppelin notebook. It only works when IRKernel Interpreter(%r.ir) is enabled.For developing one Shiny App in Zeppelin, you need to write at least 3 paragraphs (server type paragraph, ui type paragraph and run type paragraph)Server type R shiny paragraph%r.shiny(type=server)# Define server logic to summarize and view selected dataset ----server &amp;lt;- function(input, output) {    # Return the requested dataset ----    datasetInput &amp;lt;- reactive({        switch(input$dataset,        &amp;quot;rock&amp;quot; = rock,        &amp;quot;pressure&amp;quot; = pressure,        &amp;quot;cars&amp;quot; = cars)    })    # Generate a summary of the dataset ----    output$summary &amp;lt;- renderPrint({        dataset &amp;lt;- datasetInput()        summary(dataset)    })    # Show the first &amp;quot;n&amp;quot; observations ----    output$view &amp;lt;- renderTable({        head(datasetInput(), n = input$obs)    })}UI type R shiny paragraph%r.shiny(type=ui)# Define UI for dataset viewer app ----ui &amp;lt;- fluidPage(    # App title ----    titlePanel(&amp;quot;Shiny Text&amp;quot;),    # Sidebar layout with a input and output definitions ----    sidebarLayout(        # Sidebar panel for inputs ----        sidebarPanel(        # Input: Selector for choosing dataset ----        selectInput(inputId = &amp;quot;dataset&amp;quot;,        label = &amp;quot;Choose a dataset:&amp;quot;,        choices = c(&amp;quot;rock&amp;quot;, &amp;quot;pressure&amp;quot;, &amp;quot;cars&amp;quot;)),        # Input: Numeric entry for number of obs to view ----        numericInput(inputId = &amp;quot;obs&amp;quot;,        label = &amp;quot;Number of observations to view:&amp;quot;,        value = 10)        ),        # Main panel for displaying outputs ----        mainPanel(        # Output: Verbatim text for data summary ----        verbatimTextOutput(&amp;quot;summary&amp;quot;),        # Output: HTML table with requested number of observations ----        tableOutput(&amp;quot;view&amp;quot;)        )    ))Run type R shiny paragraph%r.shiny(type=run)After executing the run type R shiny paragraph, the shiny app will be launched and embedded as iframe in paragraph.Take a look at the tutorial note R Tutorial/2. Shiny App for how to develop R shiny app.Run multiple shiny appsIf you want to run multiple shiny apps, you can specify app in paragraph local property to differentiate different shiny apps.e.g.%r.shiny(type=ui, app=app_1)%r.shiny(type=server, app=app_1)%r.shiny(type=run, app=app_1)Run R in yarn clusterZeppelin support to run interpreter in yarn cluster. But there&amp;#39;s one critical problem to run R in yarn cluster: how to manage the R environment in yarn container. Because yarn cluster is a distributed cluster which is composed of many nodes, and your R interpreter can start in any node. It is not practical to manage R environment in each node.So in order to run R in yarn cluster, we would suggest you to use conda to manage your R environment, and Zeppelin can ship yourR conda environment to yarn container, so that each R interpreter can have its own R environment without affecting each other.To be noticed, you can only run IRKernel interpreter(%r.ir) in yarn cluster. So make sure you include at least the following prerequisites in the below conda env:pythonjupytergrpcioprotobufr-baser-essentialsr-irkernelpython, jupyter, grpcio and protobuf are required for jupyter interpreter, because IRKernel interpreter is based on jupyter interpreter. Others are for R runtime.Following are instructions of how to run R in yarn cluster. You can find all the code in the tutorial note R Tutorial/3. R Conda Env in Yarn Mode.Step 1We would suggest you to use conda pack to create archive of conda environment.Here&amp;#39;s one example of yaml file which is used to generate a conda environment with R and some useful R libraries.Create a yaml file for conda environment, write the following content into file r_env.ymlname: r_envchannels:  - conda-forge  - defaultsdependencies:  - python=3.7   - jupyter  - grpcio  - protobuf  - r-base=3  - r-essentials  - r-evaluate  - r-base64enc  - r-knitr  - r-ggplot2  - r-irkernel  - r-shiny  - r-googlevisCreate conda environment via this yaml file using either conda or mambaconda env create -f r_env.ymlmamba env create -f r_env.ymlPack the conda environment using condaconda pack -n r_envStep 2Specify the following properties to enable yarn mode for R interpreter via inline configuration%r.confzeppelin.interpreter.launcher yarnzeppelin.yarn.dist.archives hdfs:///tmp/r_env.tar.gz#environmentzeppelin.interpreter.conda.env.name environmentzeppelin.yarn.dist.archives is the R conda environment tar file which is created in step 1. This tar will be shipped to yarn container and untar in the working directory of yarn container.hdfs:///tmp/r_env.tar.gz is the R conda archive file you created in step 2. environment in hdfs:///tmp/r_env.tar.gz#environment is the folder name after untar. This folder name should be the same as zeppelin.interpreter.conda.env.name.Step 3Now you can use run R interpreter in yarn container and also use any R libraries you specify in step 1.",
      "url": " /interpreter/r.html",
      "group": "interpreter",
      "excerpt": "R is a free software environment for statistical computing and graphics."
    }
    ,



    "/interpreter/sap.html": {
      "title": "SAP BusinessObjects Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;SAP BusinessObjects (Universe) Interpreter for Apache ZeppelinOverviewSAP BusinessObjects BI platform (universes) can simplify the lives of business users and IT staff. SAP BusinessObjects is based on universes. The universe contains dual-semantic layer model. The users make queries upon universes. This interpreter is new interface for universes.Disclaimer SAP interpreter is not official interpreter for SAP BusinessObjects BI platform. It uses BI Semantic Layer REST APIThis interpreter is not directly supported by SAP AG.Tested with versions 4.2SP3 (14.2.3.2220) and 4.2SP5. There is no support for filters in UNX-universes converted from old UNV format.The universe name must be unique.Configuring SAP Universe InterpreterAt the &amp;quot;Interpreters&amp;quot; menu, you can edit SAP interpreter or create new one. Zeppelin provides these properties for SAP.      Property Name    Value    Description        universe.api.url    http://localhost:6405/biprws    The base url for the SAP BusinessObjects BI platform. You have to edit &quot;localhost&quot; that you may use (ex. http://0.0.0.0:6405/biprws)        universe.authType    secEnterprise    The type of authentication for API of Universe. Available values: secEnterprise, secLDAP, secWinAD, secSAPR3        universe.password        The BI platform user password        universe.user    Administrator    The BI platform user login  How to use Choose the universe Choose dimensions and measures in select statement Define conditions in where statementYou can compare two dimensions/measures or use Filter (without value). Dimesions/Measures can be compared with static values, may be is null or is not null, contains or not in list.Available the nested conditions (using braces &amp;quot;()&amp;quot;). &amp;quot;and&amp;quot; operator have more priority than &amp;quot;or&amp;quot;. If generated query contains promtps, then promtps will appear as dynamic form after paragraph submitting.Example query%sapuniverse [Universe Name];select  [Folder1].[Dimension2],  [Folder2].[Dimension3],  [Measure1]where  [Filter1]  and [Date] &amp;gt; &amp;#39;2018-01-01 00:00:00&amp;#39;  and [Folder1].[Dimension4] is not null  and [Folder1].[Dimension5] in (&amp;#39;Value1&amp;#39;, &amp;#39;Value2&amp;#39;);distinct keywordYou can write keyword distinct after keyword select to return only distinct (different) values.Example query%sapuniverse [Universe Name];select distinct  [Folder1].[Dimension2], [Measure1]where  [Filter1];limit keywordYou can write keyword limit and limit value in the end of query to limit the number of records returned based on a limit value.Example query%sapuniverse [Universe Name];select  [Folder1].[Dimension2], [Measure1]where  [Filter1]limit 100;Object InterpolationThe SAP interpreter also supports interpolation of ZeppelinContext objects into the paragraph text.To enable this feature set universe.interpolation to true. The following example shows one use of this facility:In Scala cell:z.put(&amp;quot;curr_date&amp;quot;, &amp;quot;2018-01-01 00:00:00&amp;quot;)In later SAP cell:where   [Filter1]   and [Date] &amp;gt; &amp;#39;{curr_date}&amp;#39;",
      "url": " /interpreter/sap.html",
      "group": "interpreter",
      "excerpt": "SAP BusinessObjects BI platform can simplify the lives of business users and IT staff. SAP BusinessObjects is based on universes. The universe contains dual-semantic layer model. The users make queries upon universes. This interpreter is new interface for universes."
    }
    ,



    "/interpreter/scalding.html": {
      "title": "Scalding Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Scalding Interpreter for Apache ZeppelinScalding is an open source Scala library for writing MapReduce jobs.Building the Scalding InterpreterYou have to first build the Scalding interpreter by enable the scalding profile as follows:mvn clean package -Pscalding -DskipTestsEnabling the Scalding InterpreterIn a notebook, to enable the Scalding interpreter, click on the Gear icon,select Scalding, and hit Save.Configuring the InterpreterScalding interpreter runs in two modes:localhdfsIn the local mode, you can access files on the local server and scalding transformation are done locally.In hdfs mode you can access files in HDFS and scalding transformation are run as hadoop map-reduce jobs.Zeppelin comes with a pre-configured Scalding interpreter in local mode.To run the scalding interpreter in the hdfs mode you have to do the following:Set the classpath with ZEPPELIN_CLASSPATH_OVERRIDESIn conf/zeppelinenv.sh, you have to setZEPPELINCLASSPATH_OVERRIDES to the contents of &amp;#39;hadoop classpath&amp;#39;and directories with custom jar files you need for your scalding commands.Set arguments to the scalding replThe default arguments are: --local --replFor hdfs mode you need to add: --hdfs --replIf you want to add custom jars, you need to add: -libjars directory/*:directory/*For reducer estimation, you need to add something like:-Dscalding.reducer.estimator.classes=com.twitter.scalding.reducer_estimation.InputSizeReducerEstimatorSet max.open.instancesIf you want to control the maximum number of open interpreters, you have to select &amp;quot;scoped&amp;quot; interpreter for noteoption and set max.open.instances argument.Testing the InterpreterLocal modeIn example, by using the Alice in Wonderland tutorial, we will count words (of course!), and plot a graph of the top 10 words in the book.%scaldingimport scala.io.Source// Get the Alice in Wonderland book from gutenberg.org:val alice = Source.fromURL(&amp;quot;http://www.gutenberg.org/files/11/11.txt&amp;quot;).getLinesval aliceLineNum = alice.zipWithIndex.toListval alicePipe = TypedPipe.from(aliceLineNum)// Now get a list of words for the book:val aliceWords = alicePipe.flatMap { case (text, _) =&amp;gt; text.split(&amp;quot;s+&amp;quot;).toList }// Now lets add a count for each word:val aliceWithCount = aliceWords.filterNot(_.equals(&amp;quot;&amp;quot;)).map { word =&amp;gt; (word, 1L) }// let&amp;#39;s sum them for each word:val wordCount = aliceWithCount.group.sumprint (&amp;quot;Here are the top 10 wordsn&amp;quot;)val top10 = wordCount  .groupAll  .sortBy { case (word, count) =&amp;gt; -count }  .take(10)top10.dump%scaldingval table = &amp;quot;wordst countn&amp;quot; + top10.toIterator.map{case (k, (word, count)) =&amp;gt; s&amp;quot;$wordt$count&amp;quot;}.mkString(&amp;quot;n&amp;quot;)print(&amp;quot;%table &amp;quot; + table)If you click on the icon for the pie chart, you should be able to see a chart like this:HDFS modeTest mode%scaldingmodeThis command should print:res4: com.twitter.scalding.Mode = Hdfs(true,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml)Test HDFS readval testfile = TypedPipe.from(TextLine(&amp;quot;/user/x/testfile&amp;quot;))testfile.dumpThis command should print the contents of the hdfs file /user/x/testfile.Test map-reduce jobval testfile = TypedPipe.from(TextLine(&amp;quot;/user/x/testfile&amp;quot;))val a = testfile.groupAll.size.valuesa.toListThis command should create a map reduce job.Future WorkBetter user feedback (hadoop url, progress updates)Ability to cancel jobsAbility to dynamically load jars without restarting the interpreterMultiuser scalability (run scalding interpreters on different servers)",
      "url": " /interpreter/scalding.html",
      "group": "interpreter",
      "excerpt": "Scalding is an open source Scala library for writing MapReduce jobs."
    }
    ,



    "/interpreter/scio.html": {
      "title": "Scio Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Scio Interpreter for Apache ZeppelinOverviewScio is a Scala DSL for Google Cloud Dataflow and Apache Beam inspired by Spark and Scalding. See the current wiki and API documentation for more information.Configuration      Name    Default Value    Description        zeppelin.scio.argz    --runner=InProcessPipelineRunner    Scio interpreter wide arguments. Documentation: https://github.com/spotify/scio/wiki#options and https://cloud.google.com/dataflow/pipelines/specifying-exec-params        zeppelin.scio.maxResult    1000    Max number of SCollection results to display  Enabling the Scio InterpreterIn a notebook, to enable the Scio interpreter, click the Gear icon and select beam (beam.scio).Using the Scio InterpreterIn a paragraph, use %beam.scio to select the Scio interpreter. You can use it much the same way as vanilla Scala REPL and Scio REPL. State (like variables, imports, execution etc) is shared among all Scio paragraphs. There is a special variable argz which holds arguments from Scio interpreter settings. The easiest way to proceed is to create a Scio context via standard ContextAndArgs.%beam.scioval (sc, args) = ContextAndArgs(argz)Use sc context the way you would in a regular pipeline/REPL.Example:%beam.scioval (sc, args) = ContextAndArgs(argz)sc.parallelize(Seq(&amp;quot;foo&amp;quot;, &amp;quot;foo&amp;quot;, &amp;quot;bar&amp;quot;)).countByValue.closeAndDisplay()If you close Scio context, go ahead an create a new one using ContextAndArgs. Please refer to Scio wiki for more complex examples. You can close Scio context much the same way as in Scio REPL, and use Zeppelin display helpers to synchronously close and display results - read more below.ProgressThere can be only one paragraph running at once. There is no notion of overall progress, thus progress bar will show 0.SCollection display helpersScio interpreter comes with display helpers to ease working with Zeppelin notebooks. Simply use closeAndDisplay() on SCollection to close context and display the results. The number of results is limited by zeppelin.scio.maxResult (by default 1000).Supported SCollection types:Scio&amp;#39;s typed BigQueryScala&amp;#39;s Products (case classes, tuples)Google BigQuery&amp;#39;s TableRowApache AvroAll Scala&amp;#39;s AnyValHelper methodsThere are different helper methods for different objects. You can easily display results from SCollection, Future[Tap] and Tap.SCollection helperSCollection has closeAndDisplay Zeppelin helper method for types listed above. Use it to synchronously close Scio context, and once available pull and display results.Future[Tap] helperFuture[Tap] has waitAndDisplay Zeppelin helper method for types listed above. Use it to synchronously wait for results, and once available pull and display results.Tap helperTap has display Zeppelin helper method for types listed above. Use it to pull and display results.ExamplesBigQuery example:%beam.scio@BigQueryType.fromQuery(&amp;quot;&amp;quot;&amp;quot;|SELECT departure_airport,count(case when departure_delay&amp;gt;0 then 1 else 0 end) as no_of_delays                           |FROM [bigquery-samples:airline_ontime_data.flights]                           |group by departure_airport                           |order by 2 desc                           |limit 10&amp;quot;&amp;quot;&amp;quot;.stripMargin) class Flightsval (sc, args) = ContextAndArgs(argz)sc.bigQuerySelect(Flights.query).closeAndDisplay(Flights.schema)BigQuery typed example:%beam.scio@BigQueryType.fromQuery(&amp;quot;&amp;quot;&amp;quot;|SELECT departure_airport,count(case when departure_delay&amp;gt;0 then 1 else 0 end) as no_of_delays                           |FROM [bigquery-samples:airline_ontime_data.flights]                           |group by departure_airport                           |order by 2 desc                           |limit 10&amp;quot;&amp;quot;&amp;quot;.stripMargin) class Flightsval (sc, args) = ContextAndArgs(argz)sc.typedBigQuery[Flights]().flatMap(_.no_of_delays).mean.closeAndDisplay()Avro example:%beam.scioimport com.spotify.data.ExampleAvroval (sc, args) = ContextAndArgs(argz)sc.avroFile[ExampleAvro](&amp;quot;gs://&amp;lt;bucket&amp;gt;/tmp/my.avro&amp;quot;).take(10).closeAndDisplay()Avro example with a view schema:%beam.scioimport com.spotify.data.ExampleAvroimport org.apache.avro.Schemaval (sc, args) = ContextAndArgs(argz)val view = Schema.parse(&amp;quot;&amp;quot;&amp;quot;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;ExampleAvro&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;com.spotify.data&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;track&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;}, {&amp;quot;name&amp;quot;:&amp;quot;artist&amp;quot;, &amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;}]}&amp;quot;&amp;quot;&amp;quot;)sc.avroFile[EndSongCleaned](&amp;quot;gs://&amp;lt;bucket&amp;gt;/tmp/my.avro&amp;quot;).take(10).closeAndDisplay(view)Google credentialsScio Interpreter will try to infer your Google Cloud credentials from its environment, it will take into the account:argz interpreter settings (doc)environment variable (GOOGLE_APPLICATION_CREDENTIALS)gcloud configurationBigQuery macro credentialsCurrently BigQuery project for macro expansion is inferred using Google Dataflow&amp;#39;s DefaultProjectFactory().create()",
      "url": " /interpreter/scio.html",
      "group": "interpreter",
      "excerpt": "Scio is a Scala DSL for Apache Beam/Google Dataflow model."
    }
    ,



    "/interpreter/shell.html": {
      "title": "Shell interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Shell interpreter for Apache ZeppelinOverviewZeppelin Shell has two interpreters the default is the %sh interpreter.Shell interpreterShell interpreter uses Apache Commons Exec to execute external processes. In Zeppelin notebook, you can use %sh in the beginning of a paragraph to invoke system shell and run commands.Terminal interpreterTerminal interpreter uses hterm, Pty4J analog terminal operation.Note : Currently each command runs as the user Zeppelin server is running as.ConfigurationAt the &amp;quot;Interpreters&amp;quot; menu in Zeppelin dropdown menu, you can set the property value for Shell interpreter.      Name    Default    Description        shell.command.timeout.millisecs    60000    Shell command time out in millisecs        shell.working.directory.user.home    false    If this set to true, the shell&#39;s working directory will be set to user home        zeppelin.shell.auth.type        Types of authentications&#39; methods supported are SIMPLE, and KERBEROS        zeppelin.shell.principal        The principal name to load from the keytab        zeppelin.shell.keytab.location        The path to the keytab file        zeppelin.shell.interpolation    false    Enable ZeppelinContext variable interpolation into paragraph text        zeppelin.terminal.ip.mapping        Internal and external IP mapping of zeppelin server        zeppelin.concurrency.max    10    Max concurrency of shell interpreter  ExampleShell interpreterThe following example demonstrates the basic usage of Shell in a Zeppelin notebook.If you need further information about Zeppelin Interpreter Setting for using Shell interpreter, please read What is interpreter setting? section first.Kerberos refresh intervalFor changing the default behavior of when to renew Kerberos ticket following changes can be made in conf/zeppelin-env.sh.# Change Kerberos refresh interval (default value is 1d). Allowed postfix are ms, s, m, min, h, and d.export KERBEROS_REFRESH_INTERVAL=4h# Change kinit number retries (default value is 5), which means if the kinit command fails for 5 retries consecutively it will close the interpreter. export KINIT_FAIL_THRESHOLD=10Object InterpolationThe shell interpreter also supports interpolation of ZeppelinContext objects into the paragraph text.The following example shows one use of this facility:In Scala cell:z.put(&amp;quot;dataFileName&amp;quot;, &amp;quot;members-list-003.parquet&amp;quot;)    // ...val members = spark.read.parquet(z.get(&amp;quot;dataFileName&amp;quot;))    // ...In later Shell cell:%shrm -rf {dataFileName}Object interpolation is disabled by default, and can be enabled (for the Shell interpreter) bysetting the value of the property zeppelin.shell.interpolation to true (see Configuration above).More details of this feature can be found in Zeppelin-ContextTerminal interpreterThe following example demonstrates the basic usage of terminal in a Zeppelin notebook.%sh.terminalinput any charzeppelin.terminal.ip.mappingWhen running the terminal interpreter in the notebook, the front end of the notebook needs to obtain the IP address of the server where the terminal interpreter is located to communicate.In a public cloud environment, the cloud host has an internal IP and an external access IP, and the interpreter runs in the cloud host. This will cause the notebook front end to be unable to connect to the terminal interpreter properly, resulting in the terminal interpreter being unusable.Solution: Set the mapping between internal IP and external IP in the terminal interpreter, and connect the front end of the notebook through the external IP of the terminal interpreter.Example: {&amp;quot;internal-ip1&amp;quot;:&amp;quot;external-ip1&amp;quot;, &amp;quot;internal-ip2&amp;quot;:&amp;quot;external-ip2&amp;quot;}",
      "url": " /interpreter/shell.html",
      "group": "interpreter",
      "excerpt": "Shell interpreter uses Apache Commons Exec to execute external processes."
    }
    ,



    "/interpreter/spark.html": {
      "title": "Apache Spark Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Spark Interpreter for Apache ZeppelinOverviewApache Spark is a fast and general-purpose cluster computing system.It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.Apache Spark is supported in Zeppelin with Spark interpreter group which consists of following interpreters.      Name    Class    Description        %spark    SparkInterpreter    Creates a SparkContext/SparkSession and provides a Scala environment        %spark.pyspark    PySparkInterpreter    Provides a Python environment        %spark.ipyspark    IPySparkInterpreter    Provides a IPython environment        %spark.r    SparkRInterpreter    Provides an vanilla R environment with SparkR support        %spark.ir    SparkIRInterpreter    Provides an R environment with SparkR support based on Jupyter IRKernel        %spark.shiny    SparkShinyInterpreter    Used to create R shiny app with SparkR support        %spark.sql    SparkSQLInterpreter    Provides a SQL environment        %spark.kotlin    KotlinSparkInterpreter    Provides a Kotlin environment  Main Features      Feature    Description        Support multiple versions of Spark    You can run different versions of Spark in one Zeppelin instance        Support multiple versions of Scala    You can run different Scala versions (2.10/2.11/2.12) of Spark in on Zeppelin instance        Support multiple languages    Scala, SQL, Python, R are supported, besides that you can also collaborate across languages, e.g. you can write Scala UDF and use it in PySpark        Support multiple execution modes    Local | Standalone | Yarn | K8s         Interactive development    Interactive development user experience increase your productivity        Inline Visualization    You can visualize Spark Dataset/DataFrame vis Python/R&#39;s plotting libraries, and even you can make SparkR Shiny app in Zeppelin        Multi-tenancy    Multiple user can work in one Zeppelin instance without affecting each other.        Rest API Support    You can not only submit Spark job via Zeppelin notebook UI, but also can do that via its rest api (You can use Zeppelin as Spark job server).  Play Spark in Zeppelin dockerFor beginner, we would suggest you to play Spark in Zeppelin docker.In the Zeppelin docker image, we have already installedminiconda and lots of useful python and R librariesincluding IPython and IRkernel prerequisites, so %spark.pyspark would use IPython and %spark.ir is enabled.Without any extra configuration, you can run most of tutorial notes under folder Spark Tutorial directly.First you need to download Spark, because there&amp;#39;s no Spark binary distribution shipped with Zeppelin.e.g. Here we download Spark 3.1.2 to/mnt/disk1/spark-3.1.2,and we mount it to Zeppelin docker container and run the following command to start Zeppelin docker container.docker run -u $(id -u) -p 8080:8080 -p 4040:4040 --rm -v /mnt/disk1/spark-3.1.2:/opt/spark -e SPARK_HOME=/opt/spark  --name zeppelin apache/zeppelin:0.10.0After running the above command, you can open http://localhost:8080 to play Spark in Zeppelin. We only verify the spark local mode in Zeppelin docker, other modes may not work due to network issues.-p 4040:4040 is to expose Spark web ui, so that you can access Spark web ui via http://localhost:8081.ConfigurationThe Spark interpreter can be configured with properties provided by Zeppelin.You can also set other Spark properties which are not listed in the table. For a list of additional properties, refer to Spark Available Properties.      Property    Default    Description        SPARK_HOME        Location of spark distribution        spark.master    local[*]    Spark master uri.  e.g. spark://masterhost:7077        spark.submit.deployMode        The deploy mode of Spark driver program, either &amp;quot;client&amp;quot; or &amp;quot;cluster&amp;quot;, Which means to launch driver program locally (&amp;quot;client&amp;quot;) or remotely (&amp;quot;cluster&amp;quot;) on one of the nodes inside the cluster.      spark.app.name    Zeppelin    The name of spark application.        spark.driver.cores    1    Number of cores to use for the driver process, only in cluster mode.        spark.driver.memory    1g    Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix (&amp;quot;k&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;g&amp;quot; or &amp;quot;t&amp;quot;) (e.g. 512m, 2g).        spark.executor.cores    1    The number of cores to use on each executor        spark.executor.memory    1g    Executor memory per worker instance.  e.g. 512m, 32g        spark.executor.instances    2    The number of executors for static allocation        spark.files        Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.        spark.jars        Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.        spark.jars.packages        Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories.        PYSPARK_PYTHON    python    Python binary executable to use for PySpark in both driver and executors (default is python).            Property spark.pyspark.python take precedence if it is set        PYSPARK_DRIVER_PYTHON    python    Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON).            Property spark.pyspark.driver.python take precedence if it is set        zeppelin.pyspark.useIPython    false    Whether use IPython when the ipython prerequisites are met in %spark.pyspark        zeppelin.R.cmd    R    R binary executable path.        zeppelin.spark.concurrentSQL    false    Execute multiple SQL concurrently if set true.        zeppelin.spark.concurrentSQL.max    10    Max number of SQL concurrently executed        zeppelin.spark.maxResult    1000    Max number rows of Spark SQL result to display.        zeppelin.spark.run.asLoginUser    true    Whether run spark job as the zeppelin login user, it is only applied when running spark job in hadoop yarn cluster and shiro is enabled.        zeppelin.spark.printREPLOutput    true    Print scala REPL output        zeppelin.spark.useHiveContext    true    Use HiveContext instead of SQLContext if it is true. Enable hive for SparkSession        zeppelin.spark.enableSupportedVersionCheck    true    Do not change - developer only setting, not for production use        zeppelin.spark.sql.interpolation    false    Enable ZeppelinContext variable interpolation into spark sql      zeppelin.spark.uiWebUrl              Overrides Spark UI default URL. Value should be a full URL (ex: http://{hostName}/{uniquePath}.      In Kubernetes mode, value can be Jinja template string with 3 template variables PORT,  SERVICENAME   and   SERVICEDOMAIN .      (e.g.: http://{{PORT}}-{{SERVICENAME}}.{{SERVICEDOMAIN}} ). In yarn mode, value could be a knox url with  {{applicationId}}  as placeholder,      (e.g.: https://knox-server:8443/gateway/yarnui/yarn/proxy/{{applicationId}}/)             spark.webui.yarn.useProxy    false    whether use yarn proxy url as Spark weburl, e.g. http://localhost:8088/proxy/application1583396598068_0004        spark.repl.target    jvm-1.6          Manually specifying the Java version of Spark Interpreter Scala REPL,Available options:       scala-compile v2.10.7 to v2.11.12 supports &amp;quot;jvm-1.5, jvm-1.6, jvm-1.7 and jvm-1.8&amp;quot;, and the default value is jvm-1.6.       scala-compile v2.10.1 to v2.10.6 supports &amp;quot;jvm-1.5, jvm-1.6, jvm-1.7&amp;quot;, and the default value is jvm-1.6.       scala-compile v2.12.x defaults to jvm-1.8, and only supports jvm-1.8.      Without any configuration, Spark interpreter works out of box in local mode. But if you want to connect to your Spark cluster, you&amp;#39;ll need to follow below two simple steps.Set SPARK_HOMESet masterSet SPARK_HOMEThere are several options for setting SPARK_HOME.Set SPARK_HOME in zeppelin-env.shSet SPARK_HOME in interpreter setting pageSet SPARK_HOME via inline generic configuration Set SPARK_HOME in zeppelin-env.shIf you work with only one version of Spark, then you can set SPARK_HOME in zeppelin-env.sh because any setting in zeppelin-env.sh is globally applied.e.g. export SPARK_HOME=/usr/lib/sparkYou can optionally set more environment variables in zeppelin-env.sh# set hadoop conf direxport HADOOP_CONF_DIR=/usr/lib/hadoopSet SPARK_HOME in interpreter setting pageIf you want to use multiple versions of Spark, then you need to create multiple Spark interpreters and set SPARK_HOME separately. e.g.Create a new Spark interpreter spark24 for Spark 2.4 and set its SPARK_HOME in interpreter setting page as following,Create a new Spark interpreter spark16 for Spark 1.6 and set its SPARK_HOME in interpreter setting page as following,Set SPARK_HOME via inline generic configurationBesides setting SPARK_HOME in interpreter setting page, you can also use inline generic configuration to put the configuration with code together for more flexibility. e.g.Set masterAfter setting SPARK_HOME, you need to set spark.master property in either interpreter setting page or inline configuartion. The value may vary depending on your Spark cluster deployment type.For example,local[*] in local modespark://master:7077 in standalone clusteryarn-client in Yarn client mode  (Not supported in Spark 3.x, refer below for how to configure yarn-client in Spark 3.x)yarn-cluster in Yarn cluster mode  (Not supported in Spark 3.x, refer below for how to configure yarn-cluster in Spark 3.x)mesos://host:5050 in Mesos clusterThat&amp;#39;s it. Zeppelin will work with any version of Spark and any deployment type without rebuilding Zeppelin in this way.For the further information about Spark &amp;amp; Zeppelin version compatibility, please refer to &amp;quot;Available Interpreters&amp;quot; section in Zeppelin download page.Note that without setting SPARK_HOME, it&amp;#39;s running in local mode with included version of Spark. The included version may vary depending on the build profile. And this included version Spark has limited function, so it is always recommended to set SPARK_HOME.Yarn client mode and local mode will run driver in the same machine with zeppelin server, this would be dangerous for production. Because it may run out of memory when there&amp;#39;s many Spark interpreters running at the same time. So we suggest you only allow yarn-cluster mode via setting zeppelin.spark.only_yarn_cluster in zeppelin-site.xml.Configure yarn mode for Spark 3.xSpecifying yarn-client &amp;amp; yarn-cluster in spark.master is not supported in Spark 3.x any more, instead you need to use spark.master and spark.submit.deployMode together.      Mode    spark.master    spark.submit.deployMode        Yarn Client    yarn    client        Yarn Cluster    yarn    cluster    Interpreter binding modeThe default interpreter binding mode is globally shared. That means all notes share the same Spark interpreter.So we recommend you to use isolated per note which means each note has own Spark interpreter without affecting each other. But it may run out of your machine resource if too manySpark interpreters are created, so we recommend to always use yarn-cluster mode in production if you run Spark in hadoop cluster. And you can use inline configuration via %spark.conf in the first paragraph to customize your spark configuration.You can also choose scoped mode. For scoped per note mode, Zeppelin creates separated scala compiler/python shell for each note but share a single SparkContext/SqlContext/SparkSession.SparkContext, SQLContext, SparkSession, ZeppelinContextSparkContext, SQLContext, SparkSession (for spark 2.x, 3.x) and ZeppelinContext are automatically created and exposed as variable names sc, sqlContext, spark and z respectively, in Scala, Kotlin, Python and R environments.Note that Scala/Python/R environment shares the same SparkContext, SQLContext, SparkSession and ZeppelinContext instance.Yarn ModeZeppelin support both yarn client and yarn cluster mode (yarn cluster mode is supported from 0.8.0). For yarn mode, you must specify SPARK_HOME &amp;amp; HADOOP_CONF_DIR. Usually you only have one hadoop cluster, so you can set HADOOP_CONF_DIR in zeppelin-env.sh which is applied to all Spark interpreters. If you want to use spark against multiple hadoop cluster, then you need to defineHADOOP_CONF_DIR in interpreter setting or via inline generic configuration.K8s ModeRegarding how to run Spark on K8s in Zeppelin, please check this doc.PySparkThere are 2 ways to use PySpark in Zeppelin:Vanilla PySparkIPySparkVanilla PySpark (Not Recommended)Vanilla PySpark interpreter is almost the same as vanilla Python interpreter except Spark interpreter inject SparkContext, SQLContext, SparkSession via variables sc, sqlContext, spark.By default, Zeppelin would use IPython in %spark.pyspark when IPython is available (Zeppelin would check whether ipython&amp;#39;s prerequisites are met), Otherwise it would fall back to the vanilla PySpark implementation.IPySpark (Recommended)You can use IPySpark explicitly via %spark.ipyspark. IPySpark interpreter is almost the same as IPython interpreter except Spark interpreter inject SparkContext, SQLContext, SparkSession via variables sc, sqlContext, spark.For the IPython features, you can refer doc Python InterpreterSparkRZeppelin support SparkR via %spark.r, %spark.ir and %spark.shiny. Here&amp;#39;s configuration for SparkR Interpreter.      Spark Property    Default    Description        zeppelin.R.cmd    R    R binary executable path.        zeppelin.R.knitr    true    Whether use knitr or not. (It is recommended to install knitr and use it in Zeppelin)        zeppelin.R.image.width    100%    R plotting image width.        zeppelin.R.render.options    out.format = &#39;html&#39;, comment = NA, echo = FALSE, results = &#39;asis&#39;, message = F, warning = F, fig.retina = 2    R plotting options.        zeppelin.R.shiny.iframe_width    100%    IFrame width of Shiny App        zeppelin.R.shiny.iframe_height    500px    IFrame height of Shiny App        zeppelin.R.shiny.portRange    :    Shiny app would launch a web app at some port, this property is to specify the portRange via format &#39;:&#39;, e.g. &#39;5000:5001&#39;. By default it is &#39;:&#39; which means any port  Refer R doc for how to use R in Zeppelin.SparkSqlSpark sql interpreter share the same SparkContext/SparkSession with other Spark interpreters. That means any table registered in scala, python or r code can be accessed by Spark sql.For examples:%sparkcase class People(name: String, age: Int)var df = spark.createDataFrame(List(People(&amp;quot;jeff&amp;quot;, 23), People(&amp;quot;andy&amp;quot;, 20)))df.createOrReplaceTempView(&amp;quot;people&amp;quot;)%spark.sqlselect * from peopleYou can write multiple sql statements in one paragraph. Each sql statement is separated by semicolon.Sql statement in one paragraph would run sequentially. But sql statements in different paragraphs can run concurrently by the following configuration.Set zeppelin.spark.concurrentSQL to true to enable the sql concurrent feature, underneath zeppelin will change to use fairscheduler for Spark. And also set zeppelin.spark.concurrentSQL.max to control the max number of sql statements running concurrently.Configure pools by creating fairscheduler.xml under your SPARK_CONF_DIR, check the official spark doc Configuring Pool PropertiesSet pool property via setting paragraph local property. e.g.%spark(pool=pool1)sql statementThis pool feature is also available for all versions of scala Spark, PySpark. For SparkR, it is only available starting from 2.3.0.Dependency ManagementFor Spark interpreter, it is not recommended to use Zeppelin&amp;#39;s Dependency Management for managingthird party dependencies (%spark.dep is removed from Zeppelin 0.9 as well). Instead, you should set the standard Spark properties as following:      Spark Property    Spark Submit Argument    Description        spark.files    --files    Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.        spark.jars    --jars    Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.        spark.jars.packages    --packages    Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories.  As general Spark properties, you can set them in via inline configuration or interpreter setting page or in zeppelin-env.sh via environment variable SPARK_SUBMIT_OPTIONS.For examples:export SPARK_SUBMIT_OPTIONS=&amp;quot;--files &amp;lt;my_file&amp;gt; --jars &amp;lt;my_jar&amp;gt; --packages &amp;lt;my_package&amp;gt;&amp;quot;To be noticed, SPARK_SUBMIT_OPTIONS is deprecated and will be removed in future release.ZeppelinContextZeppelin automatically injects ZeppelinContext as variable z in your Scala/Python environment. ZeppelinContext provides some additional functions and utilities.See Zeppelin-Context for more details. For Spark interpreter, you can use z to display Spark Dataset/Dataframe.Setting up Zeppelin with KerberosLogical setup with Zeppelin, Kerberos Key Distribution Center (KDC), and Spark on YARN:There are several ways to make Spark work with kerberos enabled hadoop cluster in Zeppelin. Share one single hadoop cluster.In this case you just need to specify zeppelin.server.kerberos.keytab and zeppelin.server.kerberos.principal in zeppelin-site.xml, Spark interpreter will use these setting by default.Work with multiple hadoop clusters.In this case you can specify spark.yarn.keytab and spark.yarn.principal to override zeppelin.server.kerberos.keytab and zeppelin.server.kerberos.principal.Configuration SetupOn the server that Zeppelin is installed, install Kerberos client modules and configuration, krb5.conf.This is to make the server communicate with KDC.Add the two properties below to Spark configuration ([SPARK_HOME]/conf/spark-defaults.conf):spark.yarn.principalspark.yarn.keytabNOTE: If you do not have permission to access for the above spark-defaults.conf file, optionally, you can add the above lines to the Spark Interpreter setting through the Interpreter tab in the Zeppelin UI.That&amp;#39;s it. Play with Zeppelin!User ImpersonationIn yarn mode, the user who launch the zeppelin server will be used to launch the Spark yarn application. This is not a good practise.Most of time, you will enable shiro in Zeppelin and would like to use the login user to submit the Spark yarn app. For this purpose,you need to enable user impersonation for more security control. In order the enable user impersonation, you need to do the following stepsStep 1 Enable user impersonation setting hadoop&amp;#39;s core-site.xml. E.g. if you are using user zeppelin to launch Zeppelin, then add the following to core-site.xml, then restart both hdfs and yarn. &amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;hadoop.proxyuser.zeppelin.groups&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;hadoop.proxyuser.zeppelin.hosts&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;Step 2 Enable interpreter user impersonation in Spark interpreter&amp;#39;s interpreter setting. (Enable shiro first of course)Step 3(Optional) If you are using kerberos cluster, then you need to set zeppelin.server.kerberos.keytab and zeppelin.server.kerberos.principal to the user(aka. user in Step 1) you want to impersonate in zeppelin-site.xml.Deprecate Spark 2.2 and earlier versionsStarting from 0.9, Zeppelin deprecate Spark 2.2 and earlier versions. So you will see a warning message when you use Spark 2.2 and earlier.You can get rid of this message by setting zeppelin.spark.deprecatedMsg.show to false.CommunityJoin our community to discuss with others.",
      "url": " /interpreter/spark.html",
      "group": "interpreter",
      "excerpt": "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution engine."
    }
    ,



    "/interpreter/sparql.html": {
      "title": "SPARQL Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;SPARQL Interpreter for Apache ZeppelinOverviewSPARQL is an RDF query language able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.Apache Zeppelin for now only supports Apache Jena to query SPARQL-Endpoints.To query your endpoint configure it in the Interpreter-Settings and use the %sparql interpreter.Then write your query in the paragraph.If you want the prefixes to replace the URI&amp;#39;s, set the replaceURIs setting.Configuration      Name    Default Value    Description        sparql.engine    jena    The sparql engine to use for the queries        sparql.endpoint    http://dbpedia.org/sparql    Complete URL of the endpoint        sparql.replaceURIs    true    Replace the URIs in the result with the prefixes        sparql.removeDatatypes    true    Remove the datatypes from Literals so Zeppelin can use the values  ExampleAcknowledgementThis work was partially supported by the Bavarian State Ministry of Economic Affairs,Regional Development and Energy within the framework of the Bavarian Research andDevelopment Program &amp;quot;Information and Communication Technology&amp;quot;.",
      "url": " /interpreter/sparql.html",
      "group": "interpreter",
      "excerpt": "SPARQL is an RDF query language able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Apache Zeppelin uses Apache Jena"
    }
    ,



    "/interpreter/submarine.html": {
      "title": "Apache Hadoop Submarine Interpreter for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Submarine Interpreter for Apache ZeppelinHadoop Submarine  is the latest machine learning framework subproject in the Hadoop 3.1 release. It allows Hadoop to support Tensorflow, MXNet, Caffe, Spark, etc. A variety of deep learning frameworks provide a full-featured system framework for machine learning algorithm development, distributed model training, model management, and model publishing, combined with hadoop&amp;#39;s intrinsic data storage and data processing capabilities to enable data scientists to Good mining and the value of the data.A deep learning algorithm project requires data acquisition, data processing, data cleaning, interactive visual programming adjustment parameters, algorithm testing, algorithm publishing, algorithm job scheduling, offline model training, model online services and many other processes and processes. Zeppelin is a web-based notebook that supports interactive data analysis. You can use SQL, Scala, Python, etc. to make data-driven, interactive, collaborative documents.You can use the more than 20 interpreters in zeppelin (for example: spark, hive, Cassandra, Elasticsearch, Kylin, HBase, etc.) to collect data, clean data, feature extraction, etc. in the data in Hadoop before completing the machine learning model training. The data preprocessing process.By integrating submarine in zeppelin, we use zeppelin&amp;#39;s data discovery, data analysis and data visualization and collaboration capabilities to visualize the results of algorithm development and parameter adjustment during machine learning model training.ArchitectureAs shown in the figure above, how the Submarine develops and models the machine learning algorithms through Zeppelin is explained from the system architecture.After installing and deploying Hadoop 3.1+ and Zeppelin, submarine will create a fully separate Zeppelin Submarine interpreter Docker container for each user in YARN. This container contains the development and runtime environment for Tensorflow. Zeppelin Server connects to the Zeppelin Submarine interpreter Docker container in YARN. allows algorithmic engineers to perform algorithm development and data visualization in Tensorflow&amp;#39;s stand-alone environment in Zeppelin Notebook.After the algorithm is developed, the algorithm engineer can submit the algorithm directly to the YARN in offline transfer training in Zeppelin, real-time demonstration of model training with Submarine&amp;#39;s TensorBoard for each algorithm engineer.You can not only complete the model training of the algorithm, but you can also use the more than twenty interpreters in Zeppelin. Complete the data preprocessing of the model, For example, you can perform data extraction, filtering, and feature extraction through the Spark interpreter in Zeppelin in the Algorithm Note.In the future, you can also use Zeppelin&amp;#39;s upcoming Workflow workflow orchestration service. You can complete Spark, Hive data processing and Tensorflow model training in one Note. It is organized into a workflow through visualization, etc., and the scheduling of jobs is performed in the production environment.OverviewAs shown in the figure above, from the internal implementation, how Submarine combines Zeppelin&amp;#39;s machine learning algorithm development and model training.The algorithm engineer created a Tensorflow notebook (left image) in Zeppelin by using Submarine interpreter.It is important to note that you need to complete the development of the entire algorithm in a Note.You can use Spark for data preprocessing in some of the paragraphs in Note.Use Python for algorithm development and debugging of Tensorflow in other paragraphs of notebook, Submarine creates a Zeppelin Submarine Interpreter Docker Container for you in YARN, which contains the following features and services:Shell Command line tool：Allows you to view the system environment in the Zeppelin Submarine Interpreter Docker Container, Install the extension tools you need or the Python dependencies.Kerberos lib：Allows you to perform kerberos authentication and access to Hadoop clusters with Kerberos authentication enabled.Tensorflow environment：Allows you to develop tensorflow algorithm code.Python environment：Allows you to develop tensorflow code.Complete a complete algorithm development with a Note in Zeppelin. If this algorithm contains multiple modules, You can write different algorithm modules in multiple paragraphs in Note. The title of each paragraph is the name of the algorithm module. The content of the paragraph is the code content of this algorithm module.HDFS Client：Zeppelin Submarine Interpreter will automatically submit the algorithm code you wrote in Note to HDFS.Submarine interpreter Docker Image It is Submarine that provides you with an image file that supports Tensorflow (CPU and GPU versions).And installed the algorithm library commonly used by Python.You can also install other development dependencies you need on top of the base image provided by Submarine.When you complete the development of the algorithm module, You can do this by creating a new paragraph in Note and typing %submarine dashboard. Zeppelin will create a Submarine Dashboard. The machine learning algorithm written in this Note can be submitted to YARN as a JOB by selecting the JOB RUN command option in the Control Panel. Create a Tensorflow Model Training Docker Container, The container contains the following sections:Tensorflow environmentHDFS Client Will automatically download the algorithm file Mount from HDFS into the container for distributed model training. Mount the algorithm file to the Work Dir path of the container.Submarine Tensorflow Docker Image There is Submarine that provides you with an image file that supports Tensorflow (CPU and GPU versions). And installed the algorithm library commonly used by Python. You can also install other development dependencies you need on top of the base image provided by Submarine.      Name    Class    Description        %submarine    SubmarineInterpreter    Provides interpreter for Apache Submarine dashboard        %submarine.sh    SubmarineShellInterpreter    Provides interpreter for Apache Submarine shell        %submarine.python    PySubmarineInterpreter    Provides interpreter for Apache Submarine python  Submarine shellAfter creating a Note with Submarine Interpreter in Zeppelin, You can add a paragraph to Note if you need it. Using the %submarine.sh identifier, you can use the Shell command to perform various operations on the Submarine Interpreter Docker Container, such as:View the Pythone version in the ContainerView the system environment of the ContainerInstall the dependencies you need yourselfKerberos certification with kinitUse Hadoop in Container for HDFS operations, etc.Submarine pythonYou can add one or more paragraphs to Note. Write the algorithm module for Tensorflow in Python using the %submarine.python identifier.Submarine DashboardAfter writing the Tensorflow algorithm by using %submarine.python, You can add a paragraph to Note. Enter the %submarine dashboard and execute it. Zeppelin will create a Submarine Dashboard.With Submarine Dashboard you can do all the operational control of Submarine, for example:Usage：Display Submarine&amp;#39;s command description to help developers locate problems.Refresh：Zeppelin will erase all your input in the Dashboard.Tensorboard：You will be redirected to the Tensorboard WEB system created by Submarine for each user. With Tensorboard you can view the real-time status of the Tensorflow model training in real time.CommandJOB RUN：Selecting JOB RUN will display the parameter input interface for submitting JOB.      Name    Description        Checkpoint Path/td&gt;    Submarine sets up a separate Checkpoint path for each user&#39;s Note for Tensorflow training. Saved the training data for this Note history, Used to train the output of model data, Tensorboard uses the data in this path for model presentation. Users cannot modify it. For example: `hdfs://cluster1/...` , The environment variable name for Checkpoint Path is `%checkpoint_path%`, You can use `%checkpoint_path%` instead of the input value in Data Path in `PS Launch Cmd` and `Worker Launch Cmd`.        Input Path    The user specifies the data data directory of the Tensorflow algorithm. Only HDFS-enabled directories are supported. The environment variable name for Data Path is `%input_path%`, You can use `%input_path%` instead of the input value in Data Path in `PS Launch Cmd` and `Worker Launch Cmd`.        PS Launch Cmd    Tensorflow Parameter services launch command，例如：`python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0 ...`        Worker Launch Cmd    Tensorflow Worker services launch command，例如：`python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=1 ...`  JOB STOPYou can choose to execute the JOB STOP command. Stop a Tensorflow model training task that has been submitted and is runningTENSORBOARD STARTYou can choose to execute the TENSORBOARD START command to create your TENSORBOARD Docker Container.TENSORBOARD STOPYou can choose to execute the TENSORBOARD STOP command to stop and destroy your TENSORBOARD Docker Container.Run Command：Execute the action command of your choiceClean Chechkpoint：Checking this option will clear the data in this Note&amp;#39;s Checkpoint Path before each JOB RUN execution.ConfigurationZeppelin Submarine interpreter provides the following properties to customize the Submarine interpreter      Attribute name    Attribute value    Description        DOCKER_CONTAINER_TIME_ZONE    Etc/UTC    Set the time zone in the container                           |        DOCKER_HADOOP_HDFS_HOME    /hadoop-3.1-0    Hadoop path in the following 3 images（SUBMARINE_INTERPRETER_DOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image） |        DOCKER_JAVA_HOME    /opt/java    JAVA path in the following 3 images（SUBMARINE_INTERPRETER_DOCKER_IMAGE、tf.parameter.services.docker.image、tf.worker.services.docker.image） |        HADOOP_YARN_SUBMARINE_JAR        Path to the Submarine JAR package in the Hadoop-3.1+ release installed on the Zeppelin server |        INTERPRETER_LAUNCH_MODE    local/yarn    Run the Submarine interpreter instance in local or YARN local mainly for submarine interpreter development and debugging YARN mode for production environment |        SUBMARINE_HADOOP_CONF_DIR        Set the HADOOP-CONF path to support multiple Hadoop cluster environments        SUBMARINE_HADOOP_HOME        Hadoop-3.1+ above path installed on the Zeppelin server        SUBMARINE_HADOOP_KEYTAB        Keytab file path for a hadoop cluster with kerberos authentication turned on        SUBMARINE_HADOOP_PRINCIPAL        PRINCIPAL information for the keytab file of the hadoop cluster with kerberos authentication turned on        SUBMARINE_INTERPRETER_DOCKER_IMAGE        At INTERPRETER_LAUNCH_MODE=yarn, Submarine uses this image to create a Zeppelin Submarine interpreter container to create an algorithm development environment for the user. |        docker.container.network        YARN&#39;s Docker network name        machinelearing.distributed.enable        Whether to use the model training of the distributed mode JOB RUN submission        shell.command.timeout.millisecs    60000    Execute timeout settings for shell commands in the Submarine interpreter container        submarine.algorithm.hdfs.path        Save machine-based algorithms developed using Submarine interpreter to HDFS as files        submarine.yarn.queue    root.default    Submarine submits model training YARN queue name        tf.checkpoint.path        Tensorflow checkpoint path, Each user will create a user&#39;s checkpoint secondary path using the username under this path. Each algorithm submitted by the user will create a checkpoint three-level path using the note id (the user&#39;s Tensorboard uses the checkpoint data in this path for visual display)        tf.parameter.services.cpu        Number of CPU cores applied to Tensorflow parameter services when Submarine submits model distributed training        tf.parameter.services.docker.image        Submarine creates a mirror for Tensorflow parameter services when submitting model distributed training        tf.parameter.services.gpu        GPU cores applied to Tensorflow parameter services when Submarine submits model distributed training        tf.parameter.services.memory    2G    Memory resources requested by Tensorflow parameter services when Submarine submits model distributed training        tf.parameter.services.num        Number of Tensorflow parameter services used by Submarine to submit model distributed training        tf.tensorboard.enable    true    Create a separate Tensorboard for each user        tf.worker.services.cpu        Submarine submits model resources for Tensorflow worker services when submitting model training        tf.worker.services.docker.image        Submarine creates a mirror for Tensorflow worker services when submitting model distributed training        tf.worker.services.gpu        Submarine submits GPU resources for Tensorflow worker services when submitting model training        tf.worker.services.memory        Submarine submits model resources for Tensorflow worker services when submitting model training        tf.worker.services.num        Number of Tensorflow worker services used by Submarine to submit model distributed training        yarn.webapp.http.address    http://hadoop:8088    YARN web ui address        zeppelin.interpreter.rpc.portRange    29914    You need to export this port in the SUBMARINE_INTERPRETER_DOCKER_IMAGE configuration image. RPC communication for Zeppelin Server and Submarine interpreter containers        zeppelin.ipython.grpc.message_size    33554432    Message size setting for IPython grpc in Submarine interpreter container        zeppelin.ipython.launch.timeout    30000    IPython execution timeout setting in Submarine interpreter container        zeppelin.python    python    Execution path of python in Submarine interpreter container        zeppelin.python.maxResult    10000    The maximum number of python execution results returned from the Submarine interpreter container        zeppelin.python.useIPython    false    IPython is currently not supported and must be false        zeppelin.submarine.auth.type    simple/kerberos    Has Hadoop turned on kerberos authentication?  Docker imagesThe docker images file is stored in the zeppelin/scripts/docker/submarine directory.submarine interpreter cpu versionsubmarine interpreter gpu versiontensorflow 1.10 &amp;amp; hadoop 3.1.2 cpu versiontensorflow 1.10 &amp;amp; hadoop 3.1.2 gpu versionChange Log0.1.0 (Zeppelin 0.9.0) :Support distributed or standolone tensorflow model training.Support submarine interpreter running local.Support submarine interpreter running YARN.Support Docker on YARN-3.3.0, Plan compatible with lower versions of yarn.Bugs &amp;amp; ContactsSubmarine interpreter BUGIf you encounter a bug for this interpreter, please create a sub JIRA ticket on ZEPPELIN-3856.Submarine Running problemIf you encounter a problem for Submarine runtime, please create a ISSUE on hadoop-submarine-ecosystem.YARN Submarine BUGIf you encounter a bug for Yarn Submarine, please create a JIRA ticket on SUBMARINE.DependencyYARNSubmarine currently need to run on Hadoop 3.3+The hadoop version of the hadoop submarine team git repository is periodically submitted to the code repository of the hadoop.The version of the git repository for the hadoop submarine team will be faster than the hadoop version release cycle.You can use the hadoop version of the hadoop submarine team git repository.Submarine runtime environmentyou can use Submarine-installer https://github.com/hadoopsubmarine, Deploy Docker and network environments.MoreHadoop Submarine Project: https://hadoop.apache.org/submarineYoutube Submarine Channel: https://www.youtube.com/channel/UC4JBt8Y8VJ0BW0IM9YpdCyQ",
      "url": " /interpreter/submarine.html",
      "group": "interpreter",
      "excerpt": "Hadoop Submarine is the latest machine learning framework subproject in the Hadoop 3.1 release. It allows Hadoop to support Tensorflow, MXNet, Caffe, Spark, etc."
    }
    ,




    "/quickstart/docker.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Zeppelin interpreter on DockerZeppelin service runs on local server. zeppelin is able to run the interpreter in the docker container, Isolating the operating environment of the interpreter through the docker container. Zeppelin can be easily used without having to install python, spark, etc. on the local node.Key benefits areInterpreter environment isolatingNot need to install python, spark, etc. environment on the local nodeDocker does not need to pre-install zeppelin binary package, Automatically upload local zeppelin interpreter lib files to the containerAutomatically upload local configuration files (such as spark-conf, hadoop-conf-dir, keytab file, ...) to the container, so that the running environment in the container is exactly the same as the local.Zeppelin server runs locally, making it easier to manage and maintainPrerequisitesapache/zeppelin docker imageSpark &amp;gt;= 2.2.0 docker image (in case of using Spark Interpreter)Docker 1.6+ Install DockerUse docker&amp;#39;s host network, so there is no need to set up a network specificallyDocker ConfigurationBecause DockerInterpreterProcess communicates via docker&amp;#39;s tcp interface.By default, docker provides an interface as a sock file, so you need to modify the configuration file to open the tcp interface remotely.vi /etc/docker/daemon.json, Add tcp://0.0.0.0:2375 to the hosts configuration item.{    ...    &amp;quot;hosts&amp;quot;: [&amp;quot;tcp://0.0.0.0:2375&amp;quot;,&amp;quot;unix:///var/run/docker.sock&amp;quot;]}hosts property reference: https://docs.docker.com/engine/reference/commandline/dockerd/QuickstartModify these 2 configuration items in zeppelin-site.xml.&amp;lt;property&amp;gt;&amp;lt;name&amp;gt;zeppelin.run.mode&amp;lt;/name&amp;gt;&amp;lt;value&amp;gt;docker&amp;lt;/value&amp;gt;&amp;lt;description&amp;gt;&amp;#39;auto|local|k8s|docker&amp;#39;&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;&amp;lt;name&amp;gt;zeppelin.docker.container.image&amp;lt;/name&amp;gt;&amp;lt;value&amp;gt;apache/zeppelin&amp;lt;/value&amp;gt;&amp;lt;description&amp;gt;Docker image for interpreters&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;set timezone in zeppelin-env.shSet to the same time zone as the zeppelin server, keeping the time zone in the interpreter docker container the same as the server. E.g, &amp;quot;America/New_York&amp;quot; or &amp;quot;Asia/Shanghai&amp;quot;export DOCKER_TIME_ZONE=&amp;quot;America/New_York&amp;quot;Build Zeppelin image manuallyTo build Zeppelin image, support Kerberos certification &amp;amp; install spark binary.Use the /scripts/docker/interpreter/Dockerfile to build the image.FROM apache/zeppelin:0.8.0MAINTAINER Apache Software Foundation &amp;lt;dev@zeppelin.apache.org&amp;gt;ENV SPARK_VERSION=2.3.3ENV HADOOP_VERSION=2.7# support Kerberos certificationRUN export DEBIAN_FRONTEND=noninteractive &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -yq krb5-user libpam-krb5 &amp;amp;&amp;amp; apt-get cleanRUN apt-get update &amp;amp;&amp;amp; apt-get install -y curl unzip wget grep sed vim tzdata &amp;amp;&amp;amp; apt-get clean# auto upload zeppelin interpreter libRUN rm -rf /zeppelinRUN rm -rf /sparkRUN wget https://www-us.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgzRUN tar zxvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgzRUN mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} sparkRUN rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgzThen build docker image.# build image. Replace &amp;lt;tag&amp;gt;.$ docker build -t &amp;lt;tag&amp;gt; .How it worksZeppelin interpreter on DockerZeppelin service runs on local server, it auto configure itself to use DockerInterpreterLauncher.DockerInterpreterLauncher via DockerInterpreterProcess launcher creates each interpreter in a container using docker image.DockerInterpreterProcess uploads the binaries and configuration files of the local zeppelin service to the container:${ZEPPELIN_HOME}/bin${ZEPPELIN_HOME}/lib${ZEPPELIN_HOME}/interpreter/${interpreterGroupName}${ZEPPELIN_HOME}/conf/zeppelin-site.xml${ZEPPELIN_HOME}/conf/log4j.properties${ZEPPELIN_HOME}/conf/log4j_yarn_cluster.propertiesHADOOP_CONF_DIRSPARK_CONF_DIR/etc/krb5.confKeytab file configured in the interpreter propertieszeppelin.shell.keytab.locationspark.yarn.keytabsubmarine.hadoop.keytabzeppelin.jdbc.keytab.locationzeppelin.server.kerberos.keytabAll file paths uploaded to the container, Keep the same path as the local one. This will ensure that all configurations are used correctly.Spark interpreter on DockerWhen interpreter group is spark, Zeppelin sets necessary spark configuration automatically to use Spark on Docker.Supports all running modes of local[*], yarn-client, and yarn-cluster of zeppelin spark interpreter.SPARK_CONF_DIRConfiguring in the zeppelin-env.shBecause there are only spark binary files in the interpreter image, no spark conf files are included.The configuration file in the spark-&amp;lt;version&amp;gt;/conf/ local to the zeppelin service needs to be uploaded to the /spark/conf/ directory in the spark interpreter container.So you need to setting export SPARK_CONF_DIR=/spark-&amp;lt;version&amp;gt;-path/conf/ in the zeppelin-env.sh file.Configuring in the spark PropertiesYou can also configure it in the spark interpreter properties.properties nameValueDescriptionSPARK_CONF_DIR/spark--path.../conf/Spark--path/conf/ path local on the zeppelin serviceHADOOP_CONF_DIRConfiguring in the zeppelin-env.shBecause there are only spark binary files in the interpreter image, no configuration files are included.The configuration file in the hadoop-&amp;lt;version&amp;gt;/etc/hadoop local to the zeppelin service needs to be uploaded to the spark interpreter container.So you need to setting export HADOOP_CONF_DIR=hadoop-&amp;lt;version&amp;gt;-path/etc/hadoop in the zeppelin-env.sh file.Configuring in the spark PropertiesYou can also configure it in the spark interpreter properties.properties nameValueDescriptionHADOOP_CONF_DIRhadoop--path/etc/hadoophadoop--path/etc/hadoop path local on the zeppelin serviceAccessing Spark UI (or Service running in interpreter container)Because the zeppelin interpreter container uses the host network, the spark.ui.port port is automatically allocated, so do not configure spark.ui.port=xxxx in spark-defaults.confFuture workConfiguring container resources that can be used by different interpreters by configuration.DevelopmentInstead of build Zeppelin distribution package and docker image everytime during development,Zeppelin can run locally (such as inside your IDE in debug mode) and able to run Interpreter using DockerInterpreterLauncher by configuring following environment variables.zeppelin-site.xmlConfiguration variableValueDescriptionZEPPELIN_RUN_MODEdockerMake Zeppelin run interpreter on DockerZEPPELIN_DOCKER_CONTAINER_IMAGE&amp;lt;image&amp;gt;:&amp;lt;version&amp;gt;Zeppelin interpreter docker image to use",
      "url": " /quickstart/docker.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Zeppelin and running it in the command line."
    }
    ,



    "/quickstart/explore_ui.html": {
      "title": "Explore Apache Zeppelin UI",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Explore Apache Zeppelin UIMain homeThe first time you connect to Zeppelin (default installations start on http://localhost:8080), you&amp;#39;ll land at the main page similar to the below screen capture.On the left of the page are listed all existing notes. Those notes are stored by default in the $ZEPPELIN_HOME/notebook folder.You can filter them by name using the input text form. You can also create a new note, refresh the list of existing notes(in case you manually copy them into the $ZEPPELIN_HOME/notebook folder) and import a note.When clicking on Import Note link, a new dialog open. From there you can import your note from local disk or from a remote locationif you provide the URL.By default, the name of the imported note is the same as the original note but you can override it by providing a new name.MenusNotebookThe Notebook menu proposes almost the same features as the note management section in the home page. From the drop-down menu you can:Open a selected noteFilter node by nameCreate a new noteSettingsThis menu gives you access to settings and displays information about Zeppelin. User name is set to anonymous if you use default shiro configuration. If you want to set up authentification, see Shiro Authentication.About ZeppelinYou can check Zeppelin version in this menu.InterpreterIn this menu you can:Configure existing interpreter instanceAdd/remove interpreter instancesCredentialThis menu allows you to save credentials for data sources which are passed to interpreters.ConfigurationThis menu displays all the Zeppelin configuration that are set in the config file $ZEPPELIN_HOME/conf/zeppelin-site.xmlNote LayoutEach Zeppelin note is composed of 1 .. N paragraphs. The note can be viewed as a paragraph container.ParagraphEach paragraph consists of 2 sections: code section where you put your source code and result section where you can see the result of the code execution.On the top-right corner of each paragraph there are some commands to:execute the paragraph codehide/show code sectionhide/show result sectionconfigure the paragraphTo configure the paragraph, just click on the gear icon:From this dialog, you can (in descending order):find the paragraph id ( 20150924-163507_134879501 )control paragraph width. Since Zeppelin is using the grid system of Twitter Bootstrap, each paragraph width can be changed from 1 to 12move the paragraph 1 level upmove the paragraph 1 level downcreate a new paragraphchange paragraph titleshow/hide line number in the code sectiondisable the run button for this paragraphexport the current paragraph as an iframe and open the iframe in a new windowclear the result sectiondelete the current paragraphNote toolbarAt the top of the note, you can find a toolbar which exposes command buttons as well as configuration, security and display options.On the far right is displayed the note name, just click on it to reveal the input form and update it.In the middle of the toolbar you can find the command buttons:execute all the paragraphs sequentially, in their display orderhide/show code section of all paragraphshide/show result section of all paragraphsclear the result section of all paragraphsclone the current noteexport the current note to a JSON file. _Please note that the code section and result section of all paragraphs will be exported. If you have heavy data in the result section of some paragraphs, it is recommended to clean them before exportingcommit the current node contentdelete the noteschedule the execution of all paragraph using a CRON syntaxOn the right of the note tool bar you can find configuration icons:display all the keyboard shorcutsconfigure the interpreters binding to the current noteconfigure the note permissionsswitch the node display mode between default, simple and report",
      "url": " /quickstart/explore_ui.html",
      "group": "quickstart",
      "excerpt": "If you are new to Apache Zeppelin, this document will guide you about the basic components of Zeppelin one by one."
    }
    ,



    "/quickstart/flink_with_zeppelin.html": {
      "title": "Flink with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Flink support in ZeppelinFor a brief overview of Apache Flink fundamentals with Apache Zeppelin, see the following guide:built-in Apache Flink integration.With Flink Scala Scala PyFlink Shell, Flink SQLInject ExecutionEnvironment, StreamExecutionEnvironment, BatchTableEnvironment, StreamTableEnvironment.Canceling job and displaying its progress Supports different modes: local, remote, yarn, yarn-applicationDependency managementStreaming VisualizationFor the further information about Flink support in Zeppelin, please check Flink Interpreter",
      "url": " /quickstart/flink_with_zeppelin.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,



    "/quickstart/install.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;InstallWelcome to Apache Zeppelin! On this page are instructions to help you get started.RequirementsApache Zeppelin officially supports and is tested on the following environments:      Name    Value        OpenJDK or Oracle JDK    1.8 (151+)  (set JAVA_HOME)        OS    Mac OSX  Ubuntu 18.04  Ubuntu 20.04  Downloading Binary PackageTwo binary packages are available on the download page. Only difference between these two binaries is whether all the interpreters are included in the package file.all interpreter package: unpack it in a directory of your choice and you&amp;#39;re ready to go.net-install interpreter package: only spark, python, markdown and shell interpreter included. Unpack and follow install additional interpreters to install other interpreters. If you&amp;#39;re unsure, just run ./bin/install-interpreter.sh --all and install all interpreters.Building Zeppelin from sourceFollow the instructions How to Build, If you want to build from source instead of using binary package.Starting Apache ZeppelinStarting Apache Zeppelin from the Command LineOn all unix like platforms:bin/zeppelin-daemon.sh startAfter Zeppelin has started successfully, go to http://localhost:8080 with your web browser.By default Zeppelin is listening at 127.0.0.1:8080, so you can&amp;#39;t access it when it is deployed on another remote machine.To access a remote Zeppelin, you need to change zeppelin.server.addr to 0.0.0.0 in conf/zeppelin-site.xml.Check log file at ZEPPELIN_HOME/logs/zeppelin-server-*.log if you can not open Zeppelin.Stopping Zeppelinbin/zeppelin-daemon.sh stopUsing the official docker imageMake sure that docker is installed in your local machine.  Use this command to launch Apache Zeppelin in a container.docker run -p 8080:8080 --rm --name zeppelin apache/zeppelin:0.10.0To persist logs and notebook directories, use the volume option for docker container.docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook            -e ZEPPELIN_LOG_DIR=&amp;#39;/logs&amp;#39; -e ZEPPELIN_NOTEBOOK_DIR=&amp;#39;/notebook&amp;#39;            --name zeppelin apache/zeppelin:0.10.0-u $(id -u) is to make sure you have the permission to write logs and notebooks. For many interpreters, they require other dependencies, e.g. Spark interpreter requires Spark binary distributionand Flink interpreter requires Flink binary distribution. You can also mount them via docker volumn. e.g.docker run -u $(id -u) -p 8080:8080 --rm -v /mnt/disk1/notebook:/notebook -v /usr/lib/spark-current:/opt/spark -v /mnt/disk1/flink-1.12.2:/opt/flink -e FLINK_HOME=/opt/flink  -e SPARK_HOME=/opt/spark  -e ZEPPELIN_NOTEBOOK_DIR=&amp;#39;/notebook&amp;#39; --name zeppelin apache/zeppelin:0.10.0If you have trouble accessing localhost:8080 in the browser, Please clear browser cache.Start Apache Zeppelin with a service managerNote : The below description was written based on Ubuntu.Apache Zeppelin can be auto-started as a service with an init script, using a service manager like upstart.This is an example upstart script saved as /etc/init/zeppelin.confThis allows the service to be managed with commands such assudo service zeppelin start  sudo service zeppelin stop  sudo service zeppelin restartOther service managers could use a similar approach with the upstart argument passed to the zeppelin-daemon.sh script.bin/zeppelin-daemon.sh upstartzeppelin.confdescription &amp;quot;zeppelin&amp;quot;start on (local-filesystems and net-device-up IFACE!=lo)stop on shutdown# Respawn the process on unexpected terminationrespawn# respawn the job up to 7 times within a 5 second period.# If the job exceeds these values, it will be stopped and marked as failed.respawn limit 7 5# zeppelin was installed in /usr/share/zeppelin in this examplechdir /usr/share/zeppelinexec bin/zeppelin-daemon.sh upstartNext StepsCongratulations, you have successfully installed Apache Zeppelin! Here are a few steps you might find useful:New to Apache Zeppelin...For an in-depth overview, head to Explore Zeppelin UI.And then, try run Tutorial Notebooks shipped with your Zeppelin distribution.And see how to change configurations like port number, etc.Spark, Flink, SQL, Python, R and moreSpark support in Zeppelin, to know more about deep integration with Apache Spark. Flink support in Zeppelin, to know more about deep integration with Apache Flink.SQL support in Zeppelin for SQL supportPython support in Zeppelin, for Matplotlib, Pandas, Conda/Docker integration.R support in ZeppelinAll Available InterpretersMulti-user support ...Check Multi-user support",
      "url": " /quickstart/install.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Zeppelin and running it in the command line."
    }
    ,



    "/quickstart/kubernetes.html": {
      "title": "Install",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Zeppelin on KubernetesZeppelin can run on clusters managed by Kubernetes. When Zeppelin runs in Pod, it creates pods for individual interpreter. Also Spark interpreter auto configured to use Spark on Kubernetes in client mode.Key benefits areInterpreter scale-outSpark interpreter auto configure Spark on KubernetesAble to customize Kubernetes yaml fileSpark UI accessPrerequisitesZeppelin &amp;gt;= 0.9.0 docker imageSpark &amp;gt;= 2.4.0 docker image (in case of using Spark Interpreter)A running Kubernetes cluster with access configured to it using kubectl Kubernetes DNS configured in your clusterEnough cpu and memory in your Kubernetes cluster. We recommend 4CPUs, 6g of memory to be able to start Spark Interpreter with few executors.If you&amp;#39;re using minikube, check your cluster capacity (kubectl describe node) and increase if necessary$ minikube delete    # otherwise configuration won&amp;#39;t apply$ minikube config set cpus &amp;lt;number&amp;gt;$ minikube config set memory &amp;lt;number in MB&amp;gt;$ minikube start$ minikube config viewQuickstartGet zeppelin-server.yaml from github repository or find it from Zeppelin distribution package.# Get it from Zeppelin distribution package.$ ls &amp;lt;zeppelin-distribution&amp;gt;/k8s/zeppelin-server.yaml# or download it from github$ curl -s -O https://raw.githubusercontent.com/apache/zeppelin/master/k8s/zeppelin-server.yamlStart zeppelin on kubernetes cluster,kubectl apply -f zeppelin-server.yamlPort forward Zeppelin server port,kubectl port-forward zeppelin-server 8080:80and browse localhost:8080.Try run some paragraphs and see each interpreter is running as a Pod (using kubectl get pods), instead of a local process.To shutdown,kubectl delete -f zeppelin-server.yamlSpark InterpreterBuild spark docker image to use Spark Interpreter.Download spark binary distribution and run following command.Spark 2.4.0 or later version is required.# if you&amp;#39;re using minikube, set docker-env$ eval $(minikube docker-env)# build docker image$ &amp;lt;spark-distribution&amp;gt;/bin/docker-image-tool.sh -m -t 2.4.0 buildRun docker images and check if spark:2.4.0 is created.Configure sparkContainerImage of zeppelin-server-conf ConfigMap in zeppelin-server.yaml.Create note and configure executor number (default 1)%spark.confspark.executor.instances  5And then start your spark interpreter%sparksc.parallelize(1 to 100).count...While spark.master property of SparkInterpreter starts with k8s:// (default k8s://https://kubernetes.default.svc when Zeppelin started using zeppelin-server.yaml), Spark executors will be automatically created in your Kubernetes cluster.Spark UI is accessible by clicking SPARK JOB on the Paragraph. Check here to know more about Running Spark on Kubernetes.Build Zeppelin image manuallyTo build your own Zeppelin image, first build Zeppelin project with -Pbuild-distr flag.$ mvn package -DskipTests -Pbuild-distr &amp;lt;your flags&amp;gt;Binary package will be created under zeppelin-distribution/target directory. Move created package file under scripts/docker/zeppelin/bin/ directory.$ mv zeppelin-distribution/target/zeppelin-*.tar.gz scripts/docker/zeppelin/bin/scripts/docker/zeppelin/bin/Dockerfile downloads package from internet. Modify the file to add package from filesystem....# Find following section and comment out#RUN echo &amp;quot;$LOG_TAG Download Zeppelin binary&amp;quot; &amp;amp;&amp;amp; #    wget -O /tmp/zeppelin-${Z_VERSION}-bin-all.tgz http://archive.apache.org/dist/zeppelin/zeppelin-${Z_VERSION}/zeppelin-${Z_VERSION}-bin-all.tgz &amp;amp;&amp;amp; #    tar -zxvf /tmp/zeppelin-${Z_VERSION}-bin-all.tgz &amp;amp;&amp;amp; #    rm -rf /tmp/zeppelin-${Z_VERSION}-bin-all.tgz &amp;amp;&amp;amp; #    mv /zeppelin-${Z_VERSION}-bin-all ${ZEPPELIN_HOME}# Add following lines right after the commented line aboveADD zeppelin-${Z_VERSION}.tar.gz /RUN ln -s /zeppelin-${Z_VERSION} /zeppelin...Then build docker image.# configure docker env, if you&amp;#39;re using minikube$ eval $(minikube docker-env) # change directory$ cd scripts/docker/zeppelin/bin/# build image. Replace &amp;lt;tag&amp;gt;.$ docker build -t &amp;lt;tag&amp;gt; .Finally, set custom image &amp;lt;tag&amp;gt; just created to image and ZEPPELIN_K8S_CONTAINER_IMAGE env variable of zeppelin-server container spec in zeppelin-server.yaml file.Currently, single docker image is being used in both Zeppelin server and Interpreter pods. Therefore,PodNumber of instancesImageNoteZeppelin Server1Zeppelin docker imageUser creates/deletes with kubectl commandZeppelin InterpretersnZeppelin docker imageZeppelin Server creates/deletesSpark executorsmSpark docker imageSpark Interpreter creates/deletesCurrently, size of Zeppelin docker image is quite big. Zeppelin project is planning to provides lightweight images for each individual interpreter in the future.How it worksZeppelin on Kubernetesk8s/zeppelin-server.yaml is provided to run Zeppelin Server with few sidecars and configurations.Once Zeppelin Server is started in side Kubernetes, it auto configure itself to use K8sStandardInterpreterLauncher.The launcher creates each interpreter in a Pod using templates located under k8s/interpreter/ directory.Templates in the directory applied in alphabetical order. Templates are rendered by jinjavaand all interpreter properties are accessible inside the templates.Spark on KubernetesWhen interpreter group is spark, Zeppelin sets necessary spark configuration automatically to use Spark on Kubernetes.It uses client mode, so Spark interpreter Pod works as a Spark driver, spark executors are launched in separate Pods.This auto configuration can be overrided by manually setting spark.master property of Spark interpreter.Accessing Spark UI (or Service running in interpreter Pod)Zeppelin server Pod has a reverse proxy as a sidecar, and it splits traffic to Zeppelin server and Spark UI running in the other Pods.It assume both &amp;lt;your service domain&amp;gt; and *.&amp;lt;your service domain&amp;gt; point the nginx proxy address.&amp;lt;your service domain&amp;gt; is directed to ZeppelinServer, *.&amp;lt;your service domain&amp;gt; is directed to interpreter Pods.&amp;lt;port&amp;gt;-&amp;lt;interpreter pod svc name&amp;gt;.&amp;lt;your service domain&amp;gt; is convention to access any application running in interpreter Pod.For example, When your service domain name is local.zeppelin-project.org Spark interpreter Pod is running with a name spark-axefeg and Spark UI is running on port 4040,4040-spark-axefeg.local.zeppelin-project.orgis the address to access Spark UI.Default service domain is local.zeppelin-project.org:8080. local.zeppelin-project.org and *.local.zeppelin-project.org configured to resolve 127.0.0.1.It allows access Zeppelin and Spark UI with kubectl port-forward zeppelin-server 8080:80.If you like to use your custom domainConfigure Ingress in Kubernetes cluster for http port of the service zeppelin-server defined in k8s/zeppelin-server.yaml.Configure DNS record that your service domain and wildcard subdomain point the IP Addresses of your Ingress.Modify serviceDomain of zeppelin-server-conf ConfigMap in k8s/zeppelin-server.yaml file.Apply changes (e.g. kubectl apply -f k8s/zeppelin-server.yaml)Persist /notebook and /conf directoryNotebook and configurations are not persisted by default. Please configure volume and update k8s/zeppelin-server.yamlto use the volume to persiste /notebook and /conf directory if necessary.CustomizationZeppelin Server PodEdit k8s/zeppelin-server.yaml and apply.Interpreter PodSince Interpreter Pod is created/deleted by ZeppelinServer using templates under k8s/interpreter directory,to customize,Prepare k8s/interpreter directory with customization (edit or create new yaml file), in a Kubernetes volume.Modify k8s/zeppelin-server.yaml and mount prepared volume dir k8s/interpreter to /zeppelin/k8s/interpreter/.Apply modified k8s/zeppelin-server.yaml.Run a paragraph will create an interpreter using modified yaml files.The interpreter pod can also be customized through the interpreter settings. Here are some of the properties:Property NameDefault ValueDescriptionzeppelin.k8s.namespacedefaultThe Kubernetes namespace to use.zeppelin.k8s.interpreter.serviceAccountdefaultThe Kubernetes service account to use.zeppelin.k8s.interpreter.container.imageapache/zeppelin:&amp;lt;ZEPPELIN_VERSION&amp;gt;The interpreter image to use.zeppelin.k8s.interpreter.cores(optional)The number of cpu cores to use.zeppelin.k8s.interpreter.memory(optional)The memory to use, e.g., 1g.zeppelin.k8s.interpreter.gpu.type(optional)Set the type of gpu to request when the interpreter pod is required to schedule gpu resources, e.g., nvidia.com/gpu.zeppelin.k8s.interpreter.gpu.nums(optional)Tne number of gpu to use.zeppelin.k8s.interpreter.imagePullSecrets(optional)Set the comma-separated list of Kubernetes secrets while pulling images, e.g., mysecret1,mysecret2zeppelin.k8s.interpreter.container.imagePullPolicy(optional)Set the pull policy of the interpreter image, e.g., Alwayszeppelin.k8s.spark.container.imagePullPolicy(optional)Set the pull policy of the spark image, e.g., Alwayszeppelin.spark.uiWebUrl//-.The URL for user to access Spark UI. The default value is a jinjava template that contains three variables.zeppelin.k8s.spark.useIngress(optional)If true, the Ingress will be created when creating the spark interpreter. So users can access the Spark UI through Ingress.zeppelin.k8s.spark.ingress.host-.If zeppelin.k8s.spark.useIngress is true, it configures the host value of the Ingress. The default value is a jinjava template that contains three variables. Users can access the Spark UI through a customized zeppelin.k8s.spark.ingress.host.Future workSmaller interpreter docker image.Blocking communication between interpreter Pod.Spark Interpreter Pod has Role CRUD for any pod/service in the same namespace. Which should be restricted to only Spark executors Pod.Per note interpreter mode by default when Zeppelin is running on KubernetesDevelopmentInstead of build Zeppelin distribution package and docker image everytime during development,Zeppelin can run locally (such as inside your IDE in debug mode) and able to run Interpreter using K8sStandardInterpreterLauncher by configuring following environment variables.Environment variableValueDescriptionZEPPELIN_RUN_MODEk8sMake Zeppelin run interpreter on KubernetesZEPPELIN_K8S_PORTFORWARDtrueEnable port forwarding from local Zeppelin instance to Interpreters running on KubernetesZEPPELIN_K8S_CONTAINER_IMAGE&amp;lt;image&amp;gt;:&amp;lt;version&amp;gt;Zeppelin interpreter docker image to useZEPPELIN_K8S_SPARK_CONTAINER_IMAGE&amp;lt;image&amp;gt;:&amp;lt;version&amp;gt;Spark docker image to useZEPPELIN_K8S_NAMESPACE&amp;lt;k8s namespace&amp;gt;Kubernetes namespace  to useKUBERNETES_AUTH_TOKEN&amp;lt;token&amp;gt;Kubernetes auth token to create resources",
      "url": " /quickstart/kubernetes.html",
      "group": "quickstart",
      "excerpt": "This page will help you get started and will guide you through installing Apache Zeppelin and running it in the command line."
    }
    ,



    "/quickstart/python_with_zeppelin.html": {
      "title": "Python with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Python support in ZeppelinThe following guides explain how to use Apache Zeppelin that enables you to write in Python:supports vanilla python and ipythonsupports flexible python environments using conda, dockercan query using PandasSQLalso, provides PySparkrun python interpreter in yarn cluster with customized conda python environment.with matplotlib integrationcan create results including UI widgets using Dynamic FormFor the further information about Python support in Zeppelin, please check Python Interpreter",
      "url": " /quickstart/python_with_zeppelin.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,



    "/quickstart/r_with_zeppelin.html": {
      "title": "R with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;R support in ZeppelinThe following guides explain how to use Apache Zeppelin that enables you to write in R:Supports vanilla R and IRkernelVisualize R dataframe via ZeppelinContextRun R interpreter in yarn cluster with customized conda R environment.Make R Shiny AppFor the further information about R support in Zeppelin, please checkR Interpreter",
      "url": " /quickstart/r_with_zeppelin.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,



    "/quickstart/spark_with_zeppelin.html": {
      "title": "Spark with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Spark support in ZeppelinFor a brief overview of Apache Spark fundamentals with Apache Zeppelin, see the following guide:built-in Apache Spark integration.With Spark Scala SparkSQL, PySpark, SparkRInject SparkContext, SQLContext and SparkSession automaticallyCanceling job and displaying its progress Supports different modes: local, standalone, yarn(client &amp;amp; cluster), k8sDependency managementSupports different context per user / note Sharing variables among PySpark, SparkR and Spark through ZeppelinContextLivy InterpreterFor the further information about Spark support in Zeppelin, please check Spark Interpreter",
      "url": " /quickstart/spark_with_zeppelin.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,



    "/quickstart/sql_with_zeppelin.html": {
      "title": "SQL with Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;SQL support in ZeppelinThe following guides explain how to use Apache Zeppelin that enables you to write in SQL:provides JDBC Interpreter which allows you can connect any JDBC data sources seamlesslyPostgresMySQL MariaDBAWS Redshift Apache HivePresto/TrinoImpalaApache Phoenix Apache DrillApache Tajoand so on Spark Interpreter supports SparkSQLFlink Interpreter supports Flink SQLPython Interpreter supports pandasSQL can create query result including UI widgets using Dynamic Form%sqlselect age, count(1) value from bank where age &amp;lt; ${maxAge=30} group by age order by ageFor the further information about SQL support in Zeppelin, please check JDBC InterpreterSpark InterpreterFlink InterpreterPython InterpreterIgniteSQL Interpreter for Apache IgniteKylin Interpreter for Apache Kylin",
      "url": " /quickstart/sql_with_zeppelin.html",
      "group": "quickstart",
      "excerpt": ""
    }
    ,



    "/quickstart/tutorial.html": {
      "title": "Apache Zeppelin Tutorial",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Zeppelin TutorialThis tutorial walks you through some of the fundamental Zeppelin concepts. We will assume you have already installed Zeppelin. If not, please see here first.Current main backend processing engine of Zeppelin is Apache Spark. If you&amp;#39;re new to this system, you might want to start by getting an idea of how it processes data to get the most out of Zeppelin.Tutorial with Local FileData RefineBefore you start Zeppelin tutorial, you will need to download bank.zip. First, to transform csv format data into RDD of Bank objects, run following script. This will also remove header using filter function.val bankText = sc.textFile(&amp;quot;yourPath/bank/bank-full.csv&amp;quot;)case class Bank(age:Integer, job:String, marital : String, education : String, balance : Integer)// split each line, filter out header (starts with &amp;quot;age&amp;quot;), and map it into Bank case classval bank = bankText.map(s=&amp;gt;s.split(&amp;quot;;&amp;quot;)).filter(s=&amp;gt;s(0)!=&amp;quot;&amp;quot;age&amp;quot;&amp;quot;).map(    s=&amp;gt;Bank(s(0).toInt,             s(1).replaceAll(&amp;quot;&amp;quot;&amp;quot;, &amp;quot;&amp;quot;),            s(2).replaceAll(&amp;quot;&amp;quot;&amp;quot;, &amp;quot;&amp;quot;),            s(3).replaceAll(&amp;quot;&amp;quot;&amp;quot;, &amp;quot;&amp;quot;),            s(5).replaceAll(&amp;quot;&amp;quot;&amp;quot;, &amp;quot;&amp;quot;).toInt        ))// convert to DataFrame and create temporal tablebank.toDF().registerTempTable(&amp;quot;bank&amp;quot;)Data RetrievalSuppose we want to see age distribution from bank. To do this, run:%sql select age, count(1) from bank where age &amp;lt; 30 group by age order by ageYou can make input box for setting age condition by replacing 30 with ${maxAge=30}.%sql select age, count(1) from bank where age &amp;lt; ${maxAge=30} group by age order by ageNow we want to see age distribution with certain marital status and add combo box to select marital status. Run:%sql select age, count(1) from bank where marital=&amp;quot;${marital=single,single|divorced|married}&amp;quot; group by age order by ageTutorial with Streaming DataData RefineSince this tutorial is based on Twitter&amp;#39;s sample tweet stream, you must configure authentication with a Twitter account. To do this, take a look at Twitter Credential Setup. After you get API keys, you should fill out credential related values(apiKey, apiSecret, accessToken, accessTokenSecret) with your API keys on following script.This will create a RDD of Tweet objects and register these stream data as a table:import org.apache.spark.streaming._import org.apache.spark.streaming.twitter._import org.apache.spark.storage.StorageLevelimport scala.io.Sourceimport scala.collection.mutable.HashMapimport java.io.Fileimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport sys.process.stringSeqToProcess/** Configures the Oauth Credentials for accessing Twitter */def configureTwitterCredentials(apiKey: String, apiSecret: String, accessToken: String, accessTokenSecret: String) {  val configs = new HashMap[String, String] ++= Seq(    &amp;quot;apiKey&amp;quot; -&amp;gt; apiKey, &amp;quot;apiSecret&amp;quot; -&amp;gt; apiSecret, &amp;quot;accessToken&amp;quot; -&amp;gt; accessToken, &amp;quot;accessTokenSecret&amp;quot; -&amp;gt; accessTokenSecret)  println(&amp;quot;Configuring Twitter OAuth&amp;quot;)  configs.foreach{ case(key, value) =&amp;gt;    if (value.trim.isEmpty) {      throw new Exception(&amp;quot;Error setting authentication - value for &amp;quot; + key + &amp;quot; not set&amp;quot;)    }    val fullKey = &amp;quot;twitter4j.oauth.&amp;quot; + key.replace(&amp;quot;api&amp;quot;, &amp;quot;consumer&amp;quot;)    System.setProperty(fullKey, value.trim)    println(&amp;quot;tProperty &amp;quot; + fullKey + &amp;quot; set as [&amp;quot; + value.trim + &amp;quot;]&amp;quot;)  }  println()}// Configure Twitter credentialsval apiKey = &amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;val apiSecret = &amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;val accessToken = &amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;val accessTokenSecret = &amp;quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;quot;configureTwitterCredentials(apiKey, apiSecret, accessToken, accessTokenSecret)import org.apache.spark.streaming.twitter._val ssc = new StreamingContext(sc, Seconds(2))val tweets = TwitterUtils.createStream(ssc, None)val twt = tweets.window(Seconds(60))case class Tweet(createdAt:Long, text:String)twt.map(status=&amp;gt;  Tweet(status.getCreatedAt().getTime()/1000, status.getText())).foreachRDD(rdd=&amp;gt;  // Below line works only in spark 1.3.0.  // For spark 1.1.x and spark 1.2.x,  // use rdd.registerTempTable(&amp;quot;tweets&amp;quot;) instead.  rdd.toDF().registerAsTable(&amp;quot;tweets&amp;quot;))twt.printssc.start()Data RetrievalFor each following script, every time you click run button you will see different result since it is based on real-time data.Let&amp;#39;s begin by extracting maximum 10 tweets which contain the word girl.%sql select * from tweets where text like &amp;#39;%girl%&amp;#39; limit 10This time suppose we want to see how many tweets have been created per sec during last 60 sec. To do this, run:%sql select createdAt, count(1) from tweets group by createdAt order by createdAtYou can make user-defined function and use it in Spark SQL. Let&amp;#39;s try it by making function named sentiment. This function will return one of the three attitudes( positive, negative, neutral ) towards the parameter.def sentiment(s:String) : String = {    val positive = Array(&amp;quot;like&amp;quot;, &amp;quot;love&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;great&amp;quot;, &amp;quot;happy&amp;quot;, &amp;quot;cool&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;one&amp;quot;, &amp;quot;that&amp;quot;)    val negative = Array(&amp;quot;hate&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;stupid&amp;quot;, &amp;quot;is&amp;quot;)    var st = 0;    val words = s.split(&amp;quot; &amp;quot;)        positive.foreach(p =&amp;gt;        words.foreach(w =&amp;gt;            if(p==w) st = st+1        )    )    negative.foreach(p=&amp;gt;        words.foreach(w=&amp;gt;            if(p==w) st = st-1        )    )    if(st&amp;gt;0)        &amp;quot;positivie&amp;quot;    else if(st&amp;lt;0)        &amp;quot;negative&amp;quot;    else        &amp;quot;neutral&amp;quot;}// Below line works only in spark 1.3.0.// For spark 1.1.x and spark 1.2.x,// use sqlc.registerFunction(&amp;quot;sentiment&amp;quot;, sentiment _) instead.sqlc.udf.register(&amp;quot;sentiment&amp;quot;, sentiment _)To check how people think about girls using sentiment function we&amp;#39;ve made above, run this:%sql select sentiment(text), count(1) from tweets where text like &amp;#39;%girl%&amp;#39; group by sentiment(text)",
      "url": " /quickstart/tutorial.html",
      "group": "quickstart",
      "excerpt": "This tutorial page contains a short walk-through tutorial that uses Apache Spark backend. Please note that this tutorial is valid for Spark 1.3 and higher."
    }
    ,



    "/quickstart/yarn.html": {
      "title": "Zeppelin on Yarn",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;Zeppelin on YarnZeppelin on yarn means to run interpreter process in yarn container. The key benefit is the scalability, you won&amp;#39;t run out of memoryof the zeppelin server host if you run large amount of interpreter processes.PrerequisitesThe following is required for yarn interpreter mode.Hadoop client (both 2.x and 3.x are supported) is installed.$HADOOP_HOME/bin is put in PATH. Because internally zeppelin will run command hadoop classpath to get all the hadoop jars and put them in the classpath of Zeppelin.Set USE_HADOOP as true in zeppelin-env.sh.ConfigurationYarn interpreter mode needs to be set for each interpreter. You can set zeppelin.interpreter.launcher to be yarn to run it in yarn mode.Besides that, you can also specify other properties as following table.      Name    Default Value    Description        zeppelin.interpreter.yarn.resource.memory    1024    memory for interpreter process, unit: mb        zeppelin.interpreter.yarn.resource.memoryOverhead    384    Amount of non-heap memory to be allocated per interpreter process in yarn interpreter mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.        zeppelin.interpreter.yarn.resource.cores    1    cpu cores for interpreter process        zeppelin.interpreter.yarn.queue    default    yarn queue name  Differences with non-yarn interpreter mode (local mode)There&amp;#39;re several differences between yarn interpreter mode with non-yarn interpreter mode (local mode)New yarn app will be allocated for the interpreter process.Any local path setting won&amp;#39;t work in yarn interpreter process. E.g. if you run python interpreter in yarn interpreter mode, then you need to make sure the python executable of zeppelin.python exist in all the nodes of yarn cluster. Because the python interpreter may launch in any node.Don&amp;#39;t use it for spark interpreter. Instead use spark&amp;#39;s built-in yarn-client or yarn-cluster which is more suitable for spark interpreter.",
      "url": " /quickstart/yarn.html",
      "group": "usage/interpreter",
      "excerpt": "Apache Zeppelin supports to run interpreter process in yarn containers"
    }
    ,







    "/setup/basics/hadoop_integration.html": {
      "title": "How to integrate with hadoop",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Integrate with hadoopHadoop is an optional component of zeppelin unless you need the following features* Use hdfs to store notes. * Use hdfs to store interpreter configuration* Use hdfs to store recovery data* Launch interpreter in yarn mode## RequirementsZeppelin 0.9 doesn&#39;t ship with hadoop dependencies, you need to include hadoop jars by yourself via the following steps* Hadoop client (both 2.x and 3.x are supported) is installed.* `$HADOOP_HOME/bin` is put in `PATH`. Because internally zeppelin will run command `hadoop classpath` to get all the hadoop jars and put them in the classpath of Zeppelin.* Set `USE_HADOOP` as `true` in `zeppelin-env.sh`.",
      "url": " /setup/basics/hadoop_integration.html",
      "group": "setup/basics",
      "excerpt": "How to integrate with hadoop"
    }
    ,



    "/setup/basics/how_to_build.html": {
      "title": "How to Build Zeppelin from source",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## How to Build Zeppelin from Source#### 0. RequirementsIf you want to build from source, you must first install the following dependencies:      Name    Value        Git    (Any Version)        Maven    3.6.3 or higher        OpenJDK or Oracle JDK    1.8 (151+)(set JAVA_HOME)  If you haven&#39;t installed Git and Maven yet, check the [Build requirements](#build-requirements) section and follow the step by step instructions from there.#### 1. Clone the Apache Zeppelin repository```bashgit clone https://github.com/apache/zeppelin.git```#### 2. Build sourceYou can build Zeppelin with following maven command:```bashmvn clean package -DskipTests [Options]```Check [build-profiles](#build-profiles) section for further build options.If you are behind proxy, follow instructions in [Proxy setting](#proxy-setting-optional) section.If you&#39;re interested in contribution, please check [Contributing to Apache Zeppelin (Code)](../../development/contribution/how_to_contribute_code.html) and [Contributing to Apache Zeppelin (Website)](../../development/contribution/how_to_contribute_website.html).#### 3. DoneYou can directly start Zeppelin by running the following command after successful build:```bash./bin/zeppelin-daemon.sh start```### Build profiles#### Scala profileTo be noticed, this scala profile affect the modules (e.g. cassandra, scalding) that use scala except Spark interpreter (Spark interpreter use other profiles to control its scala version, see the doc below).Set scala version (default 2.10). Available profiles are```-Pscala-2.10-Pscala-2.11```#### Spark InterpreterTo be noticed, the spark profiles here only affect the embedded mode (no need to specify `SPARK_HOME`) of spark interpreter. Zeppelin doesn&#39;t require you to build with different spark to make different versions of spark work in Zeppelin.You can run different versions of Spark in Zeppelin as long as you specify `SPARK_HOME`. Actually Zeppelin supports all the versions of Spark from 1.6 to 3.0.To build with a specific Spark version or scala versions, define one or more of the following profiles and options:##### `-Pspark-[version]`Set spark major versionAvailable profiles are```-Pspark-3.2-Pspark-3.1-Pspark-3.0-Pspark-2.4-Pspark-2.3-Pspark-2.2-Pspark-2.1-Pspark-2.0-Pspark-1.6```minor version can be adjusted by `-Dspark.version=x.x.x`##### `-Pspark-scala-[version] (optional)`To be noticed, these profiles also only affect the embedded mode (no need to specify `SPARK_HOME`) of Spark interpreter. Actually Zeppelin supports all the versions of scala (2.11, 2.12) in Spark interpreter as long as you specify `SPARK_HOME`.Available profiles are```-Pspark-scala-2.10-Pspark-scala-2.11-Pspark-scala-2.12```If you want to use Spark 3.x in the embedded mode, then you have to specify both profile `spark-3.0` and `spark-scala-2.12`,because Spark 3.x doesn&#39;t support scala 2.10 and 2.11. #### Build hadoop with Zeppelin (`-Phadoop[version]`) To be noticed, hadoop profiles only affect Zeppelin server, it doesn&#39;t affect any interpreter. Zeppelin server use hadoop in some cases, such as using hdfs as notebook storage. You can check this [page](./hadoop_integration.html) for more details about how to configure hadoop in Zeppelin.Set hadoop major version (default hadoop2).Available profiles are```-Phadoop2-Phadoop3```minor version can be adjusted by `-Dhadoop.version=x.x.x`##### `-Pvendor-repo` (optional)enable 3rd party vendor repository (Cloudera, Hortonworks)#### -Pexamples (optional)Build examples under zeppelin-examples directory### Build command examplesHere are some examples with several options:```bash# build with spark-3.0, spark-scala-2.12mvn clean package -Pspark-3.0 -Pspark-scala-2.12 -DskipTests# build with spark-2.4, spark-scala-2.11mvn clean package -Pspark-2.4 -Pspark-scala-2.11 -DskipTests# build with spark-1.6, spark-scala-2.10mvn clean package -Pspark-1.6 -Pspark-scala-2.10 -DskipTests# build with CDHmvn clean package -Pspark-1.6 -Pspark-scala-2.10 -Dhadoop.version=2.6.0-cdh5.5.0 -Pvendor-repo -DskipTests```Ignite Interpreter```bashmvn clean package -Dignite.version=1.9.0 -DskipTests```Scalding Interpreter```bashmvn clean package -Pscalding -DskipTests```### Optional configurationsHere are additional configurations that could be optionally tuned using the trailing `-D` option for maven commandsSpark package```bashspark.archive # default spark-${spark.version}spark.src.download.url # default http://d3kbcqa49mib13.cloudfront.net/${spark.archive}.tgzspark.bin.download.url # default http://d3kbcqa49mib13.cloudfront.net/${spark.archive}-bin-without-hadoop.tgz```Py4J package```bashpython.py4j.version # default 0.9.2pypi.repo.url # default https://pypi.python.org/packagespython.py4j.repo.folder # default /64/5c/01e13b68e8caafece40d549f232c9b5677ad1016071a48d04cc3895acaa3```final URL location for Py4J package will be produced as following:`${pypi.repo.url}${python.py4j.repo.folder}py4j-${python.py4j.version}.zip`Frontend Maven Plugin configurations```plugin.frontend.nodeDownloadRoot # default https://nodejs.org/dist/plugin.frontend.npmDownloadRoot # default http://registry.npmjs.org/npm/-/plugin.frontend.yarnDownloadRoot # default https://github.com/yarnpkg/yarn/releases/download/```## Build requirements### Install requirementsIf you don&#39;t have requirements prepared, install it.(The installation method may vary according to your environment, example is for Ubuntu.)```bashsudo apt-get updatesudo apt-get install gitsudo apt-get install openjdk-8-jdksudo apt-get install npmsudo apt-get install libfontconfigsudo apt-get install r-base-devsudo apt-get install r-cran-evaluate```### Install maven```bashwget http://www.eu.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gzsudo tar -zxf apache-maven-3.6.3-bin.tar.gz -C /usr/local/sudo ln -s /usr/local/apache-maven-3.6.3/bin/mvn /usr/local/bin/mvn```_Notes:_ - Ensure node is installed by running `node --version`   - Ensure maven is running version 3.6.3 or higher with `mvn -version` - Configure maven to use more memory than usual by `export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxMetaspaceSize=512m&quot;`## Proxy setting (optional)If you&#39;re behind the proxy, you&#39;ll need to configure maven and npm to pass through it.First of all, configure maven in your `~/.m2/settings.xml`.```xml            proxy-http      true      http      localhost      3128      usr      pwd --&gt;      localhost|127.0.0.1              proxy-https      true      https      localhost      3128      usr      pwd --&gt;      localhost|127.0.0.1      ```Then, next commands will configure npm.```bashnpm config set proxy http://localhost:3128npm config set https-proxy http://localhost:3128npm config set registry &quot;http://registry.npmjs.org/&quot;npm config set strict-ssl false```Configure git as well```bashgit config --global http.proxy http://localhost:3128git config --global https.proxy http://localhost:3128git config --global url.&quot;http://&quot;.insteadOf git://```To clean up, set `active false` in Maven `settings.xml` and run these commands.```bashnpm config rm proxynpm config rm https-proxygit config --global --unset http.proxygit config --global --unset https.proxygit config --global --unset url.&quot;http://&quot;.insteadOf```_Notes:_ - If you are behind NTLM proxy you can use [Cntlm Authentication Proxy](http://cntlm.sourceforge.net/). - Replace `localhost:3128` with the standard pattern `http://user:pwd@host:port`.## PackageTo package the final distribution including the compressed archive, run:```shmvn clean package -Pbuild-distr```To build a distribution with specific profiles, run:```shmvn clean package -Pbuild-distr -Pspark-2.4```The profiles `-Pspark-2.4` can be adjusted if you wish to build to a specific spark versions.  The archive is generated under _`zeppelin-distribution/target`_ directory## Run end-to-end testsZeppelin comes with a set of end-to-end acceptance tests driving headless selenium browser```sh# assumes zeppelin-server running on localhost:8080 (use -Durl=.. to override)mvn verify# or take care of starting/stoping zeppelin-server from packaged zeppelin-distribuion/targetmvn verify -P using-packaged-distr```[![Analytics](https://ga-beacon.appspot.com/UA-45176241-4/apache/zeppelin/README.md?pixel)](https://github.com/igrigorik/ga-beacon)",
      "url": " /setup/basics/how_to_build.html",
      "group": "setup/basics",
      "excerpt": "How to build Zeppelin from source"
    }
    ,



    "/setup/basics/multi_user_support.html": {
      "title": "Multi-user Support",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Multi-user Support This page describes about multi-user support.- multiple users login / logout using [Shiro Authentication](../security/shiro_authentication.html)- managing [Notebook Permission](../security/notebook_authorization.html)- how to setup [impersonation for interpreters](../../usage/interpreter/user_impersonation.html)- different contexts per user / note using [Interpreter Binding Mode](../../usage/interpreter/interpreter_binding_mode.html)- a paragraph in a notebook can be [Personalized](../../usage/other_features/personalized_mode.html) - propagates changes in notebooks through websocket in real-time",
      "url": " /setup/basics/multi_user_support.html",
      "group": "setup/basics",
      "excerpt": ""
    }
    ,



    "/setup/basics/systemd.html": {
      "title": "Manage Zeppelin with systemd",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## Zeppelin and systemd### Unit file installation / deinstallationThis script accepts two parameters: `enable` and `disable` which, as you might have guessed, enable or disable the Zeppelin systemd unit file. Go ahead and type:```# ./bin/zeppelin-systemd-service.sh enable```This command activates the Zeppelin systemd unit file on your system.If you wish to roll back and remove this unit file from said system, simply type:```# ./bin/zeppelin-systemd-service.sh disable```### Manage Zeppelin using systemd commandsTo start Zeppelin using systemd;```# systemctl start zeppelin```To stop Zeppelin using systemd:```# systemctl stop zeppelin```To check the service health:```# systemctl status zeppelin&quot;```",
      "url": " /setup/basics/systemd.html",
      "group": "setup/basics",
      "excerpt": "Zeppelin and systemd"
    }
    ,



    "/setup/deployment/cdh.html": {
      "title": "Apache Zeppelin on CDH",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin on CDH### 1. Import Cloudera QuickStart Docker image&gt;[Cloudera](http://www.cloudera.com/) has officially provided CDH Docker Hub in their own container. Please check [this guide page](http://www.cloudera.com/documentation/enterprise/latest/topics/quickstart_docker_container.html#cloudera_docker_container) for more information.You can import the Docker image by pulling it from Cloudera Docker Hub.```bashdocker pull cloudera/quickstart:latest```### 2. Run docker```bashdocker run -it  -p 80:80  -p 4040:4040  -p 8020:8020  -p 8022:8022  -p 8030:8030  -p 8032:8032  -p 8033:8033  -p 8040:8040  -p 8042:8042  -p 8088:8088  -p 8480:8480  -p 8485:8485  -p 8888:8888  -p 9083:9083  -p 10020:10020  -p 10033:10033  -p 18088:18088  -p 19888:19888  -p 25000:25000  -p 25010:25010  -p 25020:25020  -p 50010:50010  -p 50020:50020  -p 50070:50070  -p 50075:50075  -h quickstart.cloudera --privileged=true  agitated_payne_backup /usr/bin/docker-quickstart;```### 3. Verify running CDHTo verify the application is running well, check the web UI for HDFS on `http://:50070/` and YARN on `http://:8088/cluster`.### 4. Configure Spark interpreter in ZeppelinSet following configurations to `conf/zeppelin-env.sh`.```bashexport HADOOP_CONF_DIR=[your_hadoop_conf_path]export SPARK_HOME=[your_spark_home_path]````HADOOP_CONF_DIR`(Hadoop configuration path) is defined in `/scripts/docker/spark-cluster-managers/cdh/hdfs_conf`.Don&#39;t forget to set Spark `spark.master` as `yarn-client` in Zeppelin **Interpreters** setting page like below.### 5. Run Zeppelin with Spark interpreterAfter running a single paragraph with Spark interpreter in Zeppelin,browse `http://:8088/cluster/apps` to check Zeppelin application is running well or not.",
      "url": " /setup/deployment/cdh.html",
      "group": "setup/deployment",
      "excerpt": "This document will guide you how you can build and configure the environment on CDH with Apache Zeppelin using docker scripts."
    }
    ,



    "/setup/deployment/docker.html": {
      "title": "Apache Zeppelin Releases Docker Images",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Docker Image for Apache Zeppelin Releases ## Overview This document contains instructions about making docker containers for Zeppelin. It mainly provides guidance into how to create, publish and run docker images for zeppelin releases.## Quick Start### Installing DockerYou need to [install docker](https://docs.docker.com/engine/installation/) on your machine.### Running docker image for Zeppelin distribution```bashdocker run -p 8080:8080 -e ZEPPELIN_IN_DOCKER=true --rm --name zeppelin apache/zeppelin-server:```Notice, please specify environment variable `ZEPPELIN_IN_DOCKER` when starting zeppelin in docker, otherwise you can not see the interpreter log.* Zeppelin will run at `http://localhost:8080`.If you want to specify `logs` and `notebook` dir, ```bashdocker run -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_LOG_DIR=&#39;/logs&#39; -e ZEPPELIN_NOTEBOOK_DIR=&#39;/notebook&#39; -e ZEPPELIN_IN_DOCKER=true --name zeppelin apache/zeppelin-server: # e.g &#39;0.9.0&#39;```### Building dockerfile locally```bashcd $ZEPPELIN_HOMEcd scripts/docker/zeppelin/bindocker build -t my-zeppelin:my-tag ./```### Build docker image for Zeppelin server &amp; interpretersStarting from 0.9, Zeppelin support to run in k8s or docker. So we add the capability tobuild docker images for Zeppelin server &amp; interpreter.Recommendation: Edit the Docker files yourself to adapt them to your needs and reduce the image size.At first your need to build a zeppelin-distribution docker image.```bashcd $ZEPPELIN_HOMEdocker build -t zeppelin-distribution .```Build docker image for zeppelin server.```bashcd $ZEPPELIN_HOME/scripts/docker/zeppelin-serverdocker build -t zeppelin-server .```Build base docker image for zeppelin interpreter.```bashcd $ZEPPELIN_HOME/scripts/docker/zeppelin-interpreterdocker build -t zeppelin-interpreter-base  .```",
      "url": " /setup/deployment/docker.html",
      "group": "setup/deployment",
      "excerpt": "This document contains instructions about making docker containers for Zeppelin. It mainly provides guidance into how to create, publish and run docker images for zeppelin releases."
    }
    ,



    "/setup/deployment/flink_and_spark_cluster.html": {
      "title": "Install Zeppelin with Flink and Spark in cluster mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}This document is outdated, it is not verified in the latest Zeppelin.# Install with Flink and Spark clusterThis tutorial is extremely entry-level. It assumes no prior knowledge of Linux, git, or other tools. If you carefully type what I tell you when I tell you, you should be able to get Zeppelin running.## Installing Zeppelin with Flink and Spark in cluster modeThis tutorial assumes the user has a machine (real or [virtual](https://www.virtualbox.org/wiki/Downloads) with a fresh, minimal installation of [Ubuntu 14.04.3 Server](http://www.ubuntu.com/download/server).**Note:** On the size requirements of the Virtual Machine, some users reported trouble when using the default virtual machine sizes, specifically that the hard drive needed to be at least 16GB- other users did not have this issue.There are many good tutorials on how to install Ubuntu Server on a virtual box, [here is one of them](http://ilearnstack.com/2013/04/13/setting-ubuntu-vm-in-virtualbox/)### Required ProgramsAssuming the minimal install, there are several programs that we will need to install before Zeppelin, Flink, and Spark.- git- openssh-server- OpenJDK 7- Maven 3.1+For git, openssh-server, and OpenJDK 7 we will be using the apt package manager.##### gitFrom the command prompt:```bashsudo apt-get install git```##### openssh-server```bashsudo apt-get install openssh-server```##### OpenJDK 7```bashsudo apt-get install openjdk-7-jdk openjdk-7-jre-lib```*A note for those using Ubuntu 16.04*: To install `openjdk-7` on Ubuntu 16.04, one must add a repository.  [Source](http://askubuntu.com/questions/761127/ubuntu-16-04-and-openjdk-7)```bashsudo add-apt-repository ppa:openjdk-r/ppasudo apt-get updatesudo apt-get install openjdk-7-jdk openjdk-7-jre-lib```##### Maven 3.1+Zeppelin requires maven version 3.x.  The version available in the repositories at the time of writing is 2.x, so maven must be installed manually.Purge any existing versions of maven.```bashsudo apt-get purge maven maven2```Download the maven 3.3.9 binary.```bashwget &quot;http://www.us.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz&quot;```Unarchive the binary and move to the `/usr/local` directory.```bashtar -zxvf apache-maven-3.3.9-bin.tar.gzsudo mv ./apache-maven-3.3.9 /usr/local```Create symbolic links in `/usr/bin`.```bashsudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvn```### Installing ZeppelinThis provides a quick overview of Zeppelin installation from source, however the reader is encouraged to review the [Zeppelin Installation Guide](../../quickstart/install.html)From the command prompt:Clone Zeppelin.```bashgit clone https://github.com/apache/zeppelin.git```Enter the Zeppelin root directory.```bashcd zeppelin```Package Zeppelin.```bashmvn clean package -DskipTests -Pspark-1.6 -Dflink.version=1.1.3 -Pscala-2.10````-DskipTests` skips build tests- you&#39;re not developing (yet), so you don&#39;t need to do tests, the clone version *should* build.`-Pspark-1.6` tells maven to build a Zeppelin with Spark 1.6.  This is important because Zeppelin has its own Spark interpreter and the versions must be the same.`-Dflink.version=1.1.3` tells maven specifically to build Zeppelin with Flink version 1.1.3.-`-Pscala-2.10` tells maven to build with Scala v2.10.**Note:** You can build against any version of Spark that has a Zeppelin build profile available. The key is to make sure you check out the matching version of Spark to build. At the time of this writing, Spark 1.6 was the most recent Spark version available.**Note:** On build failures. Having installed Zeppelin close to 30 times now, I will tell you that sometimes the build fails for seemingly no reason.As long as you didn&#39;t edit any code, it is unlikely the build is failing because of something you did. What does tend to happen, is some dependency that maven is trying to download is unreachable.  If your build fails on this step here are some tips:- Don&#39;t get discouraged.- Scroll up and read through the logs. There will be clues there.- Retry (that is, run the `mvn clean package -DskipTests -Pspark-1.6` again)- If there were clues that a dependency couldn&#39;t be downloaded wait a few hours or even days and retry again. Open source software when compiling is trying to download all of the dependencies it needs, if a server is off-line there is nothing you can do but wait for it to come back.- Make sure you followed all of the steps carefully.- Ask the community to help you. Go [here](http://zeppelin.apache.org/community.html) and join the user mailing list. People are there to help you. Make sure to copy and paste the build output (everything that happened in the console) and include that in your message.Start the Zeppelin daemon.```bashbin/zeppelin-daemon.sh start```Use `ifconfig` to determine the host machine&#39;s IP address. If you are not familiar with how to do this, a fairly comprehensive post can be found [here](http://www.cyberciti.biz/faq/how-to-find-out-the-ip-address-assigned-to-eth0-and-display-ip-only/).Open a web-browser on a machine connected to the same network as the host (or in the host operating system if using a virtual machine).  Navigate to http://`yourip`:8080, where yourip is the IP address you found in `ifconfig`.See the [Zeppelin tutorial](../../quickstart/tutorial.html) for basic Zeppelin usage. It is also advised that you take a moment to check out the tutorial notebook that is included with each Zeppelin install, and to familiarize yourself with basic notebook functionality.##### Flink TestCreate a new notebook named &quot;Flink Test&quot; and copy and paste the following code.```scala%flink  // let Zeppelin know what interpreter to use.val text = benv.fromElements(&quot;In the time of chimpanzees, I was a monkey&quot;,   // some lines of text to analyze&quot;Butane in my veins and I&#39;m out to cut the junkie&quot;,&quot;With the plastic eyeballs, spray paint the vegetables&quot;,&quot;Dog food stalls with the beefcake pantyhose&quot;,&quot;Kill the headlights and put it in neutral&quot;,&quot;Stock car flamin&#39; with a loser in the cruise control&quot;,&quot;Baby&#39;s in Reno with the Vitamin D&quot;,&quot;Got a couple of couches, sleep on the love seat&quot;,&quot;Someone came in sayin&#39; I&#39;m insane to complain&quot;,&quot;About a shotgun wedding and a stain on my shirt&quot;,&quot;Don&#39;t believe everything that you breathe&quot;,&quot;You get a parking violation and a maggot on your sleeve&quot;,&quot;So shave your face with some mace in the dark&quot;,&quot;Savin&#39; all your food stamps and burnin&#39; down the trailer park&quot;,&quot;Yo, cut it&quot;)/*  The meat and potatoes:        this tells Flink to iterate through the elements, in this case strings,        transform the string to lower case and split the string at white space into individual words        then finally aggregate the occurrence of each word.        This creates the count variable which is a list of tuples of the form (word, occurances)counts.collect().foreach(println(_))  // execute the script and print each element in the counts list*/val counts = text.flatMap{ _.toLowerCase.split(&quot;W+&quot;) }.map { (_,1) }.groupBy(0).sum(1)counts.collect().foreach(println(_))  // execute the script and print each element in the counts list```Run the code to make sure the built-in Zeppelin Flink interpreter is working properly.##### Spark TestCreate a new notebook named &quot;Spark Test&quot; and copy and paste the following code.```scala%spark // let Zeppelin know what interpreter to use.val text = sc.parallelize(List(&quot;In the time of chimpanzees, I was a monkey&quot;,  // some lines of text to analyze&quot;Butane in my veins and I&#39;m out to cut the junkie&quot;,&quot;With the plastic eyeballs, spray paint the vegetables&quot;,&quot;Dog food stalls with the beefcake pantyhose&quot;,&quot;Kill the headlights and put it in neutral&quot;,&quot;Stock car flamin&#39; with a loser in the cruise control&quot;,&quot;Baby&#39;s in Reno with the Vitamin D&quot;,&quot;Got a couple of couches, sleep on the love seat&quot;,&quot;Someone came in sayin&#39; I&#39;m insane to complain&quot;,&quot;About a shotgun wedding and a stain on my shirt&quot;,&quot;Don&#39;t believe everything that you breathe&quot;,&quot;You get a parking violation and a maggot on your sleeve&quot;,&quot;So shave your face with some mace in the dark&quot;,&quot;Savin&#39; all your food stamps and burnin&#39; down the trailer park&quot;,&quot;Yo, cut it&quot;))/*  The meat and potatoes:        this tells spark to iterate through the elements, in this case strings,        transform the string to lower case and split the string at white space into individual words        then finally aggregate the occurrence of each word.        This creates the count variable which is a list of tuples of the form (word, occurances)*/val counts = text.flatMap { _.toLowerCase.split(&quot;W+&quot;) }                 .map { (_,1) }                 .reduceByKey(_ + _)counts.collect().foreach(println(_))  // execute the script and print each element in the counts list```Run the code to make sure the built-in Zeppelin Flink interpreter is working properly.Finally, stop the Zeppelin daemon.  From the command prompt run:```bashbin/zeppelin-daemon.sh stop```### Installing Clusters##### Flink Cluster###### Download BinariesBuilding from source is recommended  where possible, for simplicity in this tutorial we will download Flink and Spark Binaries.To download the Flink Binary use `wget````bashwget &quot;http://mirror.cogentco.com/pub/apache/flink/flink-1.1.3/flink-1.1.3-bin-hadoop24-scala_2.10.tgz&quot;tar -xzvf flink-1.1.3-bin-hadoop24-scala_2.10.tgz```This will download Flink 1.1.3, compatible with Hadoop 2.4.  You do not have to install Hadoop for this binary to work, but if you are using Hadoop, please change `24` to your appropriate version.Start the Flink Cluster.```bashflink-1.1.3/bin/start-cluster.sh```###### Building From sourceIf you wish to build Flink from source, the following will be instructive.  Note that if you have downloaded and used the binary version this should be skipped.  The changing nature of build tools and versions across platforms makes this section somewhat precarious.  For example, Java8 and Maven 3.0.3 are recommended for building Flink, which are not recommended for Zeppelin at the time of writing.  If the user wishes to attempt to build from source, this section will provide some reference.  If errors are encountered, please contact the Apache Flink community.See the [Flink Installation guide](https://github.com/apache/flink/blob/master/README.md) for more detailed instructions.Return to the directory where you have been downloading, this tutorial assumes that is `$HOME`. Clone Flink,  check out release-1.1.3-rc2, and build.```bashcd $HOMEgit clone https://github.com/apache/flink.gitcd flinkgit checkout release-1.1.3-rc2mvn clean install -DskipTests```Start the Flink Cluster in stand-alone mode```bashbuild-target/bin/start-cluster.sh```###### Ensure the cluster is upIn a browser, navigate to http://`yourip`:8082 to see the Flink Web-UI.  Click on &#39;Task Managers&#39; in the left navigation bar. Ensure there is at least one Task Manager present.![alt text]({{BASE_PATH}}/assets/themes/zeppelin/img/screenshots/flink-webui.png &quot;The Flink Web-UI&quot;)If no task managers are present, restart the Flink cluster with the following commands:(if binaries)```bashflink-1.1.3/bin/stop-cluster.shflink-1.1.3/bin/start-cluster.sh```(if built from source)```bashbuild-target/bin/stop-cluster.shbuild-target/bin/start-cluster.sh```##### Spark 1.6 Cluster###### Download BinariesBuilding from source is recommended  where possible, for simplicity in this tutorial we will download Flink and Spark Binaries.Using binaries is alsoTo download the Spark Binary use `wget````bashwget &quot;http://d3kbcqa49mib13.cloudfront.net/spark-1.6.3-bin-hadoop2.6.tgz&quot;tar -xzvf spark-1.6.3-bin-hadoop2.6.tgzmv spark-1.6.3-bin-hadoop2.6 spark```This will download Spark 1.6.3, compatible with Hadoop 2.6.  You do not have to install Hadoop for this binary to work, but if you are using Hadoop, please change `2.6` to your appropriate version.###### Building From sourceSpark is an extraordinarily large project, which takes considerable time to download and build. It is also prone to build failures for similar reasons listed in the Flink section.  If the user wishes to attempt to build from source, this section will provide some reference.  If errors are encountered, please contact the Apache Spark community.See the [Spark Installation](https://github.com/apache/spark/blob/master/README.md) guide for more detailed instructions.Return to the directory where you have been downloading, this tutorial assumes that is $HOME. Clone Spark, check out branch-1.6, and build.**Note:** Recall, we&#39;re only checking out 1.6 because it is the most recent Spark for which a Zeppelin profile exists at  the time of writing. You are free to check out other version, just make sure you build Zeppelin against the correct version of Spark. However if you use Spark 2.0, the word count example will need to be changed as Spark 2.0 is not compatible with the following examples.```bashcd $HOME```Clone, check out, and build Spark version 1.6.x.```bashgit clone https://github.com/apache/spark.gitcd sparkgit checkout branch-1.6mvn clean package -DskipTests```###### Start the Spark clusterReturn to the `$HOME` directory.```bashcd $HOME```Start the Spark cluster in stand alone mode, specifying the webui-port as some port other than 8080 (the webui-port of Zeppelin).```bashspark/sbin/start-master.sh --webui-port 8082```**Note:** Why `--webui-port 8082`? There is a digression toward the end of this document that explains this.Open a browser and navigate to http://`yourip`:8082 to ensure the Spark master is running.![alt text]({{BASE_PATH}}/assets/themes/zeppelin/img/screenshots/spark-master-webui1.png &quot;It should look like this...&quot;)Toward the top of the page there will be a *URL*: spark://`yourhost`:7077.  Note this URL, the Spark Master URI, it will be needed in subsequent steps.Start the slave using the URI from the Spark master WebUI:```bashspark/sbin/start-slave.sh spark://yourhostname:7077```Return to the root directory and start the Zeppelin daemon.```bashcd $HOMEzeppelin/bin/zeppelin-daemon.sh start```##### Configure InterpretersOpen a web browser and go to the Zeppelin web-ui at http://yourip:8080.Now go back to the Zeppelin web-ui at http://`yourip`:8080 and this time click on *anonymous* at the top right, which will open a drop-down menu, select *Interpreters* to enter interpreter configuration.In the Spark section, click the edit button in the top right corner to make the property values editable (looks like a pencil).The only field that needs to be edited in the Spark interpreter is the `spark.master` field. Change this value from `local[*]` to the URL you used to start the slave, mine was `spark://ubuntu:7077`.Click *Save* to update the parameters, and click *OK* when it asks you about restarting the interpreter.Now scroll down to the Flink section. Click the edit button and change the value of *host* from `local` to `localhost`. Click *Save* again.Reopen the examples and execute them again (I.e. you need to click the play button at the top of the screen, or the button on the paragraph .You should be able check the Flink and Spark webuis (at something like http://`yourip`:8081, http://`yourip`:8082, http://`yourip`:8083) and see that jobs have been run against the clusters.**Digression** Sorry to be vague and use terms such as &#39;something like&#39;, but exactly what web-ui is at what port is going to depend on what order you started things. What is really going on here is you are pointing your browser at specific ports, namely 8081, 8082, and 8083.  Flink and Spark all want to put their web-ui on port 8080, but are well behaved and will take the next port available. Since Zeppelin started first, it will get port 8080.  When Flink starts (assuming you started Flink first), it will try to bind to port 8080, see that it is already taken, and go to the next one available, hopefully 8081.  Spark has a webui for the master and the slave, so when they start they will try to bind to 8080   already taken by Zeppelin), then 8081 (already taken by Flink&#39;s webui), then 8082. If everything goes smoothy and you followed the directions precisely, the webuis should be 8081 and 8082.     It *is* possible to specify the port you want the webui to bind to (at the command line by passing the `--webui-port ` flag when you start the Flink and Spark, where `` is the port     you want to see that webui on.  You can also set the default webui port of Spark and Flink (and Zeppelin) in the configuration files, but this is a tutorial for novices and slightly out of scope.### Next StepsCheck out the [tutorial](../../quickstart/tutorial.html) for more cool things you can do with your new toy![Join the community](http://zeppelin.apache.org/community.html), ask questions and contribute! Every little bit helps.",
      "url": " /setup/deployment/flink_and_spark_cluster.html",
      "group": "setup/deployment",
      "excerpt": ""
    }
    ,



    "/setup/deployment/spark_cluster_mode.html": {
      "title": "Apache Zeppelin on Spark cluster mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin on Spark Cluster Mode## Overview[Apache Spark](http://spark.apache.org/) has supported three cluster manager types([Standalone](http://spark.apache.org/docs/latest/spark-standalone.html), [Apache Mesos](http://spark.apache.org/docs/latest/running-on-mesos.html) and [Hadoop YARN](http://spark.apache.org/docs/latest/running-on-yarn.html)) so far.This document will guide you how you can build and configure the environment on 3 types of Spark cluster manager with Apache Zeppelin using [Docker](https://www.docker.com/) scripts.So [install docker](https://docs.docker.com/engine/installation/) on the machine first.## Spark standalone mode[Spark standalone](http://spark.apache.org/docs/latest/spark-standalone.html) is a simple cluster manager included with Spark that makes it easy to set up a cluster.You can simply set up Spark standalone environment with below steps.&gt; **Note :** Since Apache Zeppelin and Spark use same `8080` port for their web UI, you might need to change `zeppelin.server.port` in `conf/zeppelin-site.xml`.### 1. Build Docker fileYou can find docker script files under `scripts/docker/spark-cluster-managers`.```bashcd $ZEPPELIN_HOME/scripts/docker/spark-cluster-managers/spark_standalonedocker build -t &quot;spark_standalone&quot; .```### 2. Run docker```bashdocker run -it -p 8080:8080 -p 7077:7077 -p 8888:8888 -p 8081:8081 -h sparkmaster --name spark_standalone spark_standalone bash;```Note that `sparkmaster` hostname used here to run docker container should be defined in your `/etc/hosts`.### 3. Configure Spark interpreter in ZeppelinSet Spark master as `spark://:7077` in Zeppelin **Interpreters** setting page.### 4. Run Zeppelin with Spark interpreterAfter running single paragraph with Spark interpreter in Zeppelin, browse `https://:8080` and check whether Spark cluster is running well or not.You can also simply verify that Spark is running well in Docker with below command.```bashps -ef | grep spark```## Spark on YARN modeYou can simply set up [Spark on YARN](http://spark.apache.org/docs/latest/running-on-yarn.html) docker environment with below steps.&gt; **Note :** Since Apache Zeppelin and Spark use same `8080` port for their web UI, you might need to change `zeppelin.server.port` in `conf/zeppelin-site.xml`.### 1. Build Docker fileYou can find docker script files under `scripts/docker/spark-cluster-managers`.```bashcd $ZEPPELIN_HOME/scripts/docker/spark-cluster-managers/spark_yarn_clusterdocker build -t &quot;spark_yarn&quot; .```### 2. Run docker```bashdocker run -it  -p 5000:5000  -p 9000:9000  -p 9001:9001  -p 8088:8088  -p 8042:8042  -p 8030:8030  -p 8031:8031  -p 8032:8032  -p 8033:8033  -p 8080:8080  -p 7077:7077  -p 8888:8888  -p 8081:8081  -p 50010:50010  -p 50075:50075  -p 50020:50020  -p 50070:50070  --name spark_yarn  -h sparkmaster  spark_yarn bash;```Note that `sparkmaster` hostname used here to run docker container should be defined in your `/etc/hosts`.### 3. Verify running Spark on YARN.You can simply verify the processes of Spark and YARN are running well in Docker with below command.```bashps -ef```You can also check each application web UI for HDFS on `http://:50070/`, YARN on `http://:8088/cluster` and Spark on `http://:8080/`.### 4. Configure Spark interpreter in ZeppelinSet following configurations to `conf/zeppelin-env.sh`.```bashexport HADOOP_CONF_DIR=[your_hadoop_conf_path]export SPARK_HOME=[your_spark_home_path]````HADOOP_CONF_DIR`(Hadoop configuration path) is defined in `/scripts/docker/spark-cluster-managers/spark_yarn_cluster/hdfs_conf`.Don&#39;t forget to set Spark `spark.master` as `yarn-client` in Zeppelin **Interpreters** setting page like below.### 5. Run Zeppelin with Spark interpreterAfter running a single paragraph with Spark interpreter in Zeppelin, browse `http://:8088/cluster/apps` and check Zeppelin application is running well or not.## Spark on Mesos modeYou can simply set up [Spark on Mesos](http://spark.apache.org/docs/latest/running-on-mesos.html) docker environment with below steps.### 1. Build Docker file```bashcd $ZEPPELIN_HOME/scripts/docker/spark-cluster-managers/spark_mesosdocker build -t &quot;spark_mesos&quot; .```### 2. Run docker```bashdocker run --net=host -it -p 8080:8080 -p 7077:7077 -p 8888:8888 -p 8081:8081 -p 8082:8082 -p 5050:5050 -p 5051:5051 -p 4040:4040 -h sparkmaster --name spark_mesos spark_mesos bash;```Note that `sparkmaster` hostname used here to run docker container should be defined in your `/etc/hosts`.### 3. Verify running Spark on Mesos.You can simply verify the processes of Spark and Mesos are running well in Docker with below command.```bashps -ef```You can also check each application web UI for Mesos on `http://:5050/cluster` and Spark on `http://:8080/`.### 4. Configure Spark interpreter in Zeppelin```bashexport MESOS_NATIVE_JAVA_LIBRARY=[PATH OF libmesos.so]export SPARK_HOME=[PATH OF SPARK HOME]```Don&#39;t forget to set Spark `spark.master` as `mesos://127.0.1.1:5050` in Zeppelin **Interpreters** setting page like below.### 5. Run Zeppelin with Spark interpreterAfter running a single paragraph with Spark interpreter in Zeppelin, browse `http://:5050/#/frameworks` and check Zeppelin application is running well or not.### Troubleshooting for Spark on Mesos- If you have problem with hostname, use `--add-host` option when executing `dockerrun````## use `--add-host=moby:127.0.0.1` option to resolve## since docker container couldn&#39;t resolve `moby`: java.net.UnknownHostException: moby: moby: Name or service not known        at java.net.InetAddress.getLocalHost(InetAddress.java:1496)        at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:789)        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:782)        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:782)```- If you have problem with mesos master, try `mesos://127.0.0.1` instead of `mesos://127.0.1.1````I0103 20:17:22.329269   340 sched.cpp:330] New master detected at master@127.0.1.1:5050I0103 20:17:22.330749   340 sched.cpp:341] No credentials provided. Attempting to register without authenticationW0103 20:17:22.333531   340 sched.cpp:736] Ignoring framework registered message because it was sentfrom &#39;master@127.0.0.1:5050&#39; instead of the leading master &#39;master@127.0.1.1:5050&#39;W0103 20:17:24.040252   339 sched.cpp:736] Ignoring framework registered message because it was sentfrom &#39;master@127.0.0.1:5050&#39; instead of the leading master &#39;master@127.0.1.1:5050&#39;W0103 20:17:26.150250   339 sched.cpp:736] Ignoring framework registered message because it was sentfrom &#39;master@127.0.0.1:5050&#39; instead of the leading master &#39;master@127.0.1.1:5050&#39;W0103 20:17:26.737604   339 sched.cpp:736] Ignoring framework registered message because it was sentfrom &#39;master@127.0.0.1:5050&#39; instead of the leading master &#39;master@127.0.1.1:5050&#39;W0103 20:17:35.241714   336 sched.cpp:736] Ignoring framework registered message because it was sentfrom &#39;master@127.0.0.1:5050&#39; instead of the leading master &#39;master@127.0.1.1:5050&#39;```",
      "url": " /setup/deployment/spark_cluster_mode.html",
      "group": "setup/deployment",
      "excerpt": "This document will guide you how you can build and configure the environment on 3 types of Spark cluster manager(Standalone, Hadoop Yarn, Apache Mesos) with Apache Zeppelin using docker scripts."
    }
    ,



    "/setup/deployment/virtual_machine.html": {
      "title": "Apache Zeppelin on Vagrant Virtual Machine",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin on Vagrant Virtual Machine## OverviewApache Zeppelin distribution includes a script directory `scripts/vagrant/zeppelin-dev`This script creates a virtual machine that launches a repeatable, known set of core dependencies required for developing Zeppelin. It can also be used to run an existing Zeppelin build if you don&#39;t plan to build from source.For PySpark users, this script includes several helpful [Python Libraries](#python-extras).For SparkR users, this script includes several helpful [R Libraries](#r-extras).### PrerequisitesThis script requires three applications, [Ansible](http://docs.ansible.com/ansible/intro_installation.html#latest-releases-via-pip &quot;Ansible&quot;), [Vagrant](http://www.vagrantup.com &quot;Vagrant&quot;) and [Virtual Box](https://www.virtualbox.org/ &quot;Virtual Box&quot;).  All of these applications are freely available as Open Source projects and extremely easy to set up on most operating systems.## Create a Zeppelin Ready VMIf you are running Windows and don&#39;t yet have python installed, [install Python 2.7.x](https://www.python.org/downloads/release/python-2710/) first.1. Download and Install Vagrant:  [Vagrant Downloads](http://www.vagrantup.com/downloads.html)2. Install Ansible:  [Ansible Python pip install](http://docs.ansible.com/ansible/intro_installation.html#latest-releases-via-pip)    ```bash    sudo easy_install pip    sudo pip install ansible    ansible --version    ```    After then, please check whether it reports **ansible version 1.9.2 or higher**.3. Install Virtual Box: [Virtual Box Downloads](https://www.virtualbox.org/ &quot;Virtual Box&quot;)4. Type `vagrant up`  from within the `/scripts/vagrant/zeppelin-dev` directoryThats it ! You can now run `vagrant ssh` and this will place you into the guest machines terminal prompt.If you don&#39;t wish to build Zeppelin from scratch, run the z-manager installer script while running in the guest VM:```bashcurl -fsSL https://raw.githubusercontent.com/NFLabs/z-manager/master/zeppelin-installer.sh | bash```## Building ZeppelinYou can now ```bashgit clone git://git.apache.org/zeppelin.git```into a directory on your host machine, or directly in your virtual machine.Cloning Zeppelin into the `/scripts/vagrant/zeppelin-dev` directory from the host, will allow the directory to be shared between your host and the guest machine.Cloning the project again may seem counter intuitive, since this script likely originated from the project repository.  Consider copying just the vagrant/zeppelin-dev script from the Zeppelin project as a stand alone directory, then once again clone the specific branch you wish to build.Synced folders enable Vagrant to sync a folder on the host machine to the guest machine, allowing you to continue working on your project&#39;s files on your host machine, but use the resources in the guest machine to compile or run your project. _[(1) Synced Folder Description from Vagrant Up](https://docs.vagrantup.com/v2/synced-folders/index.html)_By default, Vagrant will share your project directory (the directory with the Vagrantfile) to `/vagrant`.  Which means you should be able to build within the guest machine after you`cd /vagrant/zeppelin`## What&#39;s in this VM?Running the following commands in the guest machine should display these expected versions:* `node --version` should report *v0.12.7** `mvn --version` should report *Apache Maven 3.3.9* and *Java version: 1.7.0_85*The virtual machine consists of: - Ubuntu Server 14.04 LTS - Node.js 0.12.7 - npm 2.11.3 - ruby 1.9.3 + rake, make and bundler (only required if building jekyll documentation) - Maven 3.3.9 - Git - Unzip - libfontconfig to avoid phatomJs missing dependency issues - openjdk-7-jdk - Python addons: pip, matplotlib, scipy, numpy, pandas - [R](https://www.r-project.org/) and R Packages required to run the R Interpreter and the related R tutorial notebook, including:  Knitr, devtools, repr, rCharts, ggplot2, googleVis, mplot, htmltools, base64enc, data.table## How to build &amp; run ZeppelinThis assumes you&#39;ve already cloned the project either on the host machine in the zeppelin-dev directory (to be shared with the guest machine) or cloned directly into a directory while running inside the guest machine.  The following build steps will also include Python and R support via PySpark and SparkR:```bashcd /zeppelinmvn clean package -Pspark-1.6 -Phadoop-2.4 -DskipTests./bin/zeppelin-daemon.sh start```On your host machine browse to `http://localhost:8080/`If you [turned off port forwarding](#tweaking-the-virtual-machine) in the `Vagrantfile` browse to `http://192.168.51.52:8080`## Tweaking the Virtual MachineIf you plan to run this virtual machine along side other Vagrant images, you may wish to bind the virtual machine to a specific IP address, and not use port fowarding from your local host.Comment out the `forward_port` line, and uncomment the `private_network` line in Vagrantfile.  The subnet that works best for your local network will vary so adjust `192.168.*.*` accordingly.```#config.vm.network &quot;forwarded_port&quot;, guest: 8080, host: 8080config.vm.network &quot;private_network&quot;, ip: &quot;192.168.51.52&quot;````vagrant halt` followed by `vagrant up` will restart the guest machine bound to the IP address of `192.168.51.52`.This approach usually is typically required if running other virtual machines that discover each other directly by IP address, such as Spark Masters and Slaves as well as Cassandra Nodes, Elasticsearch Nodes, and other Spark data sources.  You may wish to launch nodes in virtual machines with IP addresses in a subnet that works for your local network, such as: 192.168.51.53, 192.168.51.54, 192.168.51.53, etc..## Extras### Python ExtrasWith Zeppelin running, **Numpy**, **SciPy**, **Pandas** and **Matplotlib** will be available.  Create a pyspark notebook, and try the below code.```python%pysparkimport numpyimport scipyimport pandasimport matplotlibprint &quot;numpy &quot; + numpy.__version__print &quot;scipy &quot; + scipy.__version__print &quot;pandas &quot; + pandas.__version__print &quot;matplotlib &quot; + matplotlib.__version__```To Test plotting using Matplotlib into a rendered `%html` SVG image, try```python%pysparkimport matplotlibmatplotlib.use(&#39;Agg&#39;)   # turn off interactive charting so this works for server side SVG renderingimport matplotlib.pyplot as pltimport numpy as npimport StringIO# clear out any previous plots on this noteplt.clf()def show(p):    img = StringIO.StringIO()    p.savefig(img, format=&#39;svg&#39;)    img.seek(0)    print &quot;%html &quot; + img.buf + &quot;&quot;# Example datapeople = (&#39;Tom&#39;, &#39;Dick&#39;, &#39;Harry&#39;, &#39;Slim&#39;, &#39;Jim&#39;)y_pos = np.arange(len(people))performance = 3 + 10 * np.random.rand(len(people))error = np.random.rand(len(people))plt.barh(y_pos, performance, xerr=error, align=&#39;center&#39;, alpha=0.4)plt.yticks(y_pos, people)plt.xlabel(&#39;Performance&#39;)plt.title(&#39;How fast do you want to go today?&#39;)show(plt)```### R ExtrasWith zeppelin running, an R Tutorial notebook will be available.  The R packages required to run the examples and graphs in this tutorial notebook were installed by this virtual machine.The installed R Packages include: `knitr`, `devtools`, `repr`, `rCharts`, `ggplot2`, `googleVis`, `mplot`, `htmltools`, `base64enc`, `data.table`.",
      "url": " /setup/deployment/virtual_machine.html",
      "group": "setup/deployment",
      "excerpt": "Apache Zeppelin provides a script for running a virtual machine for development through Vagrant. The script will create a virtual machine with core dependencies pre-installed, required for developing Apache Zeppelin."
    }
    ,



    "/setup/deployment/yarn_install.html": {
      "title": "Install Zeppelin to connect with existing YARN cluster",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## IntroductionThis page describes how to pre-configure a bare metal node, configure Zeppelin and connect it to existing YARN cluster running Hortonworks flavour of Hadoop. It also describes steps to configure Spark interpreter of Zeppelin.## Prepare Node### Zeppelin user (Optional)This step is optional, however its nice to run Zeppelin under its own user. In case you do not like to use Zeppelin (hope not) the user could be deleted along with all the packages that were installed for Zeppelin, Zeppelin binary itself and associated directories.Create a zeppelin user and switch to zeppelin user or if zeppelin user is already created then login as zeppelin.```bashuseradd zeppelinsu - zeppelinwhoami```Assuming a zeppelin user is created then running whoami command must return```bashzeppelin```Its assumed in the rest of the document that zeppelin user is indeed created and below installation instructions are performed as zeppelin user.### List of Prerequisites * CentOS 6.x, Mac OSX, Ubuntu 14.X * Java 1.8 * Hadoop client * Spark * Internet connection is required.It&#39;s assumed that the node has CentOS 6.x installed on it. Although any version of Linux distribution should work fine.#### Hadoop clientZeppelin can work with multiple versions &amp; distributions of Hadoop. A complete list is available [here](https://github.com/apache/zeppelin#build). This document assumes Hadoop 2.7.x client libraries including configuration files are installed on Zeppelin node. It also assumes /etc/hadoop/conf contains various Hadoop configuration files. The location of Hadoop configuration files may vary, hence use appropriate location.```bashhadoop versionHadoop 2.7.1.2.3.1.0-2574Subversion git@github.com:hortonworks/hadoop.git -r f66cf95e2e9367a74b0ec88b2df33458b6cff2d0Compiled by jenkins on 2015-07-25T22:36ZCompiled with protoc 2.5.0From source with checksum 54f9bbb4492f92975e84e390599b881dThis command was run using /usr/hdp/2.3.1.0-2574/hadoop/lib/hadoop-common-2.7.1.2.3.1.0-2574.jar```#### SparkSpark is supported out of the box and to take advantage of this, you need to Download appropriate version of Spark binary packages from [Spark Download page](http://spark.apache.org/downloads.html) and unzip it.Zeppelin can work with multiple versions of Spark. A complete list is available [here](https://github.com/apache/zeppelin#build).This document assumes Spark 1.6.0 is installed at /usr/lib/spark.&gt; Note: Spark should be installed on the same node as Zeppelin.&gt; Note: Spark&#39;s pre-built package for CDH 4 doesn&#39;t support yarn.#### ZeppelinCheckout source code from [git://git.apache.org/zeppelin.git](https://github.com/apache/zeppelin.git) or download binary package from [Download page](https://zeppelin.apache.org/download.html).You can refer [Install](install.html) page for the details.This document assumes that Zeppelin is located under `/home/zeppelin/zeppelin`.## Zeppelin ConfigurationZeppelin configuration needs to be modified to connect to YARN cluster. Create a copy of zeppelin environment shell script.```bashcp /home/zeppelin/zeppelin/conf/zeppelin-env.sh.template /home/zeppelin/zeppelin/conf/zeppelin-env.sh```Set the following properties```bashexport JAVA_HOME=&quot;/usr/java/jdk1.8.0_171&quot;export HADOOP_CONF_DIR=&quot;/etc/hadoop/conf&quot;export ZEPPELIN_JAVA_OPTS=&quot;-Dhdp.version=2.3.1.0-2574&quot;export SPARK_HOME=&quot;/usr/lib/spark&quot;```As /etc/hadoop/conf contains various configurations of YARN cluster, Zeppelin can now submit Spark/Hive jobs on YARN cluster form its web interface. The value of hdp.version is set to 2.3.1.0-2574. This can be obtained by running the following command```bashhdp-select status hadoop-client | sed &#39;s/hadoop-client - (.*)/1/&#39;# It returned  2.3.1.0-2574```## Start/Stop### Start Zeppelin```bashcd /home/zeppelin/zeppelinbin/zeppelin-daemon.sh start```After successful start, visit http://[zeppelin-server-host-name]:8080 with your web browser.### Stop Zeppelin```bashbin/zeppelin-daemon.sh stop```## InterpreterZeppelin provides various distributed processing frameworks to process data that ranges from Spark, JDBC, Ignite and Lens to name a few. This document describes to configure JDBC &amp; Spark interpreters.### HiveZeppelin supports Hive through JDBC interpreter. You might need the information to use Hive and can find in your hive-site.xmlOnce Zeppelin server has started successfully, visit http://[zeppelin-server-host-name]:8080 with your web browser. Click on Interpreter tab next to Notebook dropdown. Look for Hive configurations and set them appropriately. Set them as per Hive installation on YARN cluster.Click on Save button. Once these configurations are updated, Zeppelin will prompt you to restart the interpreter. Accept the prompt and the interpreter will reload the configurations.### SparkIt was assumed that 1.6.0 version of Spark is installed at /usr/lib/spark. Look for Spark configurations and click edit button to add the following properties      Property Name    Property Value    Remarks        spark.master    yarn-client    In yarn-client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.        spark.driver.extraJavaOptions    -Dhdp.version=2.3.1.0-2574            spark.yarn.am.extraJavaOptions    -Dhdp.version=2.3.1.0-2574      Click on Save button. Once these configurations are updated, Zeppelin will prompt you to restart the interpreter. Accept the prompt and the interpreter will reload the configurations.Spark &amp; Hive notebooks can be written with Zeppelin now. The resulting Spark &amp; Hive jobs will run on configured YARN cluster.## DebugZeppelin does not emit any kind of error messages on web interface when notebook/paragraph is run. If a paragraph fails it only displays ERROR. The reason for failure needs to be looked into log files which is present in logs directory under zeppelin installation base directory. Zeppelin creates a log file for each kind of interpreter.```bash[zeppelin@zeppelin-3529 logs]$ pwd/home/zeppelin/zeppelin/logs[zeppelin@zeppelin-3529 logs]$ ls -ltotal 844-rw-rw-r-- 1 zeppelin zeppelin  14648 Aug  3 14:45 zeppelin-interpreter-hive-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin 625050 Aug  3 16:05 zeppelin-interpreter-spark-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin 200394 Aug  3 21:15 zeppelin-zeppelin-zeppelin-3529.log-rw-rw-r-- 1 zeppelin zeppelin  16162 Aug  3 14:03 zeppelin-zeppelin-zeppelin-3529.out[zeppelin@zeppelin-3529 logs]$```",
      "url": " /setup/deployment/yarn_install.html",
      "group": "setup/deployment",
      "excerpt": "This page describes how to pre-configure a bare metal node, configure Apache Zeppelin and connect it to existing YARN cluster running Hortonworks flavour of Hadoop."
    }
    ,



    "/setup/operation/configuration.html": {
      "title": "Apache Zeppelin Configuration",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Configuration## Zeppelin PropertiesZeppelin can be configured via several sources.Sources descending by priority: - environment variables can be defined `conf/zeppelin-env.sh`(`confzeppelin-env.cmd` for Windows). - system properties - configuration file can be defined in `conf/zeppelin-site.xml`&gt; Mouse hover on each property and click  then you can get a link for that.      zeppelin-env.sh    zeppelin-site.xml    Default value    Description        ZEPPELIN_ADDR    zeppelin.server.addr    127.0.0.1    Zeppelin server binding address        ZEPPELIN_PORT    zeppelin.server.port    8080    Zeppelin server port        Note: Please make sure you&#39;re not using the same port with      Zeppelin web application development port (default: 9000).        ZEPPELIN_SSL_PORT    zeppelin.server.ssl.port    8443    Zeppelin Server ssl port (used when ssl environment/property is set to true)        ZEPPELIN_JMX_ENABLE    zeppelin.jmx.enable    false    Enable JMX by defining &quot;true&quot;        ZEPPELIN_JMX_PORT    zeppelin.jmx.port    9996    Port number which JMX uses        ZEPPELIN_MEM    N/A    -Xmx1024m -XX:MaxMetaspaceSize=512m    JVM mem options        ZEPPELIN_INTP_MEM    N/A    ZEPPELIN_MEM    JVM mem options for interpreter process        ZEPPELIN_JAVA_OPTS    N/A        JVM options        ZEPPELIN_ALLOWED_ORIGINS    zeppelin.server.allowed.origins    *    Enables a way to specify a &#39;,&#39; separated list of allowed origins for REST and websockets.  e.g. http://localhost:8080        ZEPPELIN_CREDENTIALS_PERSIST    zeppelin.credentials.persist    true    Persist credentials on a JSON file (credentials.json)          ZEPPELIN_CREDENTIALS_ENCRYPT_KEY    zeppelin.credentials.encryptKey        If provided, encrypt passwords on the credentials.json file (passwords will be stored as plain-text otherwise          ZEPPELIN_SERVER_CONTEXT_PATH    zeppelin.server.context.path    /    Context path of the web application        ZEPPELIN_NOTEBOOK_COLLABORATIVE_MODE_ENABLE    zeppelin.notebook.collaborative.mode.enable    true    Enable basic opportunity for collaborative editing. Does not change the logic of operation if the note is used by one person.        ZEPPELIN_SSL    zeppelin.ssl    false            ZEPPELIN_SSL_CLIENT_AUTH    zeppelin.ssl.client.auth    false            ZEPPELIN_SSL_KEYSTORE_PATH    zeppelin.ssl.keystore.path    keystore            ZEPPELIN_SSL_KEYSTORE_TYPE    zeppelin.ssl.keystore.type    JKS            ZEPPELIN_SSL_KEYSTORE_PASSWORD    zeppelin.ssl.keystore.password                ZEPPELIN_SSL_KEY_MANAGER_PASSWORD    zeppelin.ssl.key.manager.password                ZEPPELIN_SSL_TRUSTSTORE_PATH    zeppelin.ssl.truststore.path                ZEPPELIN_SSL_TRUSTSTORE_TYPE    zeppelin.ssl.truststore.type                ZEPPELIN_SSL_TRUSTSTORE_PASSWORD    zeppelin.ssl.truststore.password                ZEPPELIN_SSL_PEM_KEY    zeppelin.ssl.pem.key        This directive points to the PEM-encoded private key file for the server.        ZEPPELIN_SSL_PEM_KEY_PASSWORD    zeppelin.ssl.pem.key.password        Password of the PEM-encoded private key.        ZEPPELIN_SSL_PEM_CERT    zeppelin.ssl.pem.cert        This directive points to a file with certificate data in PEM format.        ZEPPELIN_SSL_PEM_CA    zeppelin.ssl.pem.ca        This directive sets the all-in-one file where you can assemble the Certificates of Certification Authorities (CA) whose clients you deal with. These are used for Client Authentication. Such a file is simply the concatenation of the various PEM-encoded Certificate files.        ZEPPELIN_NOTEBOOK_HOMESCREEN    zeppelin.notebook.homescreen        Display note IDs on the Apache Zeppelin homescreen e.g. 2A94M5J1Z        ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE    zeppelin.notebook.homescreen.hide    false    Hide the note ID set by ZEPPELIN_NOTEBOOK_HOMESCREEN on the Apache Zeppelin homescreen. For the further information, please read Customize your Zeppelin homepage.        ZEPPELIN_WAR_TEMPDIR    zeppelin.war.tempdir    webapps    Location of the jetty temporary directory        ZEPPELIN_NOTEBOOK_DIR    zeppelin.notebook.dir    notebook    The root directory where notebook directories are saved        ZEPPELIN_NOTEBOOK_S3_BUCKET    zeppelin.notebook.s3.bucket    zeppelin    S3 Bucket where notebook files will be saved        ZEPPELIN_NOTEBOOK_S3_USER    zeppelin.notebook.s3.user    user    User name of an S3 buckete.g. bucket/user/notebook/2A94M5J1Z/note.json        ZEPPELIN_NOTEBOOK_S3_ENDPOINT    zeppelin.notebook.s3.endpoint    s3.amazonaws.com    Endpoint for the bucket        N/A    zeppelin.notebook.s3.timeout    120000    Bucket endpoint request timeout in msec        ZEPPELIN_NOTEBOOK_S3_KMS_KEY_ID    zeppelin.notebook.s3.kmsKeyID        AWS KMS Key ID to use for encrypting data in S3 (optional)        ZEPPELIN_NOTEBOOK_S3_EMP    zeppelin.notebook.s3.encryptionMaterialsProvider        Class name of a custom S3 encryption materials provider implementation to use for encrypting data in S3 (optional)        ZEPPELIN_NOTEBOOK_S3_SSE    zeppelin.notebook.s3.sse    false    Save notebooks to S3 with server-side encryption enabled          ZEPPELIN_NOTEBOOK_S3_CANNED_ACL      zeppelin.notebook.s3.cannedAcl            Save notebooks to S3 with the given [Canned ACL](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/CannedAccessControlList.html) which determines the S3 permissions.          ZEPPELIN_NOTEBOOK_S3_PATH_STYLE_ACCESS      zeppelin.notebook.s3.pathStyleAccess      false      Access S3 bucket using path style        ZEPPELIN_NOTEBOOK_S3_SIGNEROVERRIDE    zeppelin.notebook.s3.signerOverride        Optional override to control which signature algorithm should be used to sign AWS requests        ZEPPELIN_NOTEBOOK_AZURE_CONNECTION_STRING    zeppelin.notebook.azure.connectionString        The Azure storage account connection stringe.g. DefaultEndpointsProtocol=https;AccountName=&amp;lt;accountName&amp;gt;;AccountKey=&amp;lt;accountKey&amp;gt;        ZEPPELIN_NOTEBOOK_AZURE_SHARE    zeppelin.notebook.azure.share    zeppelin    Azure Share where the notebook files will be saved        ZEPPELIN_NOTEBOOK_AZURE_USER    zeppelin.notebook.azure.user    user    Optional user name of an Azure file sharee.g. share/user/notebook/2A94M5J1Z/note.json        ZEPPELIN_NOTEBOOK_STORAGE    zeppelin.notebook.storage    org.apache.zeppelin.notebook.repo.GitNotebookRepo    Comma separated list of notebook storage locations        ZEPPELIN_NOTEBOOK_ONE_WAY_SYNC    zeppelin.notebook.one.way.sync    false    If there are multiple notebook storage locations, should we treat the first one as the only source of truth?        ZEPPELIN_NOTEBOOK_PUBLIC    zeppelin.notebook.public    true    Make notebook public (set only owners) by default when created/imported. If set to false will add user to readers and writers as well, making it private and invisible to other users unless permissions are granted.        ZEPPELIN_INTERPRETER_DIR    zeppelin.interpreter.dir    interpreter    Interpreter directory        ZEPPELIN_INTERPRETER_DEP_MVNREPO    zeppelin.interpreter.dep.mvnRepo    https://repo1.maven.org/maven2/    Remote principal repository for interpreter&#39;s additional dependency loading        ZEPPELIN_INTERPRETER_OUTPUT_LIMIT    zeppelin.interpreter.output.limit    102400    Output message from interpreter exceeding the limit will be truncated        ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT    zeppelin.interpreter.connect.timeout    30000    Output message from interpreter exceeding the limit will be truncated        ZEPPELIN_DEP_LOCALREPO    zeppelin.dep.localrepo    local-repo    Local repository for dependency loader.ex)visualiztion modules of npm.        ZEPPELIN_HELIUM_NODE_INSTALLER_URL    zeppelin.helium.node.installer.url    https://nodejs.org/dist/    Remote Node installer url for Helium dependency loader        ZEPPELIN_HELIUM_NPM_INSTALLER_URL    zeppelin.helium.npm.installer.url    http://registry.npmjs.org/    Remote Npm installer url for Helium dependency loader        ZEPPELIN_HELIUM_YARNPKG_INSTALLER_URL    zeppelin.helium.yarnpkg.installer.url    https://github.com/yarnpkg/yarn/releases/download/    Remote Yarn package installer url for Helium dependency loader        ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE    zeppelin.websocket.max.text.message.size    1024000    Size(in characters) of the maximum text message that can be received by websocket.        ZEPPELIN_SERVER_DEFAULT_DIR_ALLOWED    zeppelin.server.default.dir.allowed    false    Enable directory listings on server.        ZEPPELIN_NOTEBOOK_GIT_REMOTE_URL    zeppelin.notebook.git.remote.url        GitHub&#39;s repository URL. It could be either the HTTP URL or the SSH URL. For example git@github.com:apache/zeppelin.git        ZEPPELIN_NOTEBOOK_GIT_REMOTE_USERNAME    zeppelin.notebook.git.remote.username    token    GitHub username. By default it is `token` to use GitHub&#39;s API        ZEPPELIN_NOTEBOOK_GIT_REMOTE_ACCESS_TOKEN    zeppelin.notebook.git.remote.access-token    token    GitHub access token to use GitHub&#39;s API. If username/password combination is used and not GitHub API, then this value is the password        ZEPPELIN_NOTEBOOK_GIT_REMOTE_ORIGIN    zeppelin.notebook.git.remote.origin    token    GitHub remote name. Default is `origin`        ZEPPELIN_RUN_MODE    zeppelin.run.mode    auto    Run mode. &#39;auto|local|k8s&#39;. &#39;auto&#39; autodetect environment. &#39;local&#39; runs interpreter as a local process. k8s runs interpreter on Kubernetes cluster        ZEPPELIN_K8S_PORTFORWARD    zeppelin.k8s.portforward    false    Port forward to interpreter rpc port. Set &#39;true&#39; only on local development when zeppelin.k8s.mode &#39;on&#39;. Don&#39;t use &#39;true&#39; on production environment        ZEPPELIN_K8S_CONTAINER_IMAGE    zeppelin.k8s.container.image    apache/zeppelin:{{ site.ZEPPELIN_VERSION }}    Docker image for interpreters        ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE    zeppelin.k8s.spark.container.image    apache/spark:latest    Docker image for Spark executors        ZEPPELIN_K8S_TEMPLATE_DIR    zeppelin.k8s.template.dir    k8s    Kubernetes yaml spec files        ZEPPELIN_K8S_SERVICE_NAME    zeppelin.k8s.service.name    zeppelin-server    Name of the Zeppelin server service resources        ZEPPELIN_K8S_TIMEOUT_DURING_PENDING    zeppelin.k8s.timeout.during.pending    true    Value to enable/disable timeout handling when starting Interpreter Pods. Caution: This can lead to an infinity loop        ZEPPELIN_METRIC_ENABLE_PROMETHEUS    zeppelin.metric.enable.prometheus    false    Value to enable/disable Prometheus metric endpoint on /metric        ZEPPELIN_NOTEBOOK_CRON_ENABLE    zeppelin.notebook.cron.enable    false    Value to enable/disable Cron support in Notes        ZEPPELIN_NOTEBOOK_CRON_FOLDERS    zeppelin.notebook.cron.folders        comma-separated list of folder, where cron is allowed  ## SSL ConfigurationEnabling SSL requires a few configuration changes. First, you need to create certificates and then update necessary configurations to enable server side SSL and/or client side certificate authentication.### Creating and configuring the CertificatesInformation how about to generate certificates and a keystore can be found [here](https://wiki.eclipse.org/Jetty/Howto/Configure_SSL).A condensed example can be found in the top answer to this [StackOverflow post](http://stackoverflow.com/questions/4008837/configure-ssl-on-jetty).The keystore holds the private key and certificate on the server end. The trustore holds the trusted client certificates. Be sure that the path and password for these two stores are correctly configured in the password fields below. They can be obfuscated using the Jetty password tool. After Maven pulls in all the dependency to build Zeppelin, one of the Jetty jars contain the Password tool. Invoke this command from the Zeppelin home build directory with the appropriate version, user, and password.```bashjava -cp ./zeppelin-server/target/lib/jetty-all-server-.jar org.eclipse.jetty.util.security.Password  ```If you are using a self-signed, a certificate signed by an untrusted CA, or if client authentication is enabled, then the client must have a browser create exceptions for both the normal HTTPS port and WebSocket port. This can by done by trying to establish an HTTPS connection to both ports in a browser (e.g. if the ports are 443 and 8443, then visit https://127.0.0.1:443 and https://127.0.0.1:8443). This step can be skipped if the server certificate is signed by a trusted CA and client auth is disabled.### Configuring server side SSLThe following properties needs to be updated in the `zeppelin-site.xml` in order to enable server side SSL.```xml  zeppelin.server.ssl.port  8443  Server ssl port. (used when ssl property is set to true)  zeppelin.ssl  true  Should SSL be used by the servers?  zeppelin.ssl.keystore.path  keystore  Path to keystore relative to Zeppelin configuration directory  zeppelin.ssl.keystore.type  JKS  The format of the given keystore (e.g. JKS or PKCS12)  zeppelin.ssl.keystore.password  change me  Keystore password. Can be obfuscated by the Jetty Password tool  zeppelin.ssl.key.manager.password  change me  Key Manager password. Defaults to keystore password. Can be obfuscated.```### Enabling client side certificate authenticationThe following properties needs to be updated in the `zeppelin-site.xml` in order to enable client side certificate authentication.```xml  zeppelin.server.ssl.port  8443  Server ssl port. (used when ssl property is set to true)  zeppelin.ssl.client.auth  true  Should client authentication be used for SSL connections?  zeppelin.ssl.truststore.path  truststore  Path to truststore relative to Zeppelin configuration directory. Defaults to the keystore path  zeppelin.ssl.truststore.type  JKS  The format of the given truststore (e.g. JKS or PKCS12). Defaults to the same type as the keystore type  zeppelin.ssl.truststore.password  change me  Truststore password. Can be obfuscated by the Jetty Password tool. Defaults to the keystore password```### Storing user credentialsIn order to avoid having to re-enter credentials every time you restart/redeploy Zeppelin, you can store the user credentials. Zeppelin supports this via the ZEPPELIN_CREDENTIALS_PERSIST configuration.Please notice that passwords will be stored in *plain text* by default. To encrypt the passwords, use the ZEPPELIN_CREDENTIALS_ENCRYPT_KEY config variable. This will encrypt passwords using the AES-128 algorithm.You can generate an appropriate encryption key any way you&#39;d like - for instance, by using the openssl tool:```bashopenssl enc -aes-128-cbc -k secret -P -md sha1```*Important*: storing your encryption key in a configuration file is _not advised_. Depending on your environment security needs, you may want to consider utilizing a credentials server, storing the ZEPPELIN_CREDENTIALS_ENCRYPT_KEY as an OS env variable, or any other approach that would not colocate the encryption key and the encrypted content (the credentials.json file).### Obfuscating Passwords using the Jetty Password ToolSecurity best practices advise to not use plain text passwords and Jetty provides a password tool to help obfuscating the passwords used to access the KeyStore and TrustStore.The Password tool documentation can be found [here](http://www.eclipse.org/jetty/documentation/current/configuring-security-secure-passwords.html).After using the tool:```bashjava -cp $ZEPPELIN_HOME/zeppelin-server/target/lib/jetty-util-9.2.15.v20160210.jar          org.eclipse.jetty.util.security.Password           password2016-12-15 10:46:47.931:INFO::main: Logging initialized @101mspasswordOBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1vMD5:5f4dcc3b5aa765d61d8327deb882cf99```update your configuration with the obfuscated password :```xml  zeppelin.ssl.keystore.password  OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v  Keystore password. Can be obfuscated by the Jetty Password tool```### Create GitHub Access TokenWhen using GitHub to track notebooks, one can use GitHub&#39;s API for authentication. To create an access token, please use the following link https://github.com/settings/tokens.The value of the access token generated is set in the `zeppelin.notebook.git.remote.access-token` property.**Note:** After updating these configurations, Zeppelin server needs to be restarted.",
      "url": " /setup/operation/configuration.html",
      "group": "setup/operation",
      "excerpt": "This page will guide you to configure Apache Zeppelin using either environment variables or Java properties. Also, you can configure SSL for Zeppelin."
    }
    ,



    "/setup/operation/monitoring.html": {
      "title": "Apache Zeppelin Monitoring",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;# Apache Zeppelin Monitoring## Monitoring OptionsApache Zeppelin is using [Micrometer](https://micrometer.io/) - a vendor-neutral application metrics facade.### Prometheus Monitoring[Prometheus](https://prometheus.io/) is the leading monitoring solution for [Kubernetes](https://kubernetes.io/). The Prometheus endpoint can be activated with the configuration property `zeppelin.metric.enable.prometheus`. The metrics are accessible via the unauthenticated endpoint `/metrics`.### JMX Monitoring[JMX](https://en.wikipedia.org/wiki/Java_Management_Extensions) is a general solution for monitoring Java applications. JMX can be activated with the configuration property `zeppelin.jmx.enable`. The default port 9996 can be changed with the configuration property `zeppelin.jmx.port`.## Healthcheck ProbeApache Zeppelin has two healthcheck related unauthenticated endpoints (`/health/readiness`, `/health/liveness`) that could be used for proxy and/or cloud setups.",
      "url": " /setup/operation/monitoring.html",
      "group": "",
      "excerpt": "This page shows you the monitoring options you have in Apache Zeppelin"
    }
    ,



    "/setup/operation/proxy_setting.html": {
      "title": "Proxy Setting in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Proxy Setting ## How to Configure Proxies?Set `http_proxy` and `https_proxy` env variables. (See [more](https://wiki.archlinux.org/index.php/proxy_settings))Currently, Proxy is supported only for these features.- Helium: downloading `helium.json`, installing `npm`, `node`, `yarn`",
      "url": " /setup/operation/proxy_setting.html",
      "group": "security",
      "excerpt": "Apache Zeppelin supports Helium plugins which fetch required installer packages from remote registry/repositories"
    }
    ,



    "/setup/operation/trouble_shooting.html": {
      "title": "Trouble Shooting",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Troubleshooting ",
      "url": " /setup/operation/trouble_shooting.html",
      "group": "setup/operation",
      "excerpt": ""
    }
    ,



    "/setup/operation/upgrading.html": {
      "title": "Manual Zeppelin version upgrade procedure",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Manual upgrade procedure for ZeppelinBasically, newer version of Zeppelin works with previous version notebook directory and configurations.So, copying `notebook` and `conf` directory should be enough.## Instructions1. Stop Zeppelin: `bin/zeppelin-daemon.sh stop`2. Copy your `notebook` and `conf` directory into a backup directory3. Download newer version of Zeppelin and Install. See [Install Guide](../../quickstart/install.html#install).4. Copy backup `notebook` and `conf` directory into newer version of Zeppelin `notebook` and `conf` directory5. Start Zeppelin:  `bin/zeppelin-daemon.sh start`## Migration Guide### Upgrading from Zeppelin 0.8 to 0.9 - From 0.9, we changed the notes file name structure ([ZEPPELIN-2619](https://issues.apache.org/jira/browse/ZEPPELIN-2619)). So when you upgrading zeppelin to 0.9, you need to upgrade note files. Here&#39;s steps you need to follow:   1. Backup your notes file, in case the upgrade fails   2. Call `bin/upgrade-note.sh -d` to upgrade notes, `-d` option means to delete the old note file, missing this option will keep the old file. - From 0.9, the Zeppelin server binds to `127.0.0.1` by default, instead of `0.0.0.0`. Configure `zeppelin.server.addr` property or `ZEPPELIN_ADDR` env variable to change it to `0.0.0.0` if you want to access it remotely. - From 0.9, we have removed `zeppelin.anonymous.allowed` ([ZEPPELIN-4489](https://issues.apache.org/jira/browse/ZEPPELIN-4489)). So, when you upgrade Zeppelin to 0.9 and if `shiro.ini` file does not exist in conf path then all the Zeppelin-Users runs as anonymous. - From 0.9, we use `{crendential_entry.user}` and `{crendential_entry.password}` for credential injection, while before 0.9 we use `{user.crendential_entry}` and `{password.crendential_entry}` ### Upgrading from Zeppelin 0.8.1 (and before) to 0.8.2 (and later) - From 0.8.2, the Zeppelin server binds to `127.0.0.1` by default, instead of `0.0.0.0`. Configure the `zeppelin.server.addr` property or `ZEPPELIN_ADDR` env variable to change this.### Upgrading from Zeppelin 0.7 to 0.8 - From 0.8, we recommend using `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` instead of `zeppelin.pyspark.python` as `zeppelin.pyspark.python` only affects driver. You can use `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` as using them in spark. - From 0.8, depending on your device, the keyboard shortcut `Ctrl-L` or `Command-L` which goes to the line somewhere user wants is not supported. ### Upgrading from Zeppelin 0.6 to 0.7 - From 0.7, we don&#39;t use `ZEPPELIN_JAVA_OPTS` as default value of `ZEPPELIN_INTP_JAVA_OPTS` and also the same for `ZEPPELIN_MEM`/`ZEPPELIN_INTP_MEM`. If user want to configure the jvm opts of interpreter process, please set `ZEPPELIN_INTP_JAVA_OPTS` and `ZEPPELIN_INTP_MEM` explicitly. If you don&#39;t set `ZEPPELIN_INTP_MEM`, Zeppelin will set it to `-Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m` by default. - Mapping from `%jdbc(prefix)` to `%prefix` is no longer available. Instead, you can use %[interpreter alias] with multiple interpreter setttings on GUI. - Usage of `ZEPPELIN_PORT` is not supported in ssl mode. Instead use `ZEPPELIN_SSL_PORT` to configure the ssl port. Value from `ZEPPELIN_PORT` is used only when `ZEPPELIN_SSL` is set to `false`. - The support on Spark 1.1.x to 1.3.x is deprecated. - From 0.7, we uses `pegdown` as the `markdown.parser.type` option for the `%md` interpreter. Rendered markdown might be different from what you expected - From 0.7 note.json format has been changed to support multiple outputs in a paragraph. Zeppelin will automatically convert old format to new format. 0.6 or lower version can read new note.json format but output will not be displayed. For the detail, see [ZEPPELIN-212](http://issues.apache.org/jira/browse/ZEPPELIN-212) and [pull request](https://github.com/apache/zeppelin/pull/1658). - From 0.7 note storage layer will utilize `GitNotebookRepo` by default instead of `VFSNotebookRepo` storage layer, which is an extension of latter one with versioning capabilities on top of it.",
      "url": " /setup/operation/upgrading.html",
      "group": "setup/operation",
      "excerpt": "This document will guide you through a procedure of manual upgrade your Apache Zeppelin instance to a newer version. Apache Zeppelin keeps backward compatibility for the notebook file format."
    }
    ,



    "/setup/security/authentication_nginx.html": {
      "title": "HTTP Basic Auth using NGINX",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Authentication for NGINX[Build in authentication mechanism](./shiro_authentication.html) is recommended way for authentication. In case of you want authenticate using NGINX and [HTTP basic auth](https://en.wikipedia.org/wiki/Basic_access_authentication), please read this document.## HTTP Basic Authentication using NGINX&gt; **Quote from Wikipedia:** NGINX is a web server. It can act as a reverse proxy server for HTTP, HTTPS, SMTP, POP3, and IMAP protocols, as well as a load balancer and an HTTP cache.So you can use NGINX server as proxy server to serve HTTP Basic Authentication as a separate process along with Zeppelin server.Here are instructions how to accomplish the setup NGINX as a front-end authentication server and connect Zeppelin at behind.This instruction based on Ubuntu 14.04 LTS but may work with other OS with few configuration changes.1. Install NGINX server on your server instance    You can install NGINX server with same box where zeppelin installed or separate box where it is dedicated to serve as proxy server.    ```bash    $ apt-get install nginx    ```    &gt; **NOTE :** On pre 1.3.13 version of NGINX, Proxy for Websocket may not fully works. Please use latest version of NGINX. See: [NGINX documentation](https://www.nginx.com/blog/websocket-nginx/).1. Setup init script in NGINX    In most cases, NGINX configuration located under `/etc/nginx/sites-available`. Create your own configuration or add your existing configuration at `/etc/nginx/sites-available`.    ```bash    $ cd /etc/nginx/sites-available    $ touch my-zeppelin-auth-setting    ```    Now add this script into `my-zeppelin-auth-setting` file. You can comment out `optional` lines If you want serve Zeppelin under regular HTTP 80 Port.    ```    upstream zeppelin {        server [YOUR-ZEPPELIN-SERVER-IP]:[YOUR-ZEPPELIN-SERVER-PORT];   # For security, It is highly recommended to make this address/port as non-public accessible    }    # Zeppelin Website    server {        listen [YOUR-ZEPPELIN-WEB-SERVER-PORT];        listen 443 ssl;                                      # optional, to serve HTTPS connection        server_name [YOUR-ZEPPELIN-SERVER-HOST];             # for example: zeppelin.mycompany.com        ssl_certificate [PATH-TO-YOUR-CERT-FILE];            # optional, to serve HTTPS connection        ssl_certificate_key [PATH-TO-YOUR-CERT-KEY-FILE];    # optional, to serve HTTPS connection        if ($ssl_protocol = &quot;&quot;) {            rewrite ^ https://$host$request_uri? permanent;  # optional, to force use of HTTPS        }        location / {    # For regular websever support            proxy_pass http://zeppelin;            proxy_set_header X-Real-IP $remote_addr;            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;            proxy_set_header Host $http_host;            proxy_set_header X-NginX-Proxy true;            proxy_redirect off;            auth_basic &quot;Restricted&quot;;            auth_basic_user_file /etc/nginx/.htpasswd;        }        location /ws {  # For websocket support            proxy_pass http://zeppelin/ws;            proxy_http_version 1.1;            proxy_set_header Upgrade websocket;            proxy_set_header Connection upgrade;            proxy_read_timeout 86400;        }    }    ```    Then make a symbolic link to this file from `/etc/nginx/sites-enabled/` to enable configuration above when NGINX reloads.    ```bash    $ ln -s /etc/nginx/sites-enabled/my-zeppelin-auth-setting /etc/nginx/sites-available/my-zeppelin-auth-setting    ```1. Setup user credential into `.htpasswd` file and restart server    Now you need to setup `.htpasswd` file to serve list of authenticated user credentials for NGINX server.    ```bash    $ cd /etc/nginx    $ htpasswd -c htpasswd [YOUR-ID]    NEW passwd: [YOUR-PASSWORD]    RE-type new passwd: [YOUR-PASSWORD-AGAIN]    ```    Or you can use your own apache `.htpasswd` files in other location for setting up property: `auth_basic_user_file`    Restart NGINX server.    ```bash    $ service nginx restart    ```    Then check HTTP Basic Authentication works in browser. If you can see regular basic auth popup and then able to login with credential you entered into `.htpasswd` you are good to go.1. More security consideration* Using HTTPS connection with Basic Authentication is highly recommended since basic auth without encryption may expose your important credential information over the network.* Using [Shiro Security feature built-into Zeppelin](./shiro_authentication.html) is recommended if you prefer all-in-one solution for authentication but NGINX may provides ad-hoc solution for re-use authentication served by your system&#39;s NGINX server or in case of you need to separate authentication from zeppelin server.* It is recommended to isolate direct connection to Zeppelin server from public internet or external services to secure your zeppelin instance from unexpected attack or problems caused by public zone.## Another optionAnother option is to have an authentication server that can verify user credentials in an LDAP server.If an incoming request to the Zeppelin server does not have a cookie with user information encrypted with the authentication server public key, the useris redirected to the authentication server. Once the user is verified, the authentication server redirects the browser to a specific URL in the Zeppelin server which sets the authentication cookie in the browser.The end result is that all requests to the Zeppelin web server have the authentication cookie which contains user and groups information.",
      "url": " /setup/security/authentication_nginx.html",
      "group": "setup/security",
      "excerpt": "There are multiple ways to enable authentication in Apache Zeppelin. This page describes HTTP basic auth using NGINX."
    }
    ,



    "/setup/security/datasource_authorization.html": {
      "title": "Data Source Authorization in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Data Source Authorization in Apache Zeppelin## OverviewData source authorization involves authenticating to the data source like a Mysql database and letting it determine user permissions.Apache Zeppelin allows users to use their own credentials to authenticate with **Data Sources**.For example, let&#39;s assume you have an account in the Vertica databases with credentials. You might want to use this account to create a JDBC connection instead of a shared account with all users who are defined in `conf/shiro.ini`. In this case, you can add your credential information to Apache Zeppelin and use them with below simple steps.  ## How to save the credential information?You can add new credentials in the dropdown menu for your data source which can be passed to interpreters. **Entity** can be the key that distinguishes each credential sets.(We suggest that the convention of the **Entity** is `Interpreter Name`.)Type **Username &amp; Password** for your own credentials. ex) Mysql user &amp; password of the JDBC Interpreter.The credentials saved as per users defined in `conf/shiro.ini`.If you didn&#39;t activate [shiro authentication in Apache Zeppelin](./shiro_authentication.html), your credential information will be saved as `anonymous`.All credential information also can be found in `conf/credentials.json`. #### JDBC interpreterYou need to maintain per-user connection pools.The interpret method takes the user string as a parameter and executes the jdbc call using a connection in the user&#39;s connection pool.#### Presto You don&#39;t need a password if the Presto DB server runs backend code using HDFS authorization for the user.#### Vertica and Mysql You have to store the password information for users.## Please noteAs a first step of data source authentication feature, [ZEPPELIN-828](https://issues.apache.org/jira/browse/ZEPPELIN-828) was proposed and implemented in Pull Request [#860](https://github.com/apache/zeppelin/pull/860).Currently, only customized 3rd party interpreters can use this feature. We are planning to apply this mechanism to [the community managed interpreters](../../usage/interpreter/installation.html#available-community-managed-interpreters) in the near future. Please keep track [ZEPPELIN-1070](https://issues.apache.org/jira/browse/ZEPPELIN-1070). ",
      "url": " /setup/security/datasource_authorization.html",
      "group": "setup/security",
      "excerpt": "Apache Zeppelin supports protected data sources. In case of a MySql database, every users can set up their own credentials to access it."
    }
    ,



    "/setup/security/http_security_headers.html": {
      "title": "Setting up HTTP Response Headers",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Setting up HTTP Response Headers for Zeppelin Apache Zeppelin can be configured to include HTTP Headers which aids in preventing Cross Site Scripting (XSS), Cross-Frame Scripting (XFS) and also enforces HTTP Strict Transport Security. Apache Zeppelin also has configuration available to set the Application Server Version to desired value.## Setting up HTTP Strict Transport Security (HSTS) Response HeaderEnabling HSTS Response Header prevents Man-in-the-middle attacks by automatically redirecting HTTP requests to HTTPS when Zeppelin Server is running on SSL. Read on how to configure SSL for Zeppelin [here] (../operation/configuration.html). Even if web page contains any resource which gets served over HTTP or any HTTP links, it will automatically be redirected to HTTPS for the target domain. It also prevents MITM attack by not allowing User to override the invalid certificate message, when Attacker presents invalid SSL certificate to the User.  The following property needs to be updated in the zeppelin-site.xml in order to enable HSTS. You can choose appropriate value for &quot;max-age&quot;.```xml  zeppelin.server.strict.transport  max-age=631138519  The HTTP Strict-Transport-Security response header is a security feature that lets a web site tell browsers that it should only be communicated with using HTTPS, instead of using HTTP. Enable this when Zeppelin is running on HTTPS. Value is in Seconds, the default value is equivalent to 20 years.```Possible values are:* max-age=* max-age=; includeSubDomains* max-age=; preloadRead more about HSTS [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security).## Setting up X-XSS-PROTECTION HeaderThe HTTP X-XSS-Protection response header is a feature of Internet Explorer, Chrome and Safari Web browsers that initiates configured action when they detect reflected cross-site scripting (XSS) attacks. The below property to set X-XSS-Protection header is enabled with default value of &quot;1; mode=block&quot; in the zeppelin-site.xml```xml  zeppelin.server.xxss.protection  1; mode=block  The HTTP X-XSS-Protection response header is a feature of Internet Explorer, Chrome and Safari that stops pages from loading when they detect reflected cross-site scripting (XSS) attacks. When value is set to 1 and a cross-site scripting attack is detected, the browser will sanitize the page (remove the unsafe parts).```You can choose appropriate value from below to update the configuration if required.* 0  (Disables XSS filtering)* 1  (Enables XSS filtering. If a cross-site scripting attack is detected, the browser will sanitize the page.)* 1; mode=block  (Enables XSS filtering. The browser will prevent rendering of the page if an attack is detected.)Read more about HTTP X-XSS-Protection response header [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-XSS-Protection).## Setting up X-Frame-Options HeaderThe X-Frame-Options HTTP response header can indicate browser to avoid clickjacking attacks, by ensuring that their content is not embedded into other sites in a ``,`` or ``.The below property to set X-Frame-Options header is enabled with default value of &quot;SAMEORIGIN&quot; in the zeppelin-site.xml```xml  zeppelin.server.xframe.options  SAMEORIGIN  The X-Frame-Options HTTP response header can be used to indicate whether or not a browser should be allowed to render a page in a frame/iframe/object.```You can choose appropriate value from below to update the configuration if required.* `DENY`* `SAMEORIGIN`* `ALLOW-FROM uri`## Setting up X-Content-Type-Options HeaderThe HTTP X-Content-Type-Options response header helps to prevent MIME type sniffing attacks. It directs the browser to honor the type specified in the Content-Type header, rather than trying to determine the type from the content itself. The default value `nosniff` is really the only meaningful value. This header is supported on all browsers except Safari and Safari on iOS.The below property to set X-Content-Type-Options header is enabled with default value of &quot;nosniff&quot; in the zeppelin-site.xml```xml  zeppelin.server.xcontent.type.options  nosniff  The HTTP X-Content-Type-Options response header helps to prevent MIME type sniffing attacks.```## Setting up Server HeaderSecurity conscious organisations does not want to reveal the Application Server name and version to prevent finding this information easily by Attacker while fingerprinting the Application. The exact version number can tell an Attacker if the current Application Server is patched for or vulnerable to certain publicly known CVE associated to it.The below property to mask Jetty server version is enabled by default and configured with value of &quot; &quot; (one whitespace char) in the zeppelin-site.xml```xml    zeppelin.server.jetty.name         Hardcoding Application Server name to Prevent Fingerprinting```The value can be any &quot;String&quot;. Removing this property from configuration will cause Zeppelin to send correct Jetty server version.Also, it can be removed the from response headers and from 300/400/500 HTTP response pages.```xml    zeppelin.server.send.jetty.name    false    If set to false, will not show the Jetty version to prevent Fingerprinting```",
      "url": " /setup/security/http_security_headers.html",
      "group": "setup/security",
      "excerpt": "There are multiple HTTP Security Headers which can be configured in Apache Zeppelin. This page describes how to enable them by providing appropriate value in Zeppelin configuration file."
    }
    ,



    "/setup/security/notebook_authorization.html": {
      "title": "Notebook Authorization in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Zeppelin Notebook Authorization## OverviewWe assume that there is an **Shiro Authentication** component that associates a user string and a set of group strings with every NotebookSocket.If you don&#39;t set the authentication components yet, please check [Shiro authentication for Apache Zeppelin](./shiro_authentication.html) first.## Authorization SettingYou can set Zeppelin notebook permissions in each notebooks. Of course only **notebook owners** can change this configuration.Just click **Lock icon** and open the permission setting page in your notebook.As you can see, each Zeppelin notebooks has 3 entities :* Owners ( users or groups )* Readers ( users or groups )* Writers ( users or groups )* Runners ( users or groups )Fill out the each forms with comma seperated **users** and **groups** configured in `conf/shiro.ini` file.If the form is empty (*), it means that any users can perform that operation.If someone who doesn&#39;t have **read** permission is trying to access the notebook or someone who doesn&#39;t have **write** permission is trying to edit the notebook,or someone who doesn&#39;t have **run** permission is trying to run a paragraph Zeppelin will ask to login or block the user.By default, owners and writers have **write** permission, owners, writers and runners have **run** permission, owners, writers, runners and readers have **read** permission## Separate notebook workspaces (public vs. private)By default, the authorization rights allow other users to see the newly created note, meaning the workspace is `public`. This behavior is controllable and can be set through either `ZEPPELIN_NOTEBOOK_PUBLIC` variable in `conf/zeppelin-env.sh`, or through `zeppelin.notebook.public` property in `conf/zeppelin-site.xml`. Thus, in order to make newly created note appear only in your `private` workspace by default, you can set either `ZEPPELIN_NOTEBOOK_PUBLIC` to `false` in your `conf/zeppelin-env.sh` as follows:```bashexport ZEPPELIN_NOTEBOOK_PUBLIC=&quot;false&quot;```or set `zeppelin.notebook.public` property to `false` in `conf/zeppelin-site.xml` as follows:```xml  zeppelin.notebook.public  false  Make notebook public by default when created, private otherwise```Behind the scenes, when you create a new note only the `owners` field is filled with current user, leaving `readers`, `runners` and `writers` fields empty. All the notes with at least one empty authorization field are considered to be in `public` workspace. Thus when setting `zeppelin.notebook.public` (or corresponding `ZEPPELIN_NOTEBOOK_PUBLIC`) to false, newly created notes have `readers`, `runners`, `writers` fields filled with current user, making note appear as in `private` workspace.## How it worksIn this section, we will explain the detail about how the notebook authorization works in backend side.### NotebookServerThe [NotebookServer](https://github.com/apache/zeppelin/blob/master/zeppelin-server/src/main/java/org/apache/zeppelin/socket/NotebookServer.java) classifies every notebook operations into three categories: **Read**, **Run**, **Write**, **Manage**.Before executing a notebook operation, it checks if the user and the groups associated with the `NotebookSocket` have permissions.For example, before executing a **Read** operation, it checks if the user and the groups have at least one entity that belongs to the **Reader** entities.### Notebook REST API callZeppelin executes a [REST API call](https://github.com/apache/zeppelin/blob/master/zeppelin-server/src/main/java/org/apache/zeppelin/rest/NotebookRestApi.java) for the notebook permission information.In the backend side, Zeppelin gets the user information for the connection and allows the operation if the users and groupsassociated with the current user have at least one entity that belongs to owner entities for the notebook.",
      "url": " /setup/security/notebook_authorization.html",
      "group": "setup/security",
      "excerpt": "This page will guide you how you can set the permission for Zeppelin notebooks. This document assumes that Apache Shiro authentication was set up."
    }
    ,



    "/setup/security/shiro_authentication.html": {
      "title": "Apache Shiro Authentication for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Shiro authentication for Apache Zeppelin## Overview[Apache Shiro](http://shiro.apache.org/) is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management. In this documentation, we will explain step by step how Shiro works for Zeppelin notebook authentication.When you connect to Apache Zeppelin, you will be asked to enter your credentials. Once you logged in, then you have access to all notes including other user&#39;s notes.## Important NoteBy default, Zeppelin allows anonymous access. It is strongly recommended that you consider setting up Apache Shiro for authentication (as described in this document, see 2 Secure the Websocket channel), or only deploy and use Zeppelin in a secured and trusted environment.## Security SetupYou can setup **Zeppelin notebook authentication** in some simple steps.### 1. Enable ShiroBy default in `conf`, you will find `shiro.ini.template`, this file is used as an example and it is strongly recommendedto create a `shiro.ini` file by doing the following command line```bashcp conf/shiro.ini.template conf/shiro.ini```For the further information about  `shiro.ini` file format, please refer to [Shiro Configuration](http://shiro.apache.org/configuration.html#Configuration-INISections).### 2. Start Zeppelin```bashbin/zeppelin-daemon.sh start #(or restart)```Then you can browse Zeppelin at [http://localhost:8080](http://localhost:8080).### 3. LoginFinally, you can login using one of the below **username/password** combinations.```[users]admin = password1, adminuser1 = password2, role1, role2user2 = password3, role3user3 = password4, role2```You can set the roles for each users next to the password.## Groups and permissions (optional)In case you want to leverage user groups and permissions, use one of the following configuration for LDAP or AD under `[main]` segment in `shiro.ini`.```activeDirectoryRealm = org.apache.zeppelin.realm.ActiveDirectoryGroupRealmactiveDirectoryRealm.systemUsername = userNameAactiveDirectoryRealm.systemPassword = passwordAactiveDirectoryRealm.searchBase = CN=Users,DC=SOME_GROUP,DC=COMPANY,DC=COMactiveDirectoryRealm.url = ldap://ldap.test.com:389activeDirectoryRealm.groupRolesMap = &quot;CN=aGroupName,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM&quot;:&quot;group1&quot;activeDirectoryRealm.authorizationCachingEnabled = falseactiveDirectoryRealm.principalSuffix = @corp.company.netldapRealm = org.apache.zeppelin.realm.LdapGroupRealm# search base for ldap groups (only relevant for LdapGroupRealm):ldapRealm.contextFactory.environment[ldap.searchBase] = dc=COMPANY,dc=COMldapRealm.contextFactory.url = ldap://ldap.test.com:389ldapRealm.userDnTemplate = uid={0},ou=Users,dc=COMPANY,dc=COMldapRealm.contextFactory.authenticationMechanism = simple```also define roles/groups that you want to have in system, like below;```[roles]admin = *hr = *finance = *group1 = *```## Configure Realm (optional)Realms are responsible for authentication and authorization in Apache Zeppelin. By default, Apache Zeppelin uses [IniRealm](https://shiro.apache.org/static/latest/apidocs/org/apache/shiro/realm/text/IniRealm.html) (users and groups are configurable in `conf/shiro.ini` file under `[user]` and `[group]` section). You can also leverage Shiro Realms like [JndiLdapRealm](https://shiro.apache.org/static/latest/apidocs/org/apache/shiro/realm/ldap/JndiLdapRealm.html), [JdbcRealm](https://shiro.apache.org/static/latest/apidocs/org/apache/shiro/realm/jdbc/JdbcRealm.html) or create [our own](https://shiro.apache.org/static/latest/apidocs/org/apache/shiro/realm/AuthorizingRealm.html).To learn more about Apache Shiro Realm, please check [this documentation](http://shiro.apache.org/realm.html).We also provide community custom Realms.**Note**: When using any of the below realms the default       password-based (IniRealm) authentication needs to be disabled.### Active Directory```activeDirectoryRealm = org.apache.zeppelin.realm.ActiveDirectoryGroupRealmactiveDirectoryRealm.systemUsername = userNameAactiveDirectoryRealm.systemPassword = passwordAactiveDirectoryRealm.hadoopSecurityCredentialPath = jceks://file/user/zeppelin/conf/zeppelin.jceksactiveDirectoryRealm.searchBase = CN=Users,DC=SOME_GROUP,DC=COMPANY,DC=COMactiveDirectoryRealm.url = ldap://ldap.test.com:389activeDirectoryRealm.groupRolesMap = &quot;CN=aGroupName,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM&quot;:&quot;group1&quot;activeDirectoryRealm.authorizationCachingEnabled = falseactiveDirectoryRealm.principalSuffix = @corp.company.net```Also instead of specifying systemPassword in clear text in shiro.ini administrator can choose to specify the same in &quot;hadoop credential&quot;.Create a keystore file using the hadoop credential commandline, for this the hadoop commons should be in the classpath`hadoop credential create activeDirectoryRealm.systempassword -provider jceks://file/user/zeppelin/conf/zeppelin.jceks`Change the following values in the Shiro.ini file, and uncomment the line:`activeDirectoryRealm.hadoopSecurityCredentialPath = jceks://file/user/zeppelin/conf/zeppelin.jceks`### LDAPTwo options exist for configuring an LDAP Realm. The simpler to use is the LdapGroupRealm. How ever it has limitedflexibility with mapping of ldap groups to users and for authorization for user groups. A sample configuration file forthis realm is given below.```ldapRealm = org.apache.zeppelin.realm.LdapGroupRealm# search base for ldap groups (only relevant for LdapGroupRealm):ldapRealm.contextFactory.environment[ldap.searchBase] = dc=COMPANY,dc=COMldapRealm.contextFactory.url = ldap://ldap.test.com:389ldapRealm.userDnTemplate = uid={0},ou=Users,dc=COMPANY,dc=COMldapRealm.contextFactory.authenticationMechanism = simple```The other more flexible option is to use the LdapRealm. It allows for mapping of ldapgroups to roles and also allows for role/group based authentication into the zeppelin server. Sample configuration for this realm is given below.```[main]ldapRealm=org.apache.zeppelin.realm.LdapRealmldapRealm.contextFactory.authenticationMechanism=simpleldapRealm.contextFactory.url=ldap://localhost:33389ldapRealm.userDnTemplate=uid={0},ou=people,dc=hadoop,dc=apache,dc=org# Ability to set ldap paging Size if needed default is 100ldapRealm.pagingSize = 200ldapRealm.authorizationEnabled=trueldapRealm.contextFactory.systemAuthenticationMechanism=simpleldapRealm.searchBase=dc=hadoop,dc=apache,dc=orgldapRealm.userSearchBase = dc=hadoop,dc=apache,dc=orgldapRealm.groupSearchBase = ou=groups,dc=hadoop,dc=apache,dc=orgldapRealm.groupObjectClass=groupofnames# Allow userSearchAttribute to be customizedldapRealm.userSearchAttributeName = sAMAccountNameldapRealm.memberAttribute=member# force usernames returned from ldap to lowercase useful for ADldapRealm.userLowerCase = true# ability set searchScopes subtree (default), one, baseldapRealm.userSearchScope = subtree;ldapRealm.groupSearchScope = subtree;ldapRealm.memberAttributeValueTemplate=cn={0},ou=people,dc=hadoop,dc=apache,dc=orgldapRealm.contextFactory.systemUsername=uid=guest,ou=people,dc=hadoop,dc=apache,dc=orgldapRealm.contextFactory.systemPassword=S{ALIAS=ldcSystemPassword}# enable support for nested groups using the LDAP_MATCHING_RULE_IN_CHAIN operatorldapRealm.groupSearchEnableMatchingRuleInChain = true# optional mapping from physical groups to logical application rolesldapRealm.rolesByGroup = LDN_USERS: user_role, NYK_USERS: user_role, HKG_USERS: user_role, GLOBAL_ADMIN: admin_role# optional list of roles that are allowed to authenticate. Incase not present all groups are allowed to authenticate (login).# This changes nothing for url specific permissions that will continue to work as specified in [urls].ldapRealm.allowedRolesForAuthentication = admin_role,user_roleldapRealm.permissionsByRole= user_role = *:ToDoItemsJdo:*:*, *:ToDoItem:*:*; admin_role = *securityManager.sessionManager = $sessionManagersecurityManager.realms = $ldapRealm```Also instead of specifying systemPassword in clear text in `shiro.ini` administrator can choose to specify the same in &quot;hadoop credential&quot;. Create a keystore file using the hadoop credential command line:``` hadoop credential create ldapRealm.systemPassword -provider jceks://file/user/zeppelin/conf/zeppelin.jceks```Add the following line in the `shiro.ini` file:``` ldapRealm.hadoopSecurityCredentialPath = jceks://file/user/zeppelin/conf/zeppelin.jceks```**Caution** due to a bug in LDAPRealm only ```ldapRealm.pagingSize``` results will be fetched from LDAP. In big directory Trees this may cause missing Roles. Try limiting the search Scope using ```ldapRealm.groupSearchBase``` or narrow down the required Groups using ```ldapRealm.groupSearchFilter```### PAM[PAM](https://en.wikipedia.org/wiki/Pluggable_authentication_module) authentication support allows the reuse of existing authenticationmoduls on the host where Zeppelin is running. On a typical system modules are configured per service for example sshd, passwd, etc. under `/etc/pam.d/`. You caneither reuse one of these services or create your own for Zeppelin. Activiting PAM authentication requires two parameters: 1. realm: The Shiro realm being used 2. service: The service configured under `/etc/pam.d/` to be used. The name here needs to be the same as the file name under `/etc/pam.d/````[main] pamRealm=org.apache.zeppelin.realm.PamRealm pamRealm.service=sshd```### ZeppelinHub[ZeppelinHub](https://www.zeppelinhub.com) is a service that synchronize your Apache Zeppelin notebooks and enables you to collaborate easily.To enable login with your ZeppelinHub credential, apply the following change in `conf/shiro.ini` under `[main]` section.```### A sample for configuring ZeppelinHub RealmzeppelinHubRealm = org.apache.zeppelin.realm.ZeppelinHubRealm## Url of ZeppelinHubzeppelinHubRealm.zeppelinhubUrl = https://www.zeppelinhub.comsecurityManager.realms = $zeppelinHubRealm```&gt; Note: ZeppelinHub is not related to Apache Zeppelin project.### Knox SSO[KnoxSSO](https://knox.apache.org/books/knox-0-13-0/dev-guide.html#KnoxSSO+Integration) provides an abstraction for integrating any number of authentication systems and SSO solutions and enables participating web applications to scale to those solutions more easily. Without the token exchange capabilities offered by KnoxSSO each component UI would need to integrate with each desired solution on its own.To enable this, apply the following change in `conf/shiro.ini` under `[main]` section.```### A sample for configuring Knox JWT RealmknoxJwtRealm = org.apache.zeppelin.realm.jwt.KnoxJwtRealm## Domain of Knox SSOknoxJwtRealm.providerUrl = https://domain.example.com/## Url for loginknoxJwtRealm.login = gateway/knoxsso/knoxauth/login.html## Url for logoutknoxJwtRealm.logout = gateway/knoxssout/api/v1/webssoutknoxJwtRealm.redirectParam = originalUrlknoxJwtRealm.cookieName = hadoop-jwtknoxJwtRealm.publicKeyPath = /etc/zeppelin/conf/knox-sso.pem# This is required if KNOX SSO is enabled, to check if &quot;knoxJwtRealm.cookieName&quot; cookie was expired/deleted.  authc = org.apache.zeppelin.realm.jwt.KnoxAuthenticationFilter```### HTTP SPNEGO AuthenticationHTTP SPNEGO (Simple and Protected GSS-API NEGOtiation) is the standard way to support Kerberos Ticket based user authentication for Web Services. Based on [Apache Hadoop Auth](https://hadoop.apache.org/docs/current/hadoop-auth/index.html), Zeppelin supports ability to authenticate users by accepting and validating their Kerberos Ticket.When HTTP SPNEGO Authentication is enabled for Zeppelin, the [Apache Hadoop Groups Mapping](https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/GroupsMapping.html) configuration will used internally to determine group membership of user who is trying to log in. Role-based access permission can be set based on groups as seen by Hadoop.To enable this, apply the following change in `conf/shiro.ini` under `[main]` section.```krbRealm = org.apache.zeppelin.realm.kerberos.KerberosRealmkrbRealm.principal=HTTP/zeppelin.fqdn.domain.com@EXAMPLE.COMkrbRealm.keytab=/etc/security/keytabs/spnego.service.keytabkrbRealm.nameRules=DEFAULTkrbRealm.signatureSecretFile=/etc/security/http_secretkrbRealm.tokenValidity=36000krbRealm.cookieDomain=domain.comkrbRealm.cookiePath=/authc = org.apache.zeppelin.realm.kerberos.KerberosAuthenticationFilter```For above configuration to work, user need to do some more configurations outside Zeppelin.1). A valid SPNEGO keytab should be available on the Zeppelin node and should be readable by &#39;zeppelin&#39; user. If there is a SPNEGO keytab already available (because of other Hadoop service), it can be reused here and no need to generate a new keytab. An example of working SPNEGO keytab could be:```$ klist -kt /etc/security/keytabs/spnego.service.keytabKeytab name: FILE:/etc/security/keytabs/spnego.service.keytabKVNO Timestamp           Principal---- ------------------- ------------------------------------------------------   2 11/26/2018 16:58:38 HTTP/zeppelin.fqdn.domain.com@EXAMPLE.COM   2 11/26/2018 16:58:38 HTTP/zeppelin.fqdn.domain.com@EXAMPLE.COM   2 11/26/2018 16:58:38 HTTP/zeppelin.fqdn.domain.com@EXAMPLE.COM   2 11/26/2018 16:58:38 HTTP/zeppelin.fqdn.domain.com@EXAMPLE.COM```and the keytab permission should be: (VERY IMPORTANT to not to set this to 777 or readable by all !!!):```$ ls -l /etc/security/keytabs/spnego.service.keytab-r--r-----. 1 root hadoop 346 Nov 26 16:58 /etc/security/keytabs/spnego.service.keytab```Above &#39;zeppelin&#39; user happens to be member of &#39;hadoop&#39; group.2). A secret signature file must be present on Zeppelin node (readable to &#39;zeppelin&#39; user). This file contains the random binary numbers which is used to sign &#39;hadoop.auth&#39; cookie, generated during SPNEGO exchange. If such a file is already generated and available on the Zeppelin node, it should be used rather than generating a new file.Commands to generate a secret signature file (if required):```dd if=/dev/urandom of=/etc/security/http_secret bs=1024 count=1chown hdfs:hadoop /etc/security/http_secretchmod 440 /etc/security/http_secret```## Secure Cookie for Zeppelin Sessions (optional)Zeppelin can be configured to set `HttpOnly` flag in the session cookie. With this configuration, Zeppelin cookies can not be accessed via client side scripts thus preventing majority of Cross-site scripting (XSS) attacks.To enable secure cookie support via Shiro, add the following lines in `conf/shiro.ini` under `[main]` section, afterdefining a `sessionManager`.```cookie = org.apache.shiro.web.servlet.SimpleCookiecookie.name = JSESSIONIDcookie.secure = truecookie.httpOnly = truesessionManager.sessionIdCookie = $cookie```## Secure your Zeppelin information (optional)By default, anyone who defined in `[users]` can share **Interpreter Setting**, **Credential** and **Configuration** information in Apache Zeppelin.Sometimes you might want to hide these information for your use case.Since Shiro provides **url-based security**, you can hide the information by commenting or uncommenting these below lines in `conf/shiro.ini`.```[urls]/api/interpreter/** = authc, roles[admin]/api/configurations/** = authc, roles[admin]/api/credential/** = authc, roles[admin]```In this case, only who have `admin` role can see **Interpreter Setting**, **Credential** and **Configuration** information.If you want to grant this permission to other users, you can change **roles[ ]** as you defined at `[users]` section.### Apply multiple roles in Shiro configurationBy default, Shiro will allow access to a URL if only user is part of &quot;**all the roles**&quot; defined like this:```[urls]/api/interpreter/** = authc, roles[admin, role1]```### Apply multiple roles or user in Shiro configurationIf there is a need that user with &quot;**any of the defined roles or user itself**&quot; should be allowed, then following Shiro configuration can be used:```[main]anyofrolesuser = org.apache.zeppelin.utils.AnyOfRolesUserAuthorizationFilter[urls]/api/interpreter/** = authc, anyofrolesuser[admin, user1]/api/configurations/** = authc, roles[admin]/api/credential/** = authc, roles[admin]```&gt; **NOTE :** All of the above configurations are defined in the `conf/shiro.ini` file.## FAQZeppelin sever is configured as form-based authentication but is behind proxy configured as basic-authentication for example [NGINX](./authentication_nginx.html#http-basic-authentication-using-nginx) and don&#39;t want Zeppelin-Server to clear authentication headers. &gt; Set `zeppelin.server.authorization.header.clear` to `false` in zeppelin-site.xml## Other authentication methods- [HTTP Basic Authentication using NGINX](./authentication_nginx.html)",
      "url": " /setup/security/shiro_authentication.html",
      "group": "setup/security",
      "excerpt": "Apache Shiro is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management. This document explains step by step how Shiro can be used for Zeppelin notebook authentication."
    }
    ,



    "/setup/storage/storage.html": {
      "title": "Notebook Storage for Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Notebook storage options for Apache Zeppelin## OverviewApache Zeppelin has a pluggable notebook storage mechanism controlled by `zeppelin.notebook.storage` configuration option with multiple implementations.There are few notebook storage systems available for a use out of the box:  * (default) use local file system and version it using local Git repository - `GitNotebookRepo`  * all notes are saved in the notebook folder in your local File System - `VFSNotebookRepo`  * all notes are saved in the notebook folder in hadoop compatible file system - `FileSystemNotebookRepo`  * storage using Amazon S3 service - `S3NotebookRepo`  * storage using Azure service - `AzureNotebookRepo`  * storage using Google Cloud Storage - `GCSNotebookRepo`  * storage using Aliyun OSS - `OSSNotebookRepo`  * storage using MongoDB - `MongoNotebookRepo`  * storage using GitHub - `GitHubNotebookRepo`Multiple storage systems can be used at the same time by providing a comma-separated list of the class-names in the configuration.By default, only first two of them will be automatically kept in sync by Zeppelin.## Notebook Storage in local Git repository To enable versioning for all your local notebooks though a standard Git repository - uncomment the next property in `zeppelin-site.xml` in order to use GitNotebookRepo class:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo  notebook persistence layer implementation```## Notebook Storage in hadoop compatible file system repository Notes may be stored in hadoop compatible file system such as hdfs, so that multiple Zeppelin instances can share the same notes. It supports all the versions of hadoop 2.x. If you use `FileSystemNotebookRepo`, then `zeppelin.notebook.dir` is the path on the hadoop compatible file system. And you need to specify `HADOOP_CONF_DIR` in `zeppelin-env.sh` so that zeppelin can find the right hadoop configuration files.If your hadoop cluster is kerberized, then you need to specify `zeppelin.server.kerberos.keytab` and `zeppelin.server.kerberos.principal````xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo  hadoop compatible file system notebook persistence layer implementation```## Notebook Storage in S3 Notebooks may be stored in S3, and optionally encrypted.  The [``DefaultAWSCredentialsProviderChain``](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html) credentials provider is used for credentials and checks the following:- The ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` environment variables- The ``aws.accessKeyId`` and ``aws.secretKey`` Java System properties- Credential profiles file at the default location (````~/.aws/credentials````) used by the AWS CLI- Instance profile credentials delivered through the Amazon EC2 metadata serviceThe following folder structure will be created in S3:```s3://bucket_name/username/notebook-id/```Configure by setting environment variables in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_BUCKET=bucket_nameexport ZEPPELIN_NOTEBOOK_S3_USER=username```Or using the file **zeppelin-site.xml** uncomment and complete the S3 settings:```xml  zeppelin.notebook.s3.bucket  bucket_name  bucket name for notebook storage  zeppelin.notebook.s3.user  username  user name for s3 folder structure```Uncomment the next property for use S3NotebookRepo class:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.S3NotebookRepo  notebook persistence layer implementation```Comment out the next property to disable local git notebook storage (the default):```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo  versioned notebook persistence layer implementation```### Data Encryption in S3#### AWS KMS encryption keysTo use an [AWS KMS](https://aws.amazon.com/kms/) encryption key to encrypt notebooks, set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_KMS_KEY_ID=kms-key-id```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.kmsKeyID  AWS-KMS-Key-UUID  AWS KMS key ID used to encrypt notebook data in S3```In order to set custom KMS key region, set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_KMS_KEY_REGION=kms-key-region```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.kmsKeyRegion  target-region  AWS KMS key region in your AWS account```Format of `target-region` is described in more details [here](http://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region) in second `Region` column (e.g. `us-east-1`).#### Custom Encryption Materials Provider classYou may use a custom [``EncryptionMaterialsProvider``](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/EncryptionMaterialsProvider.html) class as long as it is available in the classpath and able to initialize itself from system properties or another mechanism.  To use this, set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_EMP=class-name```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.encryptionMaterialsProvider  provider implementation class name  Custom encryption materials provider used to encrypt notebook data in S3```   #### Enable server-side encryptionTo request server-side encryption of notebooks, set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_SSE=true```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.sse  true  Server-side encryption enabled for notebooks```### S3 Object PermissionsS3 allows writing objects into buckets owned by a different account than the requestor, when this happens S3 by default does not grant the bucket owner permissions to the written object. Setting the Canned ACL when communicating with S3 determines the permissions of notebooks saved in S3. Allowed values for Canned ACL are found [here](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/CannedAccessControlList.html), the most frequent value is &quot;BucketOwnerFullControl&quot;. Set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_CANNED_ACL=BucketOwnerFullControl```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.cannedAcl  BucketOwnerFullControl  Saves notebooks in S3 with the given Canned Access Control List.```#### S3 Enable Path Style AccessTo request path style s3 bucket access, set the following environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_S3_PATH_STYLE_ACCESS=true```Or using the following setting in **zeppelin-site.xml**:```xml  zeppelin.notebook.s3.pathStyleAccess  true  Path Style S3 bucket access enabled for notebook repo```## Notebook Storage in Azure Using `AzureNotebookRepo` you can connect your Zeppelin with your Azure account for notebook storage.First of all, input your `AccountName`, `AccountKey`, and `Share Name` in the file **zeppelin-site.xml** by commenting out and completing the next properties:```xml  zeppelin.notebook.azure.connectionString  DefaultEndpointsProtocol=https;AccountName=;AccountKey=  Azure account credentials  zeppelin.notebook.azure.share  zeppelin  share name for notebook storage```Secondly, you can initialize `AzureNotebookRepo` class in the file **zeppelin-site.xml** by commenting the next property:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo  versioned notebook persistence layer implementation```and commenting out:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.AzureNotebookRepo  notebook persistence layer implementation```In case you want to use simultaneously your local git storage with Azure storage use the following property instead: ```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo, apache.zeppelin.notebook.repo.AzureNotebookRepo  notebook persistence layer implementation```Optionally, you can specify Azure folder structure name in the file **zeppelin-site.xml** by commenting out the next property: ```xml   zeppelin.notebook.azure.user  user  optional user name for Azure folder structure```## Notebook Storage in Google Cloud Storage Using `GCSNotebookRepo` you can connect Zeppelin with Google Cloud Storage using [Application Default Credentials](https://cloud.google.com/docs/authentication/production).First, choose a GCS path under which to store notebooks.```xml  zeppelin.notebook.gcs.dir        A GCS path in the form gs://bucketname/path/to/dir.    Notes are stored at {zeppelin.notebook.gcs.dir}/{notebook-id}/note.json ```Then, initialize the `GCSNotebookRepo` class in the file **zeppelin-site.xml** by commenting the next property:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo  versioned notebook persistence layer implementation```and commenting out:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GCSNotebookRepo  notebook persistence layer implementation```Or, if you want to simultaneously use your local git storage with GCS, use the following property instead:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo,org.apache.zeppelin.notebook.repo.GCSNotebookRepo  notebook persistence layer implementation```### Google Cloud API AuthenticationNote: On Google App Engine, Google Cloud Shell, and Google Compute Engine, thesesteps are not necessary if you are using the default built in service account.For more information, see [Application Default Credentials](https://cloud.google.com/docs/authentication/production)#### Using gcloud auth application-default loginSee the [gcloud docs](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login)As the user running the zeppelin daemon, run:```bashgcloud auth application-default login```You can also use `--scopes` to restrict access to specific Google APIs, such asCloud Storage and BigQuery.#### Using service account key filesAlternatively, to use a [service account](https://cloud.google.com/compute/docs/access/service-accounts)for authentication with GCS, you will need a JSON service account key file.1. Navigate to the [service accounts page](https://console.cloud.google.com/iam-admin/serviceaccounts/project)2. Click `CREATE SERVICE ACCOUNT`3. Select at least `Storage -&gt; Storage Object Admin`. Note that this is   **different** than `Storage Admin`.4. If you are also using the BigQuery Interpreter, add the appropriate   permissions (e.g. `Bigquery -&gt; Bigquery Data Viewer and BigQuery User`)5. Name your service account, and select &quot;Furnish a new private key&quot; to download   a `.json` file. Click &quot;Create&quot;.6. Move the downloaded file to a location of your choice (e.g.   `/path/to/my/key.json`), and give it appropriate permissions. Ensure at   least the user running the zeppelin daemon can read it. If you wish to set this as your default credential file to access Google Services, point `GOOGLE_APPLICATION_CREDENTIALS` at your new key file in **zeppelin-env.sh**. For example:```bashexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/my/key.json```If you do not want to use this key file as default credential file and want to specify a custom keyfile for authentication with GCS, update the following property :```xml  zeppelin.notebook.google.credentialsJsonFilePath  path/to/key.json      Path to GCS credential key file for authentication with Google Storage. ```## Notebook Storage in OSS Notebooks may be stored in Aliyun OSS.The following folder structure will be created in OSS:```oss://bucket_name/{noteboo_dir}/note_path```And you should configure oss related properties in file **zeppelin-site.xml**.```xml  zeppelin.notebook.oss.bucket  zeppelin  bucket name for notebook storage  zeppelin.notebook.oss.endpoint  http://oss-cn-hangzhou.aliyuncs.com  endpoint for oss bucket  zeppelin.notebook.oss.accesskeyid    Access key id for your OSS account  zeppelin.notebook.oss.accesskeysecret    Access key secret for your OSS account```Uncomment the next property for use OSSNotebookRepo class:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.OSSNotebookRepo  notebook persistence layer implementation```## Notebook Storage in ZeppelinHub  ZeppelinHub storage layer allows out of the box connection of Zeppelin instance with your ZeppelinHub account. First of all, you need to either comment out the following  property in **zeppelin-site.xml**:```xml&lt;!--  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo  two notebook persistence layers (local + ZeppelinHub)--&gt;```or set the environment variable in the file **zeppelin-env.sh**:```bashexport ZEPPELIN_NOTEBOOK_STORAGE=&quot;org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo&quot;```Secondly, you need to set the environment variables in the file **zeppelin-env.sh**:```bashexport ZEPPELINHUB_API_TOKEN=ZeppelinHub tokenexport ZEPPELINHUB_API_ADDRESS=address of ZeppelinHub service (e.g. https://www.zeppelinhub.com)```You can get more information on generating `token` and using authentication on the corresponding [help page](http://help.zeppelinhub.com/zeppelin_integration/#add-a-new-zeppelin-instance-and-generate-a-token).## Notebook Storage in MongoDB Using `MongoNotebookRepo`, you can store your notebook in [MongoDB](https://www.mongodb.com/).### Why MongoDB?* **[High Availability (HA)](https://en.wikipedia.org/wiki/High_availability)** by a [replica set](https://docs.mongodb.com/manual/reference/glossary/#term-replica-set)* Seperation of storage from server### How to useYou can use MongoDB as notebook storage by editting `zeppelin-env.sh` or `zeppelin-site.xml`.#### (Method 1) by editting `zeppelin-env.sh`Add a line below to `$ZEPPELIN_HOME/conf/zeppelin-env.sh`:```bashexport ZEPPELIN_NOTEBOOK_STORAGE=org.apache.zeppelin.notebook.repo.MongoNotebookRepo```&gt; *NOTE:* The default MongoDB connection URI is `mongodb://localhost`#### (Method 2) by editting `zeppelin-site.xml`Or, **uncomment** lines below at `$ZEPPELIN_HOME/conf/zeppelin-site.xml`:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.MongoNotebookRepo  notebook persistence layer implementation```And **comment** lines below:```xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitNotebookRepo  versioned notebook persistence layer implementation```### Configurable OptionsYou can configure options below in `zeppelin-env.sh`.* `ZEPPELIN_NOTEBOOK_MONGO_URI` [MongoDB connection URI](https://docs.mongodb.com/manual/reference/connection-string/) used to connect to a MongoDB database server* `ZEPPELIN_NOTEBOOK_MONGO_DATABASE` Database name* `ZEPPELIN_NOTEBOOK_MONGO_COLLECTION` Collection name* `ZEPPELIN_NOTEBOOK_MONGO_AUTOIMPORT` If `true`, import local notes (refer to description below for details)Or, you can configure them in `zeppelin-site.xml`. Corresponding option names as follows:* `zeppelin.notebook.mongo.uri`* `zeppelin.notebook.mongo.database`* `zeppelin.notebook.mongo.collection`* `zeppelin.notebook.mongo.autoimport`#### Example configurations in `zeppelin-env.sh````shexport ZEPPELIN_NOTEBOOK_MONGO_URI=mongodb://db1.example.com:27017export ZEPPELIN_NOTEBOOK_MONGO_DATABASE=myfancyexport ZEPPELIN_NOTEBOOK_MONGO_COLLECTION=notebookexport ZEPPELIN_NOTEBOOK_MONGO_AUTOIMPORT=true```#### Import your local notes automaticallyBy setting `ZEPPELIN_NOTEBOOK_MONGO_AUTOIMPORT` as `true` (default `false`), you can import your local notes automatically when Zeppelin daemon starts up. This feature is for easy migration from local file system storage to MongoDB storage. A note with ID already existing in the collection will not be imported.## Notebook Storage in GitHubTo enable GitHub tracking, uncomment the following properties in `zeppelin-site.xml````xml  zeppelin.notebook.git.remote.url    remote Git repository URL  zeppelin.notebook.git.remote.username  token  remote Git repository username  zeppelin.notebook.git.remote.access-token    remote Git repository password  zeppelin.notebook.git.remote.origin  origin  Git repository remote```And set the `zeppelin.notebook.storage` propery to `org.apache.zeppelin.notebook.repo.GitHubNotebookRepo````xml  zeppelin.notebook.storage  org.apache.zeppelin.notebook.repo.GitHubNotebookRepo```The access token could be obtained by following the steps on this link https://github.com/settings/tokens.",
      "url": " /setup/storage/storage.html",
      "group": "setup/storage",
      "excerpt": "Apache Zeppelin has a pluggable notebook storage mechanism controlled by zeppelin.notebook.storage configuration option with multiple implementations.\""
    }
    ,




    "/usage/display_system/angular_backend.html": {
      "title": "Backend Angular API in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Backend Angular API in Apache Zeppelin## OverviewAngular display system treats output as a view template for [AngularJS](https://angularjs.org/).It compiles templates and displays them inside of Apache Zeppelin. Zeppelin provides a gateway between your interpreter and your compiled **AngularJS view** templates.Therefore, you can not only update scope variables from your interpreter but also watch them in the interpreter, which is JVM process.## Basic Usage ### Print AngularJS viewTo use angular display system, you should start with `%angular`.Since `name` is not defined, `Hello {{name}}` will display `Hello`.&gt; **Please Note:** Display system is backend independent.### Bind / Unbind VariablesThrough **ZeppelinContext**, you can bind / unbind variables to **AngularJS view**. Currently, it only works in **Spark Interpreter ( scala )**.```scala// bind my &#39;object&#39; as angular scope variable &#39;name&#39; in current notebook.z.angularBind(String name, Object object)// bind my &#39;object&#39; as angular scope variable &#39;name&#39; in all notebooks related to current interpreter.z.angularBindGlobal(String name, Object object)// unbind angular scope variable &#39;name&#39; in current notebook.z.angularUnbind(String name)// unbind angular scope variable &#39;name&#39; in all notebooks related to current interpreter.z.angularUnbindGlobal(String name)```Using the above example, let&#39;s bind `world` variable to `name`. Then you can see **AngularJs view** is immediately updated.### Watch / Unwatch VariablesThrough **ZeppelinContext**, you can watch / unwatch variables in **AngularJs view**. Currently, it only works in **Spark Interpreter ( scala )**.```scala// register for angular scope variable &#39;name&#39; (notebook)z.angularWatch(String name, (before, after) =&gt; { ... })// unregister watcher for angular variable &#39;name&#39; (notebook)z.angularUnwatch(String name)// register for angular scope variable &#39;name&#39; (global)z.angularWatchGlobal(String name, (before, after) =&gt; { ... })// unregister watcher for angular variable &#39;name&#39; (global)z.angularUnwatchGlobal(String name)```Let&#39;s make a button. When it is clicked, the value of `run` will be increased 1 by 1.`z.angularBind(&quot;run&quot;, 0)` will initialize `run` to zero. And then, it will be also applied to `run` in `z.angularWatch()`.When the button is clicked, you&#39;ll see both `run` and `numWatched` are incremented by 1.## Let&#39;s make it Simpler and more IntuitiveIn this section, we will introduce a simpler and more intuitive way of using **Angular Display System** in Zeppelin.Here are some usages.### Import```scala// In notebook scopeimport org.apache.zeppelin.display.angular.notebookscope._import AngularElem._// In paragraph scopeimport org.apache.zeppelin.display.angular.paragraphscope._import AngularElem._```### Display Element```scala// automatically convert to string and print with %angular display system directive in front..display```### Event Handler```scala// on click.onClick(() =&gt; {   my callback routine}).display// on change.onChange(() =&gt; {  my callback routine}).display// arbitrary event.onEvent(&quot;ng-click&quot;, () =&gt; {  my callback routine}).display```### Bind Model```scala// bind model.model(&quot;myModel&quot;).display// bind model with initial value.model(&quot;myModel&quot;, initialValue).display```### Interact with Model```scala// read modelAngularModel(&quot;myModel&quot;)()// update modelAngularModel(&quot;myModel&quot;, &quot;newValue&quot;)```### Example: Basic UsageUsing the above basic usages, you can apply them like below examples.#### Display Elements```scala  Hello Angular Display System.display```#### OnClick Event```scala  Click me.onClick{() =&gt;  // callback for button click}.display```#### Bind Model{% raw %}```scala  {{{{myModel}}}}.model(&quot;myModel&quot;, &quot;Initial Value&quot;).display```{% endraw %}#### Interact With Model```scala// read the valueAngularModel(&quot;myModel&quot;)()// update the valueAngularModel(&quot;myModel&quot;, &quot;New value&quot;)```### Example: String ConverterUsing below example, you can convert the lowercase string to uppercase.{% raw %}```scala// clear previously created angular object.AngularElem.disassociateval button = Convert.onClick{() =&gt;  val inputString = AngularModel(&quot;input&quot;)().toString  AngularModel(&quot;title&quot;, inputString.toUpperCase)}  {  {{{{title}}}}.model(&quot;title&quot;, &quot;Please type text to convert uppercase&quot;) }   Your text { .model(&quot;input&quot;, &quot;&quot;) }  {button}.display```{% endraw %}",
      "url": " /usage/display_system/angular_backend.html",
      "group": "usage/display_system",
      "excerpt": "Apache Zeppelin provides a gateway between your interpreter and your compiled AngularJS view templates. You can not only update scope variables from your interpreter but also watch them in the interpreter, which is JVM process."
    }
    ,



    "/usage/display_system/angular_frontend.html": {
      "title": "Frontend Angular API in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Frontend Angular API in Apache Zeppelin## Basic UsageIn addition to the [backend Angular API](./angular_backend.html) to handle Angular objects binding, Apache Zeppelin also exposes a simple AngularJS **z** object on the front-end side to expose the same capabilities.This **z** object is accessible in the Angular isolated scope for each paragraph.### Bind / Unbind VariablesThrough the **`z`**, you can bind / unbind variables to **AngularJS view**.Bind a value to an angular object and a **mandatory** target paragraph:```html%angular      Super Hero:          Bind```Unbind/remove a value from angular object and a **mandatory** target paragraph:```html%angular   UnBind```The signature for the **`z.angularBind() / z.angularUnbind()`** functions are:```javascript// Bindz.angularBind(angularObjectName, angularObjectValue, paragraphId);// Unbindz.angularUnbind(angularObjectName, angularObjectValue, paragraphId);```All the parameters are mandatory.### Run ParagraphYou can also trigger paragraph execution by calling **`z.runParagraph()`** function passing the appropriate paragraphId: ```html%angular      Paragraph Id:          Run Paragraph```## Overriding dynamic form with Angular ObjectThe front-end Angular Interaction API has been designed to offer richer form capabilities and variable binding. With the existing **Dynamic Form** system you can already create input text, select and checkbox forms but the choice is rather limited and the look &amp; feel cannot be changed.The idea is to create a custom form using plain HTML/AngularJS code and bind actions on this form to push/remove Angular variables to targeted paragraphs using this new API. Consequently if you use the **Dynamic Form** syntax in a paragraph and there is a bound Angular object having the same name as the `${formName}`, the Angular object will have higher priority and the **Dynamic Form** will not be displayed. Example:  ## Feature matrix comparisonHow does the front-end AngularJS API compares to the [backend Angular API](./angular_backend.html)? Below is a comparison matrix for both APIs:                        Actions            Front-end API            Back-end API                                Initiate binding            z.angularbind(var, initialValue, paragraphId)            z.angularBind(var, initialValue)                            Update value            same to ordinary angularjs scope variable, or z.angularbind(var, newValue, paragraphId)            z.angularBind(var, newValue)                            Watching value            same to ordinary angularjs scope variable            z.angularWatch(var, (oldVal, newVal) =&gt; ...)                            Destroy binding            z.angularUnbind(var, paragraphId)            z.angularUnbind(var)                            Executing Paragraph            z.runParagraph(paragraphId)            z.run(paragraphId)                            Executing Paragraph (Specific paragraphs in other notes) (                        z.run(noteid, paragraphId)                            Executing note                        z.runNote(noteId)                     Both APIs are pretty similar, except for value watching where it is done naturally by AngularJS internals on the front-end and by user custom watcher functions in the back-end.There is also a slight difference in term of scope. Front-end API limits the Angular object binding to a paragraph scope whereas back-end API allows you to bind an Angular object at the global or note scope. This restriction has been designed purposely to avoid Angular object leaks and scope pollution.",
      "url": " /usage/display_system/angular_frontend.html",
      "group": "usage/display_system",
      "excerpt": "In addition to the back-end API to handle Angular objects binding, Apache Zeppelin exposes a simple AngularJS z object on the front-end side to expose the same capabilities."
    }
    ,



    "/usage/display_system/basic.html": {
      "title": "Basic Display System in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Basic Display System in Apache Zeppelin## TextBy default, Apache Zeppelin prints interpreter response as a plain text using `text` display system.You can explicitly say you&#39;re using `text` display system.## HtmlWith `%html` directive, Zeppelin treats your output as HTML### Mathematical expressionsHTML display system automatically formats mathematical expression using [MathJax](https://www.mathjax.org/). You can use`( INLINE EXPRESSION )` and `$$ EXPRESSION $$` to format. For example## TableIf you have data that row separated by `n` (newline) and column separated by `t` (tab) with first row as header row, for exampleYou can simply use `%table` display system to leverage Zeppelin&#39;s built in visualization.If table contents start with `%html`, it is interpreted as an HTML.&gt; **Note :** Display system is backend independent.## NetworkWith the `%network` directive, Zeppelin treats your output as a graph. Zeppelin can leverage the Property Graph Model.### What is the Labelled Property Graph Model?A [Property Graph](https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph) is a graph that has these elements:* a set of vertices    * each vertex has a unique identifier.    * each vertex has a set of outgoing edges.    * each vertex has a set of incoming edges.    * each vertex has a collection of properties defined by a map from key to value* a set of edges    * each edge has a unique identifier.    * each edge has an outgoing tail vertex.    * each edge has an incoming head vertex.    * each edge has a label that denotes the type of relationship between its two vertices.    * each edge has a collection of properties defined by a map from key to value.A [Labelled Property Graph](https://neo4j.com/developer/graph-database/#property-graph) is a Property Graph where the nodes can be tagged with **labels** representing their different roles in the graph model### What are the APIs?The new NETWORK visualization is based on json with the following params:* &quot;nodes&quot; (mandatory): list of nodes of the graph every node can have the following params:    * &quot;id&quot; (mandatory): the id of the node (must be unique);    * &quot;label&quot;: the main Label of the node;    * &quot;labels&quot;: the list of the labels of the node;    * &quot;data&quot;: the data attached to the node;* &quot;edges&quot;: list of the edges of the graph;    * &quot;id&quot; (mandatory): the id of the edge (must be unique);    * &quot;source&quot; (mandatory): the id of source node of the edge;    * &quot;target&quot; (mandatory): the id of target node of the edge;    * &quot;label&quot;: the main type of the edge;    * &quot;data&quot;: the data attached to the edge;* &quot;labels&quot;: a map (K, V) where K is the node label and V is the color of the node;* &quot;directed&quot;: (true/false, default false) wich tells if is directed graph or not;* &quot;types&quot;: a *distinct* list of the edge types of the graphIf you click on a node or edge on the bottom of the paragraph you find a list of entity propertiesThis kind of graph can be easily *flatten* in order to support other visualization formats provided by Zeppelin.### How to use it?An example of a simple graph```scala%sparkprint(s&quot;&quot;&quot;%network {    &quot;nodes&quot;: [        {&quot;id&quot;: 1},        {&quot;id&quot;: 2},        {&quot;id&quot;: 3}    ],    &quot;edges&quot;: [{&quot;source&quot;: 1, &quot;target&quot;: 2, &quot;id&quot; : 1},{&quot;source&quot;: 2, &quot;target&quot;: 3, &quot;id&quot; : 2},{&quot;source&quot;: 1, &quot;target&quot;: 2, &quot;id&quot; : 3},{&quot;source&quot;: 1, &quot;target&quot;: 2, &quot;id&quot; : 4},{&quot;source&quot;: 2, &quot;target&quot;: 1, &quot;id&quot; : 5},{&quot;source&quot;: 2, &quot;target&quot;: 1, &quot;id&quot; : 6}]}&quot;&quot;&quot;)```that will look like:A little more complex graph:```scala%sparkprint(s&quot;&quot;&quot;%network {    &quot;nodes&quot;: [{&quot;id&quot;: 1, &quot;label&quot;: &quot;User&quot;, &quot;data&quot;: {&quot;fullName&quot;:&quot;Andrea Santurbano&quot;}},{&quot;id&quot;: 2, &quot;label&quot;: &quot;User&quot;, &quot;data&quot;: {&quot;fullName&quot;:&quot;Lee Moon Soo&quot;}},{&quot;id&quot;: 3, &quot;label&quot;: &quot;Project&quot;, &quot;data&quot;: {&quot;name&quot;:&quot;Zeppelin&quot;}}],    &quot;edges&quot;: [{&quot;source&quot;: 2, &quot;target&quot;: 1, &quot;id&quot; : 1, &quot;label&quot;: &quot;HELPS&quot;},{&quot;source&quot;: 2, &quot;target&quot;: 3, &quot;id&quot; : 2, &quot;label&quot;: &quot;CREATE&quot;},{&quot;source&quot;: 1, &quot;target&quot;: 3, &quot;id&quot; : 3, &quot;label&quot;: &quot;CONTRIBUTE_TO&quot;, &quot;data&quot;: {&quot;oldPR&quot;: &quot;https://github.com/apache/zeppelin/pull/1582&quot;}}],&quot;labels&quot;: {&quot;User&quot;: &quot;#8BC34A&quot;, &quot;Project&quot;: &quot;#3071A9&quot;},&quot;directed&quot;: true,&quot;types&quot;: [&quot;HELPS&quot;, &quot;CREATE&quot;, &quot;CONTRIBUTE_TO&quot;]}&quot;&quot;&quot;)```that will look like:",
      "url": " /usage/display_system/basic.html",
      "group": "usage/display_system",
      "excerpt": "There are 3 basic display systems in Apache Zeppelin. By default, Zeppelin prints interpreter responce as a plain text using text display system. With %html directive, Zeppelin treats your output as HTML. You can also simply use %table display system..."
    }
    ,



    "/usage/dynamic_form/intro.html": {
      "title": "Dynamic Form in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# What is Dynamic Form?Apache Zeppelin dynamically creates input forms. Depending on language backend, there&#39;re two different ways to create dynamic form.Custom language backend can select which type of form creation it wants to use. Forms can have different scope (paragraph or note). Forms with scope &quot;note&quot; available in all paragraphs regardless of which paragraph has code to create these forms.## Using form Templates (scope: paragraph)This mode creates form using simple template language. It&#39;s simple and easy to use. For example Markdown, Shell, Spark SQL language backend uses it.### Text input formTo create text input form, use `${formName}` templates.for exampleAlso you can provide default value, using `${formName=defaultValue}`.### Password formTo create password form, use `${password:formName}` templates.for example### Select formTo create select form, use `${formName=defaultValue,option1|option2...}`for exampleAlso you can separate option&#39;s display name and value, using `${formName=defaultValue,option1(DisplayName)|option2(DisplayName)...}`The paragraph will be automatically run after you change your selection by default.But in case you have multiple types dynamic form in one paragraph, you might want to run the paragraph after changing all the selections.You can control this by unchecking the below **Run on selection change** option in the setting menu.Even if you uncheck this option, still you can run it by pressing `Enter`.### Checkbox formFor multi-selection, you can create a checkbox form using `${checkbox:formName=defaultValue1|defaultValue2...,option1|option2...}`. The variable will be substituted by a comma-separated string based on the selected items. For example:You can specify the delimiter using `${checkbox(delimiter):formName=...}`:Like [select form](#select-form), the paragraph will be automatically run after you change your selection by default.But in case you have multiple types dynamic form in one paragraph, you might want to run the paragraph after changing all the selections.You can control this by unchecking the below **Run on selection change** option in the setting menu.Even if you uncheck this option, still you can run it by pressing `Enter`.## Using form Templates (scope: note)Has a same syntax but starts with two symbols `$`. (for ex. input `$${forName}`)## Creates Programmatically (scope: paragraph)Some language backends can programmatically create forms. For example [ZeppelinContext](../../interpreter/spark.html#zeppelincontext) provides a form creation APIHere are some examples:### Text input form    {% highlight scala %}%sparkprintln(&quot;Hello &quot;+z.textbox(&quot;name&quot;)){% endhighlight %}        {% highlight python %}%pysparkprint(&quot;Hello &quot;+z.textbox(&quot;name&quot;)){% endhighlight %}    Use `z.input()` instead in version 0.7.3 or prior. `z.input()` is deprecated in 0.8.0.### Text input form with default value    {% highlight scala %}%sparkprintln(&quot;Hello &quot;+z.textbox(&quot;name&quot;, &quot;sun&quot;)) {% endhighlight %}        {% highlight python %}%pysparkprint(&quot;Hello &quot;+z.textbox(&quot;name&quot;, &quot;sun&quot;)){% endhighlight %}    Use `z.input()` instead in version 0.7.3 or prior. `z.input()` is deprecated in 0.8.0.### Password form    {% highlight scala %}%sparkprint(&quot;Password is &quot;+ z.password(&quot;my_password&quot;)){% endhighlight %}        {% highlight python %}%pysparkprint(&quot;Password is &quot;+ z.password(&quot;my_password&quot;)){% endhighlight %}    ### Select form    {% highlight scala %}%sparkprintln(&quot;Hello &quot;+z.select(&quot;day&quot;, Seq((&quot;1&quot;,&quot;mon&quot;),                                    (&quot;2&quot;,&quot;tue&quot;),                                    (&quot;3&quot;,&quot;wed&quot;),                                    (&quot;4&quot;,&quot;thurs&quot;),                                    (&quot;5&quot;,&quot;fri&quot;),                                    (&quot;6&quot;,&quot;sat&quot;),                                    (&quot;7&quot;,&quot;sun&quot;)))){% endhighlight %}        {% highlight python %}%pysparkprint(&quot;Hello &quot;+z.select(&quot;day&quot;, [(&quot;1&quot;,&quot;mon&quot;),                                (&quot;2&quot;,&quot;tue&quot;),                                (&quot;3&quot;,&quot;wed&quot;),                                (&quot;4&quot;,&quot;thurs&quot;),                                (&quot;5&quot;,&quot;fri&quot;),                                (&quot;6&quot;,&quot;sat&quot;),                                (&quot;7&quot;,&quot;sun&quot;)])){% endhighlight %}    #### Checkbox form    {% highlight scala %}%sparkval options = Seq((&quot;apple&quot;,&quot;Apple&quot;), (&quot;banana&quot;,&quot;Banana&quot;), (&quot;orange&quot;,&quot;Orange&quot;))println(&quot;Hello &quot;+z.checkbox(&quot;fruit&quot;, options).mkString(&quot; and &quot;)){% endhighlight %}        {% highlight python %}%pysparkoptions = [(&quot;apple&quot;,&quot;Apple&quot;), (&quot;banana&quot;,&quot;Banana&quot;), (&quot;orange&quot;,&quot;Orange&quot;)]print(&quot;Hello &quot;+ &quot; and &quot;.join(z.checkbox(&quot;fruit&quot;, options, [&quot;apple&quot;]))){% endhighlight %}    ## Creates Programmatically (scope: note)The difference in the method names:      Scope paragraph    Scope note        input (or textbox)    noteTextbox        select    noteSelect        checkbox    noteCheckbox  ",
      "url": " /usage/dynamic_form/intro.html",
      "group": "usage/dynamic_form",
      "excerpt": "Apache Zeppelin dynamically creates input forms. Depending on language backend, there're two different ways to create dynamic form."
    }
    ,



    "/usage/interpreter/dependency_management.html": {
      "title": "Dependency Management for Interpreter",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}## Dependency Management for InterpreterYou can include external libraries to interpreter by setting dependencies in interpreter menu.To be noticed, this approach doesn&#39;t work for spark and flink interpreters. They have their own dependency management, please refer their doc for details.When your code requires external library, instead of doing download/copy/restart Zeppelin, you can easily do following jobs in this menu. * Load libraries recursively from Maven repository * Load libraries from local filesystem * Add additional maven repository                          Load Dependencies to Interpreter                Click &#39;Interpreter&#39; menu in navigation bar.        Click &#39;edit&#39; button of the interpreter which you want to load dependencies to.        Fill artifact and exclude field to your needs.           You can enter not only groupId:artifactId:version but also local file in artifact field.        Press &#39;Save&#39; to restart the interpreter with loaded libraries.                                              Add repository for dependency resolving                Press  icon in &#39;Interpreter&#39; menu on the top right side.           It will show you available repository lists.       If you need to resolve dependencies from other than central maven repository or     local ~/.m2 repository, hit  icon next to repository lists.        Fill out the form and click &#39;Add&#39; button, then you will be able to see that new repository is added.        Optionally, if you are behind a corporate firewall, you can specify also all proxy settings so that Zeppelin can download the dependencies using the given credentials      ",
      "url": " /usage/interpreter/dependency_management.html",
      "group": "usage/interpreter",
      "excerpt": "Include external libraries to Apache Spark Interpreter by setting dependencies in interpreter menu."
    }
    ,



    "/usage/interpreter/dynamic_loading.html": {
      "title": "Dynamic Interpreter Loading using REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Dynamic Interpreter Loading using REST APIApache Zeppelin provides pluggable interpreter architecture which results in a wide and variety of the supported backend system. In this section, we will introduce **Dynamic interpreter loading** using **REST API**. This concept actually comes from [Zeppelin Helium Proposal](https://cwiki.apache.org/confluence/display/ZEPPELIN/Helium+proposal).Before we start, if you are not familiar with the concept of **Zeppelin interpreter**, you can check out [Overview: Zeppelin Interpreter](./overview.html) first.## Overview In order to simplify using 3rd party interpreters, we **dynamically** load interpreters from **Maven Repository** using **REST API**. Hopefully, the picture below will help you to understand the process. ## Load &amp; Unload Interpreters Using REST API### Load You can **load** interpreters located in Maven repository using REST API, like this:(Maybe, you are unfamiliar with `[interpreter_group_name]` or `[interpreter_name]`. If so, please checkout [Overview: Zeppelin Interpreter](./overview.html) again.)```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/load/[interpreter_group_name]/[interpreter_name]```The Restful method will be **POST**. And the parameters you need are:  1.  **Artifact:** Maven artifact ( groupId:artifactId:version )   2.  **Class Name:** Package name + Interpreter class name  3. **Repository ( optional ):** Additional maven repository addressFor example, if you want to load `markdown` interpreter to your Zeppelin, the parameters and URL you need may look like:```http://127.0.0.1:8080/api/interpreter/load/md/markdown``````json{  &quot;artifact&quot;: &quot;org.apache.zeppelin:zeppelin-markdown:0.6.0-SNAPSHOT&quot;,  &quot;className&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,  &quot;repository&quot;: {    &quot;url&quot;: &quot;http://dl.bintray.com/spark-packages/maven&quot;,    &quot;snapshot&quot;: false  }}```The meaning of each parameters is:   1. **Artifact**  - groupId: org.apache.zeppelin  - artifactId: zeppelin-markdown  - version: 0.6.0-SNAPSHOT  2. **Class Name**  - Package Name: org.apache.zeppelin  - Interpreter Class Name: markdown.Markdown  3. **Repository ( optional )**  - Url: http://dl.bintray.com/spark-packages/maven  - Snapshot: false&gt; Please note: The interpreters you downloaded need to be **reload**, when your Zeppelin server is down. ### UnloadIf you want to **unload** the interpreters using REST API, ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/unload/[interpreter_group_name]/[interpreter_name]```In this case, the Restful method will be **DELETE**.## What is the next step after Loading ? ### Q1. Where is the location of interpreters you downloaded ?  Actually, the answer about this question is in the above picture. Once the REST API is called, the `.jar` files of interpreters you get are saved under `ZEPPELIN_HOME/local-repo` first. Then, they will be copied to `ZEPPELIN_HOME/interpreter` directory. So, please checkout your `ZEPPELIN_HOME/interpreter`.### Q2. Then, how can I use this interpreter ?After loading an interpreter, you can use it by creating and configuring it in Zeppelin&#39;s **Interpreter tab**.Oh, you don&#39;t need to restart your Zeppelin server. Because it is **Dynamic Loading**, you can configure and load it **at runtime** !1. After Zeppelin server up, browse Zeppelin home and click **Interpreter tab**.2. At the **Interpreter** section, click **+Create** button.  3. Then, you can verify the interpreter list that you loaded.4. After choosing an interpreter, you can configure and use it. Don&#39;t forget to save it.5. Create a new notebook in the **Notebook** section, then you can bind the interpreters from your interpreter list. Just drag and drop !6. At last, you can use your interpreter !If you want to get the specific information about respective interpreters, please checkout each interpreter documentation. ",
      "url": " /usage/interpreter/dynamic_loading.html",
      "group": "usage/interpreter",
      "excerpt": "Apache Zeppelin provides pluggable interpreter architecture which results in a wide and variety of the supported backend system. In this page, we will introduce dynamic interpreter loading using REST API."
    }
    ,



    "/usage/interpreter/execution_hooks.html": {
      "title": "Interpreter Execution Hooks (Experimental)",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Interpreter Execution Hooks (Experimental)## OverviewApache Zeppelin allows for users to specify additional code to be executed by an interpreter at pre and post-paragraph code execution.This is primarily useful if you need to run the same set of code for all of the paragraphs within your notebook at specific times.Currently, this feature is only available for the spark and pyspark interpreters.To specify your hook code, you may use `z.registerHook()`. For example, enter the following into one paragraph:```python%pysparkz.registerHook(&quot;post_exec&quot;, &quot;print &#39;This code should be executed before the parapgraph code!&#39;&quot;)z.registerHook(&quot;pre_exec&quot;, &quot;print &#39;This code should be executed after the paragraph code!&#39;&quot;)```These calls will not take into effect until the next time you run a paragraph. In another paragraph, enter```python%pysparkprint &quot;This code should be entered into the paragraph by the user!&quot;```The output should be:```This code should be executed before the paragraph code!This code should be entered into the paragraph by the user!This code should be executed after the paragraph code!```If you ever need to know the hook code, use `z.getHook()`:```python%pysparkprint z.getHook(&quot;post_exec&quot;)print &#39;This code should be executed after the paragraph code!&#39;```Any call to `z.registerHook()` will automatically overwrite what was previously registered.To completely unregister a hook event, use `z.unregisterHook(eventCode)`.Currently only `&quot;post_exec&quot;` and `&quot;pre_exec&quot;` are valid event codes for the Zeppelin Hook Registry system.Finally, the hook registry is internally shared by other interpreters in the same group.This would allow for hook code for one interpreter REPL to be set by another as follows:```scala%sparkz.unregisterHook(&quot;post_exec&quot;, &quot;pyspark&quot;)```The API is identical for both the spark (scala) and pyspark (python) implementations.### CaveatsCalls to `z.registerHook(&quot;pre_exec&quot;, ...)` should be made with care. If there are errors in your specified hook code, this will cause the interpreter REPL to become unable to execute any code pass the pre-execute stage making it impossible for direct calls to `z.unregisterHook()` to take into effect. Current workarounds include calling `z.unregisterHook()` from a different interpreter REPL in the same interpreter group (see above) or manually restarting the interpreter group in the UI. ",
      "url": " /usage/interpreter/execution_hooks.html",
      "group": "manual",
      "excerpt": "Apache Zeppelin allows for users to specify additional code to be executed by an interpreter at pre and post-paragraph code execution."
    }
    ,



    "/usage/interpreter/installation.html": {
      "title": "Installing Interpreters",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Installing Interpreters Apache Zeppelin provides **Interpreter Installation** mechanism for whom downloaded Zeppelin `netinst` binary package, or just want to install another 3rd party interpreters.## Community managed interpretersApache Zeppelin provides several interpreters as [community managed interpreters](#available-community-managed-interpreters).If you downloaded `netinst` binary package, you need to install by using below commands.#### Install all community managed interpreters```bash./bin/install-interpreter.sh --all```#### Install specific interpreters```bash./bin/install-interpreter.sh --name md,shell,jdbc,python```You can get full list of community managed interpreters by running```bash./bin/install-interpreter.sh --list```#### Install interpreter built with Scala 2.10Zeppelin support both Scala 2.10 and 2.11 for several interpreters as below:      Name    Maven Artifact for Scala 2.10    Maven Artifact for Scala 2.11        spark    org.apache.zeppelin:zeppelin-spark_2.10:0.9.0    org.apache.zeppelin:zeppelin-spark_2.11:0.9.0        scalding    org.apache.zeppelin:zeppelin-scalding_2.10:0.9.0    org.apache.zeppelin:zeppelin-scalding_2.11:0.9.0  If you install one of these interpreters only with `--name` option, installer will download interpreter built with Scala 2.11 by default. If you want to specify Scala version, you will need to add `--artifact` option. Here is the example of installing flink interpreter built with Scala 2.10.```bash./bin/install-interpreter.sh --name flink --artifact org.apache.zeppelin:zeppelin-scalding_2.10:0.9.0```#### Install Spark interpreter built with Scala 2.10Spark distribution package has been built with Scala 2.10 until 1.6.2. If you have `SPARK_HOME` set pointing to Spark version earlier than 2.0.0, you need to download Spark interpreter packaged with Scala 2.10. To do so, use follow command:```bashrm -rf ./interpreter/spark./bin/install-interpreter.sh --name spark --artifact org.apache.zeppelin:zeppelin-spark_2.10:0.9.0```Once you have installed interpreters, you need to restart Zeppelin. And then [create interpreter setting](./overview.html#what-is-zeppelin-interpreter) and [bind it with your notebook](./overview.html#what-is-zeppelin-interpreter-setting).## 3rd party interpretersYou can also install 3rd party interpreters located in the maven repository by using below commands.#### Install 3rd party interpreters```bash./bin/install-interpreter.sh --name interpreter1 --artifact groupId1:artifact1:version1```The above command will download maven artifact `groupId1:artifact1:version1` and all of its transitive dependencies into `interpreter/interpreter1` directory.After restart Zeppelin, then [create interpreter setting](./overview.html#what-is-zeppelin-interpreter) and [bind it with your note](./overview.html#what-is-interpreter-setting).#### Install multiple 3rd party interpreters at once```bash./bin/install-interpreter.sh --name interpreter1,interpreter2 --artifact groupId1:artifact1:version1,groupId2:artifact2:version2````--name` and `--artifact` arguments will recieve comma separated list.## Available community managed interpretersYou can also find the below community managed interpreter list in `conf/interpreter-list` file.      Name    Maven Artifact    Description        alluxio    org.apache.zeppelin:zeppelin-alluxio:0.9.0    Alluxio interpreter        angular    org.apache.zeppelin:zeppelin-angular:0.9.0    HTML and AngularJS view rendering        beam    org.apache.zeppelin:zeppelin-beam:0.9.0    Beam interpreter        bigquery    org.apache.zeppelin:zeppelin-bigquery:0.9.0    BigQuery interpreter        cassandra    org.apache.zeppelin:zeppelin-cassandra:0.9.0    Cassandra interpreter        elasticsearch    org.apache.zeppelin:zeppelin-elasticsearch:0.9.0    Elasticsearch interpreter        file    org.apache.zeppelin:zeppelin-file:0.9.0    HDFS file interpreter        flink    org.apache.zeppelin:zeppelin-flink:0.9.0    Flink interpreter        hbase    org.apache.zeppelin:zeppelin-hbase:0.9.0    Hbase interpreter        geode    org.apache.zeppelin:zeppelin-geode:0.9.0    Apache Geode interpreter        groovy    org.apache.zeppelin:zeppelin-groovy:0.9.0    Groovy interpreter        ignite    org.apache.zeppelin:zeppelin-ignite:0.9.0    Ignite interpreter        java    org.apache.zeppelin:zeppelin-java:0.9.0    Java interpreter        jdbc    org.apache.zeppelin:zeppelin-jdbc:0.9.0    Jdbc interpreter        kotlin    org.apache.zeppelin:zeppelin-kotlin:0.7.0    Kotlin interpreter        kylin    org.apache.zeppelin:zeppelin-kylin:0.9.0    Kylin interpreter        lens    org.apache.zeppelin:zeppelin-lens:0.9.0    Lens interpreter        livy    org.apache.zeppelin:zeppelin-livy:0.9.0    Livy interpreter        md    org.apache.zeppelin:zeppelin-markdown:0.9.0    Markdown support        neo4j    org.apache.zeppelin:zeppelin-neo4j:0.9.0    Neo4j interpreter        pig    org.apache.zeppelin:zeppelin-pig:0.9.0    Pig interpreter        python    org.apache.zeppelin:zeppelin-python:0.9.0    Python interpreter        sap    org.apache.zeppelin:zeppelin-sap:0.9.0    SAP support        scalding    org.apache.zeppelin:zeppelin-scalding_2.0.10:0.9.0    Scalding interpreter        scio    org.apache.zeppelin:zeppelin-scio:0.9.0    Scio interpreter        shell    org.apache.zeppelin:zeppelin-shell:0.9.0    Shell command        sparql    org.apache.zeppelin:zeppelin-sparql:0.9.0    Sparql interpreter        submarine    org.apache.zeppelin:zeppelin-submarine:0.9.0    Submarine interpreter  ",
      "url": " /usage/interpreter/installation.html",
      "group": "usage/interpreter",
      "excerpt": "Apache Zeppelin provides Interpreter Installation mechanism for whom downloaded Zeppelin netinst binary package, or just want to install another 3rd party interpreters."
    }
    ,



    "/usage/interpreter/interpreter_binding_mode.html": {
      "title": "Interpreter Binding Mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Interpreter Binding Mode ## OverviewInterpreter Process is a JVM process that communicates with Zeppelin daemon using thrift. Each interpreter process has a single interpreter group, and this interpreter group can have one or more instances of an interpreter.(See [here](../../development/writing_zeppelin_interpreter.html) to understand more about its internal structure.) Zeppelin provides 3 different modes to run interpreter process: **shared**, **scoped** and **isolated**.   Also, the user can specify the scope of these modes: **per user** or **per note**.These 3 modes give flexibility to fit Zeppelin into any type of use cases.In this documentation, we mainly discuss the **per note** scope in combination with the **shared**, **scoped** and **isolated** modes.## Shared Mode    In **Shared** mode, single JVM process and a single session serves all notes. As a result, `note A` can access variables (e.g python, scala, ..) directly created from other notes.. ## Scoped Mode    In **Scoped** mode, Zeppelin still runs a single interpreter JVM process but, in the case of **per note** scope, each note runs in its own dedicated session.(Note it is still possible to share objects between these notes via [ResourcePool](../../interpreter/spark.html#object-exchange)) ## Isolated Mode    **Isolated** mode runs a separate interpreter process for each note in the case of **per note** scope. So, each note has an absolutely isolated session. (But it is still possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)) ## Which mode should I use?Mode | Each notebook...| Benefits | Disadvantages | Sharing objects--- | --- | --- | --- | ---**shared** |  Shares a single session in a single interpreter process (JVM) |  Low resource utilization and it&#39;s easy to share data between notebooks |  All notebooks are affected if the interpreter process dies | Can share directly**scoped** | Has its own session in the same interpreter process (JVM) | Less resource utilization than isolated mode |  All notebooks are affected if the interpreter process dies | Can&#39;t share directly, but it&#39;s possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)**isolated** | Has its own Interpreter Process | One notebook is not affected directly by other notebooks (**per note**) | Can&#39;t share data between notebooks easily (**per note**) | Can&#39;t share directly, but it&#39;s possible to share objects via [ResourcePool](../../interpreter/spark.html#object-exchange)In the case of the **per user** scope (available in a multi-user environment), Zeppelin manages interpreter sessions on a per user basis rather than a per note basis. For example: - In **scoped + per user** mode, `User A`&#39;s notes **might** be affected by `User B`&#39;s notes. (e.g JVM dies, ...) Because all notes are running on the same JVM- On the other hand, **isolated + per user** mode, `User A`&#39;s notes will not be affected by others&#39; notes which running on separated JVMsEach Interpreter implementation may have different characteristics depending on the back end system that they integrate. And 3 interpreter modes can be used differently.Let’s take a look how Spark Interpreter implementation uses these 3 interpreter modes with **per note** scope, as an example.Spark Interpreter implementation includes 4 different interpreters in the group: Spark, SparkSQL, Pyspark and SparkR. SparkInterpreter instance embeds Scala REPL for interactive Spark API execution.    In **Shared** mode, a SparkContext and a Scala REPL is being shared among all interpreters in the group.So every note will be sharing single SparkContext and single Scala REPL. In this mode, if `Note A` defines variable ‘a’ then `Note B` not only able to read variable ‘a’ but also able to override the variable.    In **Scoped** mode, each note has its own Scala REPL.So variable defined in a note can not be read or overridden in another note. However, a single SparkContext still serves all the sessions.And all the jobs are submitted to this SparkContext and the fair scheduler schedules the jobs.This could be useful when user does not want to share Scala session, but want to keep single Spark application and leverage its fair scheduler.In **Isolated** mode, each note has its own SparkContext and Scala REPL.    ",
      "url": " /usage/interpreter/interpreter_binding_mode.html",
      "group": "usage/interpreter",
      "excerpt": ""
    }
    ,



    "/usage/interpreter/overview.html": {
      "title": "Interpreter in Apache Zeppelin",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Interpreter in Apache Zeppelin## OverviewIn this section, we will explain the role of interpreters, interpreter groups and interpreter settings in Zeppelin.The concept of Zeppelin interpreters allows any language or data-processing backend to be plugged into Zeppelin.Currently, Zeppelin supports many interpreters such as Scala (with Apache Spark), Python (with Apache Spark), Spark SQL, Hive, JDBC, Markdown, Shell and so on.## What are Zeppelin Interpreters ?A Zeppelin interpreter is a plug-in which enables Zeppelin users to use a specific language/data-processing-backend. For example, to use Scala code in Zeppelin, you would use the `%spark` interpreter.When you click the ```+Create``` button on the interpreter page, the interpreter drop-down list box will show all the available interpreters on your server.You can create multiple interpreters for the same engine with different interpreter setting. e.g. You can create `spark2` for Spark 2.x and create `spark1` for Spark 1.x.For each paragraph you write in Zeppelin, you need to specify its interpreter first via `%interpreter_group.interpreter_name`. e.g. `%spark.pyspark`, `%spark.r`.If you specify interpreter, you can also pass local properties to it (if it needs them).  This is done by providing a set of key/value pairs, separated by comma, inside the round brackets right after the interpreter name. If key or value contain characters like `=`, or `,`, then you can either escape them with `` character, or wrap the whole value inside the double quotes For example:```%cassandra(outputFormat=cql, dateFormat=&quot;E, d MMM yy&quot;, timeFormat=E, d MMM yy)```## What are the Interpreter Settings?The interpreter settings are the configuration of a given interpreter on the Zeppelin server. For example, certain properties need to be set for the Apache Hive JDBC interpreter to connect to the Hive server.Properties are exported as environment variables on the system if the property name consists of upper-case characters, numbers or underscores ([A-Z_0-9]). Otherwise, the property is set as a common interpreter property. e.g. You can define `SPARK_HOME` and `HADOOP_CONF_DIR` in spark&#39;s interpreter setting, they are be passed to Spark interpreter process as environment variable which is used by Spark. You may use parameters from the context of the interpreter by adding #{contextParameterName} in the interpreter property value. The parameter can be of the following types: string, number, boolean.### Context Parameters      Name    Type        user    string        noteId    string        replName    string        className    string  If the context parameter is null, then it is replaced by an empty string. The following screenshot is one example where we make the user name as the property value of `default.user`.## What are Interpreter Groups ?Every interpreter belongs to an **Interpreter Group**. Interpreter Groups are units of interpreters that run in one single JVM process and can be started/stopped together.By default, every interpreter belongs to a separate group, but the group might contain more interpreters. For example, the Spark interpreter group includes Scala Spark, PySpark, IPySpark, SparkR and Spark SQL.Technically, Zeppelin interpreters from the same group run within the same JVM. For more information about this, please consult [the documentation on writing interpreters](../development/writing_zeppelin_interpreter.html).Each interpreter belongs to a single group and is registered together. All relevant properties are listed in the interpreter setting as in the below example.## Interpreter Binding ModeIn the Interpreter Settings, one can choose one of the `shared`, `scoped`, or `isolated` interpreter binding modes.In `shared` mode, every note/user using this interpreter will share a single interpreter instance. `scoped` and `isolated` mode can be used under 2 dimensions: `per user` or `per note`.e.g. In `scoped per note` mode, each note will create a new interpreter instance in the same interpreter process. In `isolated per note` mode, each note will create a new interpreter process.For more information, please consult [Interpreter Binding Mode](./interpreter_binding_mode.html). ## Interpreter Lifecycle ManagementBefore 0.8.0, Zeppelin doesn&#39;t have lifecycle management for interpreters. Users had to shut down interpreters explicitly via the UI. Starting from 0.8.0, Zeppelin provides a new interface`LifecycleManager` to control the lifecycle of interpreters. For now, there are two implementations: `NullLifecycleManager` and `TimeoutLifecycleManager`. `NullLifecycleManager` will do nothing, i.e., the user needs to control the lifecycle of interpreter by themselves as before. `TimeoutLifecycleManager` will shut down interpreters after an interpreter remains idle for a while. By default, the idle threshold is 1 hour.Users can change this threshold via the `zeppelin.interpreter.lifecyclemanager.timeout.threshold` setting. `NullLifecycleManager` is the default lifecycle manager, and users can change it via `zeppelin.interpreter.lifecyclemanager.class`.## Inline Generic ConfigurationZeppelin&#39;s interpreter setting is shared by all users and notes, if you want to have different settings, you have to create a new interpreter, e.g. you can create `spark_jar1` for running Spark with dependency `jar1` and `spark_jar2` for running Spark with dependency `jar2`.This approach works, but is not convenient. Inline generic configuration can provide more fine-grained control on interpreter settings and more flexibility. `ConfInterpreter` is a generic interpreter that can be used by any interpreter. You can use it just like defining a java property file.It can be used to make custom settings for any interpreter. However, `ConfInterpreter` needs to run before that interpreter process is launched. When that interpreter process is launched is determined by the interpreter binding mode setting.So users need to understand the [interpreter binding mode setting](../usage/interpreter/interpreter_bindings_mode.html) of Zeppelin and be aware of when the interpreter process is launched. E.g., if we set the Spark interpreter setting as isolated per note, then under this setting, each note will launch one interpreter process. In this scenario, users need to put `ConfInterpreter` as the first paragraph as in the below example. Otherwise, the customized setting cannot be applied (actually it would report `ERROR`).## PrecodeSnippet of code (language of interpreter) that executes after initialization of the interpreter depends on [Binding mode](#interpreter-binding-mode). To configure, add a parameter with the class of the interpreter (`zeppelin..precode`) except JDBCInterpreter ([JDBC precode](../../interpreter/jdbc.html#usage-precode)). ## Credential InjectionCredentials from the credential manager can be injected into Notebooks. Credential injection works by replacing the following patterns in Notebooks with matching credentials for the Credential Manager: `{CREDENTIAL_ENTITY.user}` and `{CREDENTIAL_ENTITY.password}`. However, credential injection must be enabled per Interpreter, by adding a boolean `injectCredentials` setting in the Interpreters configuration. Injected passwords are removed from Notebook output to prevent accidentally leaking passwords.**Credential Injection Setting****Credential Entry Example****Credential Injection Example**```scalaval password = &quot;{SOME_CREDENTIAL_ENTITY.password}&quot;val username = &quot;{SOME_CREDENTIAL_ENTITY.user}&quot;```## Interpreter Process Recovery (Experimental)Before 0.8.0, shutting down Zeppelin also meant to shutdown all the running interpreter processes. Usually, an administrator will shutdown the Zeppelin server for maintenance or upgrades, but would not want to shut down the running interpreter processes.In such cases, interpreter process recovery is necessary. Starting from 0.8.0, users can enable interpreter process recovery via the setting `zeppelin.recovery.storage.class` as `org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorage` or other implementations if available in the future. By default it is `org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage`, which means recovery is not enabled. `zeppelin.recovery.dir` is used for specify where to store the recovery metadata. Enabling recovery means shutting down Zeppelin would not terminate interpreter processes, and when Zeppelin is restarted, it would try to reconnect to the existing running interpreter processes. If you want to kill all the interpreter processes after terminating Zeppelin even when recovery is enabled, you can run `bin/stop-interpreter.sh`. In 0.8.x, Zeppelin server would reconnect to the running interpreter process only when you run paragraph again, but it won&#39;t recover the running paragraph. E.g. if you restart zeppelin server when some paragraph is still running,then when you restart Zeppelin, although the interpreter process is still running, you won&#39;t see the paragraph is running in frontend. In 0.9.x, we fix it by recovering the running paragraphs.Here&#39;s one screenshot of how one running paragraph of flink interpreter works.## Choose InterpretersBy default, Zeppelin will register and display all the interpreters under folder `$ZEPPELIN_HOME/interpreters`.But you can configure property `zeppelin.interpreter.include` to specify what interpreters you want to include or `zeppelin.interpreter.exclude` to specify what interpreters you want to exclude.Only one of them can be specified, you can not specify them together.",
      "url": " /usage/interpreter/overview.html",
      "group": "usage/interpreter",
      "excerpt": "This document explains the role of interpreters, interpreter groups and interpreter settings in Apache Zeppelin. The concept of Zeppelin interpreters allows any language or data-processing backend to be plugged into Zeppelin."
    }
    ,



    "/usage/interpreter/user_impersonation.html": {
      "title": "Impersonation",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# ImpersonationUser impersonation enables to run zeppelin interpreter process as a web frontend user## Setup### Linux User#### 1. Enable Shiro auth in `conf/shiro.ini````[users]user1 = password1, role1user2 = password2, role2```#### 2. Enable password-less ssh for the user you want to impersonate (say user1).```bashadduser user1#ssh-keygen (optional if you don&#39;t already have generated ssh-key.ssh user1@localhost mkdir -p .sshcat ~/.ssh/id_rsa.pub | ssh user1@localhost &#39;cat &gt;&gt; .ssh/authorized_keys&#39;```Alternatively instead of password-less, user can override ZEPPELIN_IMPERSONATE_CMD in zeppelin-env.sh```bashexport ZEPPELIN_IMPERSONATE_CMD=&#39;sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c &#39;```#### 4. Restart zeppelin server.```bash# for OSX, linuxbin/zeppelin-daemon restart# for windowsbinzeppelin.cmd```#### 5. Configure impersonation for interpreter                        Go to interpreter setting page, and enable &quot;User Impersonate&quot; in any of the interpreter (in my example its shell interpreter)#### 6. Test with a simple paragraph```bash%shwhoami```Note that usage of &quot;User Impersonate&quot; option will enable Spark interpreter to use `--proxy-user` option with current user by default. If you want to disable `--proxy-user` option, then refer to `ZEPPELIN_IMPERSONATE_SPARK_PROXY_USER` variable in `conf/zeppelin-env.sh`### LDAP User with kerberized HDFS#### 1. Set the user(zeppelin) to be enable to set proxyuser in `core-site.xml````bash  hadoop.proxyuser.zeppelin.groups  *  hadoop.proxyuser.zeppelin.users  *  hadoop.proxyuser.zeppelin.hosts  *```#### 2. Set the group to be enable to connect Hive metastore in &#39;core-site.xml&#39;```bash  hadoop.proxyuser.hive.groups  zeppelin```#### 3. Enable Kerberos setting in `zeppelin-site.xml````bash  zeppelin.server.kerberos.keytab  zeppelin.keytab  zeppelin.server.kerberos.principal  zeppelin@principal```#### 4. Restart zeppelin server.```bash# for OSX, linuxbin/zeppelin-daemon restart# for windowsbinzeppelin.cmd```#### 5. Configure impersonation for interpreterOptionThe interpreter will be instantiated *Per User* in *isolated* process*User impersonate*",
      "url": " /usage/interpreter/user_impersonation.html",
      "group": "usage/interpreter",
      "excerpt": "Set up zeppelin interpreter process as web front end user."
    }
    ,



    "/usage/other_features/cron_scheduler.html": {
      "title": "Running a Notebook on a Given Schedule Automatically",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Running a Notebook on a Given Schedule AutomaticallyApache Zeppelin provides a cron scheduler for each notebook. You can run a notebook on a given schedule automatically by setting up a cron scheduler on the notebook.## Setting up a cron scheduler on a notebookClick the clock icon on the tool bar and open a cron scheduler dialog box.There are the following items which you can input or set:### PresetYou can set a cron schedule easily by clicking each option such as `1m` and `5m`. The login user is set as a cron executing user automatically. You can also clear the cron schedule settings by clicking `None`.### Cron expressionYou can set the cron schedule by filling in this form. Please see [Cron Trigger Tutorial](http://www.quartz-scheduler.org/documentation/quartz-2.2.x/tutorials/crontrigger) for the available cron syntax.### Cron executing user (It is removed from 0.8 where it enforces the cron execution user to be the note owner for security purpose)You can set the cron executing user by filling in this form and press the enter key.### After execution stop the interpreterWhen this checkbox is set to &quot;on&quot;, the interpreters which are binded to the notebook are stopped automatically after the cron execution. This feature is useful if you want to release the interpreter resources after the cron execution.&gt; **Note**: A cron execution is skipped if one of the paragraphs is in a state of `RUNNING` or `PENDING` no matter whether it is executed automatically (i.e. by the cron scheduler) or manually by a user opening this notebook.### Enable cronSet property **zeppelin.notebook.cron.enable** to **true** in `$ZEPPELIN_HOME/conf/zeppelin-site.xml` to enable Cron feature.### Run cron selectively on foldersIn `$ZEPPELIN_HOME/conf/zeppelin-site.xml` make sure the property **zeppelin.notebook.cron.enable** is set to **true**, and then set property **zeppelin.notebook.cron.folders** to the desired folder as comma-separated values, e.g. `/cron,/test/cron`.",
      "url": " /usage/other_features/cron_scheduler.html",
      "group": "usage/other_features",
      "excerpt": "You can run a notebook on a given schedule automatically by setting up a cron scheduler on the notebook."
    }
    ,



    "/usage/other_features/customizing_homepage.html": {
      "title": "Customizing Apache Zeppelin homepage",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Customizing Apache Zeppelin homepageApache Zeppelin allows you to use one of the notes you create as your Zeppelin Homepage.With that you can brand your Zeppelin installation, adjust the instruction to your users needs and even translate to other languages.## How to set a note as your Zeppelin homepageThe process for creating your homepage is very simple as shown below:1. Create a note using Zeppelin2. Set the note id in the config file3. Restart Zeppelin### Create a note using ZeppelinCreate a new note using Zeppelin,you can use ```%md``` interpreter for markdown content or any other interpreter you like.You can also use the display system to generate [text](../display_system/basic.html#text), [html](../display_system/basic.html#html), [table](../display_system/basic.html#table) orAngular ([backend API](../display_system/angular_backend.html), [frontend API](../display_system/angular_frontend.html)).Run (shift+Enter) the note and see the output. Optionally, change the note view to report to hidethe code sections.### Set the note id in the config fileTo set the note id in the config file, you should copy it from the last word in the note url.For example,Set the note id to the ```ZEPPELIN_NOTEBOOK_HOMESCREEN``` environment variableor ```zeppelin.notebook.homescreen``` property.You can also set the ```ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE``` environment variableor ```zeppelin.notebook.homescreen.hide``` property to hide the new note from the note list.### Restart ZeppelinRestart your Zeppelin server```bash./bin/zeppelin-daemon stop./bin/zeppelin-daemon start```That&#39;s it! Open your browser and navigate to Apache Zeppelin and see your customized homepage.## Show note list in your custom homepageIf you want to display the list of notes on your custom Apache Zeppelin homepage allyou need to do is use our %angular support.Add the following code to a paragraph in your Apache Zeppelin note and run it.```javascript%sparkprintln(&quot;&quot;&quot;%angular  &quot;&quot;&quot;)```After running the paragraph, you will see output similar to this one:That&#39;s it! Voila! You have your note list.",
      "url": " /usage/other_features/customizing_homepage.html",
      "group": "usage/other_features",
      "excerpt": "Apache Zeppelin allows you to use one of the notes you create as your Zeppelin Homepage. With that you can brand your Zeppelin installation, adjust the instruction to your users needs and even translate to other languages."
    }
    ,



    "/usage/other_features/notebook_actions.html": {
      "title": "Notebook Actions",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Revisions comparatorApache Zeppelin allows you to compare revisions of notebook.To see which paragraphs have been changed, removed or added.This action becomes available if your notebook has more than one revision.## How to compare two revisionsFor compare two revisions need open dialog of comparator (by click button) and click on any revision in the table.Or choose two revisions into comboboxes.After click on any revision in the table or selecting the second revision will see the result of the comparison.## How to read the result of the comparisonResult it is list of paragraphs which was in both revisions. If paragraph was added in second revision (&quot;Head&quot;)then so it will be marked as added, if was deleted then it will be marked asdeleted. If paragraph exists in both revisions then it marked as there are differences.To view the comparison click on the section.Сhanges in the text of the paragraph are highlighted in green and red. Red it is line (block of lines) which was deleted, green it is line (block of lines) which was added).",
      "url": " /usage/other_features/notebook_actions.html",
      "group": "usage/other_features",
      "excerpt": "Description of some actions for notebooks"
    }
    ,



    "/usage/other_features/personalized_mode.html": {
      "title": "Personalized Mode",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# What is Personalized Mode? ",
      "url": " /usage/other_features/personalized_mode.html",
      "group": "usage/other_features",
      "excerpt": ""
    }
    ,



    "/usage/other_features/publishing_paragraphs.html": {
      "title": "How can you publish your paragraphs",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# How can you publish your paragraphs?Apache Zeppelin provides a feature for publishing your notebook paragraph results. Using this feature, you can show Zeppelin notebook paragraph results in your own website. It&#39;s very straightforward. Just use `` tag in your page.## Copy a Paragraph LinkA first step to publish your paragraph result is **Copy a Paragraph Link**.  * After running a paragraph in your Zeppelin notebook, click a gear button located on the right side. Then, click **Link this Paragraph** menu like below image.    * Just copy the provided link. ## Embed the Paragraph to Your WebsiteFor publishing the copied paragraph, you may use `` tag in your website page.For example,```:/#/notebook/2B3QSZTKR/paragraph/...?asIframe&quot; height=&quot;&quot; width=&quot;&quot; &gt;```Finally, you can show off your beautiful visualization results in your website. &gt; **Note**: To embed the paragraph in a website, Apache Zeppelin needs to be reachable by that website. And please use this feature with caution and in a trusted environment only, as Zeppelin entire Webapp could be accessible by whoever visits your website. ",
      "url": " /usage/other_features/publishing_paragraphs.html",
      "group": "usage/other_features",
      "excerpt": "Apache Zeppelin provides a feature for publishing your notebook paragraph results. Using this feature, you can show Zeppelin notebook paragraph results in your own website."
    }
    ,



    "/usage/other_features/zeppelin_context.html": {
      "title": "Zeppelin-Context",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Zeppelin-ContextThe zeppelin-context is a system-wide container for common utility functions anduser-specific data. It implements functions for data input, data display, etc. that areoften needed but are not uniformly available in all interpreters.Its single per-user instance is accessible across all of the user&#39;s notebooks and cells,enabling data exchange between cells - even in different notebooks.But the way in which the zeppelin-context is used, and the functionality available differsdepending on whether or not the associated interpreter is based on a programming language.Details of how the zeppelin-context is used for different purposes and in differentenvironments is described below.## Usage in Programming Language CellsIn many programming-language interpreters (e.g. Apache Spark, Python, R) the zeppelin-context is availableas a predefined variable `z` that can be used by directly invoking its methods.The methods available on the `z` object are described below.Other interpreters based on programming languages like Apache Beam, etc. also provide thepredefined variable `z`.### Exploring Spark DataFramesIn the Apache Spark interpreter, the zeppelin-context provides a `show` method, which, using Zeppelin&#39;s `table` feature, can be used to nicely display a Spark DataFrame:```scaladf = spark.read.csv(&#39;/path/to/csv&#39;)z.show(df)```This display functionality using the `show` method is planned to be extended uniformly to other interpreters that can access the `z` object (Flink already support to show table too).### Object Exchange`ZeppelinContext` extends map and it&#39;s shared between the Apache Spark and Python environments.So you can put some objects using Scala (in an Apache Spark cell) and read it from Python, and vice versa.  {% highlight scala %}// Put/Get object from scala%sparkval myObject = &quot;hello&quot;z.put(&quot;objName&quot;, myObject)z.get(&quot;objName&quot;){% endhighlight %}    {% highlight python %}# Put/Get object from python%spark.pysparkval myObject = &quot;hello&quot;z.put(&quot;objName&quot;, myObject)myObject = z.get(&quot;objName&quot;)# df is Python pandas DataFrame# &quot;table_name&quot; must be table type. Currently only sql interpreter (%spark.sql or %jdbc) result is supported.df = z.getAsDataFrame(&quot;table_name&quot;){% endhighlight %}    {% highlight python %}# Get/Put object from R%spark.rz.put(&quot;objName&quot;, myObject)myObject &lt;- z.get(&quot;objName&quot;)# df is R DataFrame# &quot;table_name&quot; must be table type. Currently only sql interpreter (%spark.sql or %jdbc) result is supported.df &lt;- z.getAsDataFrame(&quot;table_name&quot;){% endhighlight %}    Currently, there&#39;re two types of data could be shared across interpreters:* String Data* Table Data#### Share String ObjectHere&#39;s one example we share one String object `maxAge` between Spark interpreter and jdbc interpreter.```scala%sparkz.put(&quot;maxAge&quot;, 83)``````sql%jdbc(interpolate=true)select * from bank where age = {maxAge}```#### Share Table ObjectHere&#39;s one example we share one Table object between jdbc interpreter and python interpreter.```sql%jdbc(saveAs=bank)select * from bank``````python%python.ipython%matplotlib inlineimport warningswarnings.filterwarnings(&quot;ignore&quot;)from plotnine import ggplot, geom_histogram, aes, facet_wrapbank = z.getAsDataFrame(&#39;bank&#39;)(ggplot(bank, aes(x=&#39;age&#39;))```### Form Creation`ZeppelinContext` provides functions for creating forms.In Scala and Python environments, you can create forms programmatically.  {% highlight scala %}%spark/* Create text input form */z.input(&quot;input_1&quot;)/* Create text input form with default value */z.input(&quot;input_2&quot;, &quot;defaultValue&quot;)/* Create select form */z.select(&quot;select_1&quot;, Seq((&quot;option1&quot;, &quot;option1DisplayName&quot;),                         (&quot;option2&quot;, &quot;option2DisplayName&quot;)))/* Create select form with default value*/z.select(&quot;select_2&quot;, &quot;option1&quot;, Seq((&quot;option1&quot;, &quot;option1DisplayName&quot;),                                    (&quot;option2&quot;, &quot;option2DisplayName&quot;))){% endhighlight %}    {% highlight python %}%spark.pyspark# Create text input formz.input(&quot;input_1&quot;)# Create text input form with default valuez.input(&quot;input_2&quot;, &quot;defaultValue&quot;)# Create select formz.select(&quot;select_1&quot;, [(&quot;option1&quot;, &quot;option1DisplayName&quot;),                      (&quot;option2&quot;, &quot;option2DisplayName&quot;)])# Create select form with default valuez.select(&quot;select_2&quot;, [(&quot;option1&quot;, &quot;option1DisplayName&quot;),                      (&quot;option2&quot;, &quot;option2DisplayName&quot;)], &quot;option1&quot;){% endhighlight %}  Patterns of the form ${ ... } are used to dynamically create additional HTML elementsfor requesting user input (that replaces the corresponding pattern in the paragraph text). Currently only [text](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/text), [select](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/select) with [options](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/option), and [checkbox](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/checkbox) are supported.Dynamic forms are described in detail here: [Dynamic Form](../usage/dynamic_form/intro.html).In sql environment, you can create dynamic form in simple template.```sql%spark.sqlselect * from ${table=defaultTableName} where text like &#39;%${search}%&#39;```To learn more about dynamic form, checkout [Dynamic Form](../usage/dynamic_form/intro.html).## Usage with Embedded CommandsIn certain interpreters (see table below) zeppelin-context features may be invoked by embedding command strings into the paragraph text. Such embedded command strings are used to invoke dynamic-forms and object-interpolation as described below. |             Interpreters that use Embedded Commands               ||-------------------------------------------------------------------||spark.sql (*), bigquery, cassandra, elasticsearch, file, hbase, ignite, jdbc (*), kylin, livy, markdown, neo4j, pig, python, shell (*), zengine |Dynamic forms are available in all of the interpreters in the table above, but object interpolation is only available in a small, but growing, list of interpreters (marked with an asterisk in the table above).Both these zeppelin-context features are described below.### Object InterpolationSome interpreters can interpolate object values from `z` into the paragraph text by using the`{variable-name}` syntax. The value of any object previously `put` into `z` can beinterpolated into a paragraph text by using such a pattern containing the object&#39;s name.The following example shows one use of this facility:####In Scala cell:```scala%sparkz.put(&quot;minAge&quot;, 35)```####In later SQL cell:```sql%spark.sql select * from members where age &gt;= {minAge}```The interpolation of a `{var-name}` pattern is performed only when `z` contains an object with the specified name.But the pattern is left unchanged if the named object does not exist in `z`.Further, all `{var-name}` patterns within the paragraph text must be translatable for any interpolation to occur --translation of only some of the patterns in a paragraph text is never done.In some situations, it is necessary to use { and } characters in a paragraph text without invoking theobject interpolation mechanism. For these cases an escaping mechanism is available --doubled braces {{ and }} should be used. The following example shows the use of {{ and }} for passing aregular expression containing just { and } into the paragraph text.```sql%spark.sql {% raw %}select * from members where name rlike &#39;[aeiou]{{3}}&#39;{% endraw %}```To summarize, patterns of the form `{var-name}` within the paragraph text will be interpolated only if a predefinedobject of the specified name exists. Additionally, all such patterns within the paragraph text should alsobe translatable for any interpolation to occur. Patterns of the form {% raw %} `{{any-text}}` {% endraw %} are translated into `{any-text}`.These translations are performed only when all occurrences of `{`, `}`, {% raw %} `{{`, and `}}`{% endraw %} in the paragraph text conformto one of the two forms described above. Paragraph text containing `{` and/or `}` characters used in any other way(than `{var-name}` and {% raw %} `{{any-text}}` {% endraw %} ) is used as-is without any changes.No error is flagged in any case. This behavior is identical to the implementation of a similar feature inJupyter&#39;s shell invocation using the `!` magic command.This feature is disabled by default, and must be explicitly turned on for each interpreter independentlyby setting the value of an interpreter-specific property to `true`.Consult the _Configuration_ section of each interpreter&#39;s documentationto find out if object interpolation is implemented, and the name of the parameter that must be set to `true` toenable the feature. The name of the parameter used to enable this feature is different for each interpreter.For example, the SparkSQL and Shell interpreters use the parameter names `zeppelin.spark.sql.interpolation` and`zeppelin.shell.interpolation` respectively.At present only the SparkSQL, JDBC, and Shell interpreters support object interpolation.### Interpreter-Specific FunctionsSome interpreters use a subclass of `BaseZepplinContext` augmented with interpreter-specific functions.Such interpreter-specific functions are described within each interpreter&#39;s documentation.",
      "url": " /usage/other_features/zeppelin_context.html",
      "group": "usage/other_features",
      "excerpt": "The Zeppelin-Context is a system-wide container for a variety of user-specific settings and parameters that are accessible across notebooks, cells, and interpreters."
    }
    ,



    "/usage/rest_api/configuration.html": {
      "title": "Apache Zeppelin Configuration REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Configuration REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Configuration REST API list### List all key/value pair of configurations                Description      This ```GET``` method return all key/value pair of configurations on the server.       Note: For security reason, some pairs would not be shown.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/configurations/all```              Success code      200               Fail code       500                sample JSON response            ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.war.tempdir&quot;: &quot;webapps&quot;,    &quot;zeppelin.notebook.homescreen.hide&quot;: &quot;false&quot;,    &quot;zeppelin.interpreter.remoterunner&quot;: &quot;bin/interpreter.sh&quot;,    &quot;zeppelin.notebook.s3.user&quot;: &quot;user&quot;,    &quot;zeppelin.server.port&quot;: &quot;8089&quot;,    &quot;zeppelin.dep.localrepo&quot;: &quot;local-repo&quot;,    &quot;zeppelin.ssl.truststore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.keystore.path&quot;: &quot;keystore&quot;,    &quot;zeppelin.notebook.s3.bucket&quot;: &quot;zeppelin&quot;,    &quot;zeppelin.server.addr&quot;: &quot;0.0.0.0&quot;,    &quot;zeppelin.ssl.client.auth&quot;: &quot;false&quot;,    &quot;zeppelin.server.context.path&quot;: &quot;/&quot;,    &quot;zeppelin.ssl.keystore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.truststore.path&quot;: &quot;truststore&quot;,    &quot;zeppelin.ssl&quot;: &quot;false&quot;,    &quot;zeppelin.notebook.autoInterpreterBinding&quot;: &quot;true&quot;,    &quot;zeppelin.notebook.homescreen&quot;: &quot;&quot;,    &quot;zeppelin.notebook.storage&quot;: &quot;org.apache.zeppelin.notebook.repo.VFSNotebookRepo&quot;,    &quot;zeppelin.interpreter.connect.timeout&quot;: &quot;30000&quot;,    &quot;zeppelin.server.allowed.origins&quot;:&quot;*&quot;,    &quot;zeppelin.encoding&quot;: &quot;UTF-8&quot;  }}```      ### List all prefix matched key/value pair of configurations                Description      This ```GET``` method return all prefix matched key/value pair of configurations on the server.      Note: For security reason, some pairs would not be shown.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/configurations/prefix/[prefix]```              Success code      200               Fail code       500                sample JSON response            ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.ssl.keystore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.truststore.path&quot;: &quot;truststore&quot;,    &quot;zeppelin.ssl.truststore.type&quot;: &quot;JKS&quot;,    &quot;zeppelin.ssl.keystore.path&quot;: &quot;keystore&quot;,    &quot;zeppelin.ssl&quot;: &quot;false&quot;,    &quot;zeppelin.ssl.client.auth&quot;: &quot;false&quot;  }}```      ",
      "url": " /usage/rest_api/configuration.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Configuration REST API information."
    }
    ,



    "/usage/rest_api/credential.html": {
      "title": "Apache Zeppelin Credential REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Credential REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Credential REST API List### List Credential information                Description      This ```GET``` method returns all key/value pairs of the credential information on the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/credential```              Success code      200               Fail code       500                sample JSON response            ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;userCredentials&quot;:{      &quot;entity1&quot;:{        &quot;username&quot;:&quot;user1&quot;,        &quot;password&quot;:&quot;password1&quot;      },      &quot;entity2&quot;:{        &quot;username&quot;:&quot;user2&quot;,        &quot;password&quot;:&quot;password2&quot;      }    }  }}```      ### Create an Credential Information                Description      This ```PUT``` method creates the credential information with new properties.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/credential/```              Success code      200              Fail code       500               Sample JSON input      ```json{  &quot;entity&quot;: &quot;e1&quot;,  &quot;username&quot;: &quot;user&quot;,  &quot;password&quot;: &quot;password&quot;}```              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;}```      ### Delete all Credential Information                Description      This ```DELETE``` method deletes the credential information.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/credential```              Success code      200               Fail code       500               Sample JSON response      ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Delete an Credential entity                Description      This ```DELETE``` method deletes a given credential entity.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/credential/[entity]```              Success code      200               Fail code       500               Sample JSON response              ```json{&quot;status&quot;:&quot;OK&quot;}```      ",
      "url": " /usage/rest_api/credential.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Credential REST API information."
    }
    ,



    "/usage/rest_api/helium.html": {
      "title": "Apache Zeppelin Helium REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Helium REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Helium REST API List### Get all available helium packages                Description      This ```GET``` method returns all the available helium packages in configured registries.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/package```              Success code      200              Fail code       500               Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.clock&quot;: [      {        &quot;registry&quot;: &quot;local&quot;,        &quot;pkg&quot;: {          &quot;type&quot;: &quot;APPLICATION&quot;,          &quot;name&quot;: &quot;zeppelin.clock&quot;,          &quot;description&quot;: &quot;Clock (example)&quot;,          &quot;artifact&quot;: &quot;zeppelin-examples/zeppelin-example-clock/target/zeppelin-example-clock-0.7.0-SNAPSHOT.jar&quot;,          &quot;className&quot;: &quot;org.apache.zeppelin.example.app.clock.Clock&quot;,          &quot;resources&quot;: [            [              &quot;:java.util.Date&quot;            ]          ],          &quot;icon&quot;: &quot;icon&quot;        },        &quot;enabled&quot;: false      }    ]  }}```      ### Get all enabled helium packages                Description      This ```GET``` method returns all enabled helium packages in configured registries.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/enabledPackage```              Success code      200              Fail code       500               Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.clock&quot;: [      {        &quot;registry&quot;: &quot;local&quot;,        &quot;pkg&quot;: {          &quot;type&quot;: &quot;APPLICATION&quot;,          &quot;name&quot;: &quot;zeppelin.clock&quot;,          &quot;description&quot;: &quot;Clock (example)&quot;,          &quot;artifact&quot;: &quot;zeppelin-examples/zeppelin-example-clock/target/zeppelin-example-clock-0.7.0-SNAPSHOT.jar&quot;,          &quot;className&quot;: &quot;org.apache.zeppelin.example.app.clock.Clock&quot;,          &quot;resources&quot;: [            [              &quot;:java.util.Date&quot;            ]          ],          &quot;icon&quot;: &quot;icon&quot;        },        &quot;enabled&quot;: false      }    ]  }}```      ### Get single helium package                Description      This ```GET``` method returns specified helium package information              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/package/[Package Name]```              Success code      200              Fail code       500               Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;zeppelin.clock&quot;: [      {        &quot;registry&quot;: &quot;local&quot;,        &quot;pkg&quot;: {          &quot;type&quot;: &quot;APPLICATION&quot;,          &quot;name&quot;: &quot;zeppelin.clock&quot;,          &quot;description&quot;: &quot;Clock (example)&quot;,          &quot;artifact&quot;: &quot;zeppelin-examples/zeppelin-example-clock/target/zeppelin-example-clock-0.7.0-SNAPSHOT.jar&quot;,          &quot;className&quot;: &quot;org.apache.zeppelin.example.app.clock.Clock&quot;,          &quot;resources&quot;: [            [              &quot;:java.util.Date&quot;            ]          ],          &quot;icon&quot;: &quot;icon&quot;        },        &quot;enabled&quot;: false      }    ]  }}```      ### Suggest Helium package on a paragraph                Description      This ```GET``` method returns suggested helium package for the paragraph.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/suggest/[Note ID]/[Paragraph ID]```              Success code      200              Fail code              404 on note or paragraph not exists         500                    Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;available&quot;: [      {        &quot;registry&quot;: &quot;local&quot;,        &quot;pkg&quot;: {          &quot;type&quot;: &quot;APPLICATION&quot;,          &quot;name&quot;: &quot;zeppelin.clock&quot;,          &quot;description&quot;: &quot;Clock (example)&quot;,          &quot;artifact&quot;: &quot;zeppelin-examples/zeppelin-example-clock/target/zeppelin-example-clock-0.7.0-SNAPSHOT.jar&quot;,          &quot;className&quot;: &quot;org.apache.zeppelin.example.app.clock.Clock&quot;,          &quot;resources&quot;: [            [              &quot;:java.util.Date&quot;            ]          ],          &quot;icon&quot;: &quot;icon&quot;        },        &quot;enabled&quot;: true      }    ]  }}```      ### Load Helium package on a paragraph                Description      This ```POST``` method loads helium package to target paragraph.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/load/[Note ID]/[Paragraph ID]```              Success code      200              Fail code              404 on note or paragraph not exists         500                    Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: &quot;app_2C5FYRZ1E-20170108-040449_2068241472zeppelin_clock&quot;}```      ### Load bundled visualization script                Description      This ```GET``` method returns bundled helium visualization javascript. When refresh=true (optional) is provided, Zeppelin rebuilds bundle. Otherwise, it&#39;s provided from cache              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/bundle/load/[Package Name][?refresh=true]```              Success code      200 reponse body is executable javascript              Fail code                200 reponse body is error message string starts with ERROR:            ### Enable package                Description      This ```POST``` method enables a helium package. Needs artifact name in input payload              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/enable/[Package Name]```              Success code      200              Fail code       500               Sample input              zeppelin-examples/zeppelin-example-clock/target/zeppelin-example-clock-0.7.0-SNAPSHOT.jar                            Sample JSON response      ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Disable package                Description      This ```POST``` method disables a helium package.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/disable/[Package Name]```              Success code      200               Fail code       500               Sample JSON response              ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Get visualization display order                Description      This ```GET``` method returns display order of enabled visualization packages.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/order/visualization```              Success code      200               Fail code       500               Sample JSON response              ```json{&quot;status&quot;:&quot;OK&quot;,&quot;body&quot;:[&quot;zeppelin_horizontalbar&quot;,&quot;zeppelin-bubblechart&quot;]}```      ### Set visualization display order                Description      This ```POST``` method sets visualization packages display order.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/order/visualization```              Success code      200               Fail code       500               Sample JSON input              ```json[&quot;zeppelin-bubblechart&quot;, &quot;zeppelin_horizontalbar&quot;]```              Sample JSON response              ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Get configuration for all Helium packages                Description      This ```GET``` method returns configuration for all Helium packages              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/config```              Success code      200               Fail code       500          ### Get configuration for specific package                     Description       This ```GET``` method returns configuration for the specified package name and artifact                 URL       ```http://[zeppelin-server]:[zeppelin-port]/api/helium/config/[Package Name]/[Artifact]```                 Success code       200                  Fail code        500         ### Set configuration for specific package                Description      This ```POST``` method updates configuration for specified package name and artifact              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/helium/config/[Package Name]/[Artifact]```              Success code      200               Fail code       500             ### Get Spell configuration for single package                     Description       This ```GET``` method returns specified package Spell configuration                 URL       ```http://[zeppelin-server]:[zeppelin-port]/api/helium/spell/config/[Package Name]```                 Success code       200                  Fail code        500           ",
      "url": " /usage/rest_api/helium.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Helium REST API information."
    }
    ,



    "/usage/rest_api/interpreter.html": {
      "title": "Apache Zeppelin Interpreter REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Interpreter REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Interpreter REST API ListThe role of registered interpreters, settings and interpreters group are described in [here](../interpreter/overview.html).### List of registered interpreters                Description      This ```GET``` method returns all the registered interpreters available on the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter```              Success code      200              Fail code       500               Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;md.md&quot;: {      &quot;name&quot;: &quot;md&quot;,      &quot;group&quot;: &quot;md&quot;,      &quot;className&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,      &quot;properties&quot;: {},      &quot;path&quot;: &quot;/zeppelin/interpreter/md&quot;    },    &quot;spark.spark&quot;: {      &quot;name&quot;: &quot;spark&quot;,      &quot;group&quot;: &quot;spark&quot;,      &quot;className&quot;: &quot;org.apache.zeppelin.spark.SparkInterpreter&quot;,      &quot;properties&quot;: {        &quot;spark.executor.memory&quot;: {          &quot;name&quot;: &quot;spark.executor.memory&quot;,          &quot;defaultValue&quot;: &quot;1g&quot;,          &quot;description&quot;: &quot;Executor memory per worker instance. ex) 512m, 32g&quot;,          &quot;type&quot;: &quot;string&quot;        },        &quot;spark.cores.max&quot;: {          &quot;defaultValue&quot;: &quot;&quot;,          &quot;description&quot;: &quot;Total number of cores to use. Empty value uses all available core.&quot;,          &quot;type&quot;: &quot;number&quot;        },      },      &quot;path&quot;: &quot;/zeppelin/interpreter/spark&quot;    },    &quot;spark.sql&quot;: {      &quot;name&quot;: &quot;sql&quot;,      &quot;group&quot;: &quot;spark&quot;,      &quot;className&quot;: &quot;org.apache.zeppelin.spark.SparkSqlInterpreter&quot;,      &quot;properties&quot;: {        &quot;zeppelin.spark.maxResult&quot;: {          &quot;name&quot;: &quot;zeppelin.spark.maxResult&quot;,          &quot;defaultValue&quot;: &quot;1000&quot;,          &quot;description&quot;: &quot;Max number of Spark SQL result to display.&quot;,          &quot;type&quot;: &quot;number&quot;        }      },      &quot;path&quot;: &quot;/zeppelin/interpreter/spark&quot;    }  }}```      ### List of registered interpreter settings                Description      This ```GET``` method returns all the interpreters settings registered on the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting```              Success code      200              Fail code       500               Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: [    {      &quot;id&quot;: &quot;2AYUGP2D5&quot;,      &quot;name&quot;: &quot;md&quot;,      &quot;group&quot;: &quot;md&quot;,      &quot;properties&quot;: {        &quot;_empty_&quot;: &quot;&quot;      },      &quot;interpreterGroup&quot;: [        {          &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,          &quot;name&quot;: &quot;md&quot;        }      ],      &quot;dependencies&quot;: []    },      {      &quot;id&quot;: &quot;2AY6GV7Q3&quot;,      &quot;name&quot;: &quot;spark&quot;,      &quot;group&quot;: &quot;spark&quot;,      &quot;properties&quot;: {        &quot;spark.cores.max&quot;: {          &quot;name&quot;: &quot;&quot;,          &quot;value&quot;: &quot;spark.cores.max&quot;,          &quot;type&quot;: &quot;number&quot;        },        &quot;spark.executor.memory&quot;: {          &quot;name&quot;: &quot;spark.executor.memory&quot;,          &quot;value&quot;: &quot;1g&quot;,          &quot;type&quot;: &quot;string&quot;        }      },      &quot;interpreterGroup&quot;: [        {          &quot;class&quot;: &quot;org.apache.zeppelin.spark.SparkInterpreter&quot;,          &quot;name&quot;: &quot;spark&quot;        },        {          &quot;class&quot;: &quot;org.apache.zeppelin.spark.SparkSqlInterpreter&quot;,          &quot;name&quot;: &quot;sql&quot;        }      ],      &quot;dependencies&quot;: [        {          &quot;groupArtifactVersion&quot;: &quot;com.databricks:spark-csv_2.10:1.3.0&quot;        }      ]    }  ]}```        ### Get a registered interpreter setting by the setting id                 Description      This ```GET``` method returns a registered interpreter setting on the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting/[setting ID]```              Success code      200              Fail code                400 if such interpreter setting id does not exist           500 for any other errors                    Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;id&quot;: &quot;2AYW25ANY&quot;,    &quot;name&quot;: &quot;Markdown setting name&quot;,    &quot;group&quot;: &quot;md&quot;,    &quot;properties&quot;: {      &quot;propname&quot;: {        &quot;name&quot;: &quot;propname&quot;,        &quot;value&quot;: &quot;propvalue&quot;,        &quot;type&quot;: &quot;textarea&quot;      }    },    &quot;interpreterGroup&quot;: [      {        &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,        &quot;name&quot;: &quot;md&quot;      }    ],    &quot;dependencies&quot;: [      {        &quot;groupArtifactVersion&quot;: &quot;groupId:artifactId:version&quot;,        &quot;exclusions&quot;: [          &quot;groupId:artifactId&quot;        ]      }    ]  }}```      ### Create a new interpreter setting                  Description      This ```POST``` method adds a new interpreter setting using a registered interpreter to the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting```              Success code      200              Fail code                400 if the input json is empty           500 for any other errors                    Sample JSON input      ```json{  &quot;name&quot;: &quot;Markdown setting name&quot;,  &quot;group&quot;: &quot;md&quot;,  &quot;properties&quot;: {    &quot;propname&quot;: {      &quot;name&quot;: &quot;propname&quot;,      &quot;value&quot;: &quot;propvalue&quot;,      &quot;type&quot;: &quot;textarea&quot;  },  &quot;interpreterGroup&quot;: [    {      &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,      &quot;name&quot;: &quot;md&quot;    }  ],  &quot;dependencies&quot;: [    {      &quot;groupArtifactVersion&quot;: &quot;groupId:artifactId:version&quot;,      &quot;exclusions&quot;: [        &quot;groupId:artifactId&quot;      ]    }  ]}```              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;id&quot;: &quot;2AYW25ANY&quot;,    &quot;name&quot;: &quot;Markdown setting name&quot;,    &quot;group&quot;: &quot;md&quot;,    &quot;properties&quot;: {      &quot;propname&quot;: {        &quot;name&quot;: &quot;propname&quot;,        &quot;value&quot;: &quot;propvalue&quot;,        &quot;type&quot;: &quot;textarea&quot;    },    &quot;interpreterGroup&quot;: [      {        &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,        &quot;name&quot;: &quot;md&quot;      }    ],    &quot;dependencies&quot;: [      {        &quot;groupArtifactVersion&quot;: &quot;groupId:artifactId:version&quot;,        &quot;exclusions&quot;: [          &quot;groupId:artifactId&quot;        ]      }    ]  }}```      ### Update an interpreter setting                Description      This ```PUT``` method updates an interpreter setting with new properties.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting/[interpreter ID]```              Success code      200              Fail code       500               Sample JSON input      ```json{  &quot;name&quot;: &quot;Markdown setting name&quot;,  &quot;group&quot;: &quot;md&quot;,  &quot;properties&quot;: {    &quot;propname&quot;: {      &quot;name&quot;: &quot;propname&quot;,      &quot;value&quot;: &quot;Otherpropvalue&quot;,      &quot;type&quot;: &quot;textarea&quot;  },  &quot;interpreterGroup&quot;: [    {      &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,      &quot;name&quot;: &quot;md&quot;    }  ],  &quot;dependencies&quot;: [    {      &quot;groupArtifactVersion&quot;: &quot;groupId:artifactId:version&quot;,      &quot;exclusions&quot;: [        &quot;groupId:artifactId&quot;      ]    }  ]}```              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;id&quot;: &quot;2AYW25ANY&quot;,    &quot;name&quot;: &quot;Markdown setting name&quot;,    &quot;group&quot;: &quot;md&quot;,    &quot;properties&quot;: {      &quot;propname&quot;: {        &quot;name&quot;: &quot;propname&quot;,        &quot;value&quot;: &quot;Otherpropvalue&quot;,        &quot;type&quot;: &quot;textarea&quot;    },    &quot;interpreterGroup&quot;: [      {        &quot;class&quot;: &quot;org.apache.zeppelin.markdown.Markdown&quot;,        &quot;name&quot;: &quot;md&quot;      }    ],    &quot;dependencies&quot;: [      {        &quot;groupArtifactVersion&quot;: &quot;groupId:artifactId:version&quot;,        &quot;exclusions&quot;: [          &quot;groupId:artifactId&quot;        ]      }    ]  }}```      ### Delete an interpreter setting                Description      This ```DELETE``` method deletes an given interpreter setting.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting/[interpreter ID]```              Success code      200               Fail code       500               Sample JSON response      ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Restart an interpreter                Description      This ```PUT``` method restarts the given interpreter id.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/setting/restart/[interpreter ID]```              Success code      200              Fail code       500               Sample JSON input (Optional)      ```json{  &quot;noteId&quot;: &quot;2AVQJVC8N&quot;}```              Sample JSON response      ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Add a new repository for dependency resolving                Description      This ```POST``` method adds new repository.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/repository```              Success code      200              Fail code       500               Sample JSON input      ```json{  &quot;id&quot;: &quot;securecentral&quot;,  &quot;url&quot;: &quot;https://repo1.maven.org/maven2&quot;,  &quot;snapshot&quot;: false}```              Sample JSON response      ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Delete a repository for dependency resolving                Description      This ```DELETE``` method delete repository with given id.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/repository/[repository ID]```              Success code      200              Fail code       500         ### Get available types for property                Description      This ```GET``` method returns available types for interpreter property.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/property/types```              Success code      200              Fail code       500               Sample JSON response        ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;body&quot;: [ &quot;textarea&quot;, &quot;string&quot;, ...  ]}            ```                  ### Get interpreter settings metadata info                Description      This ```GET``` method returns interpreter settings metadata info.               URL      ```http://[zeppelin-server]:[zeppelin-port]/api/interpreter/metadata/[setting ID]```              Success code      200              Fail code       500         ",
      "url": " /usage/rest_api/interpreter.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Interpreter REST API information."
    }
    ,



    "/usage/rest_api/notebook.html": {
      "title": "Apache Zeppelin Notebook REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Notebook REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).Notebooks REST API supports the following operations: List, Create, Get, Delete, Clone, Run, Export, Import as detailed in the following tables.## Note operations### List of the notes                Description      This ```GET``` method lists the available notes on your server.          Notebook JSON contains the ```name``` and ```id``` of all notes.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook```              Success code      200               Fail code       500                sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: [    {      &quot;path&quot;:&quot;Homepage&quot;,      &quot;id&quot;:&quot;2AV4WUEMK&quot;    },    {      &quot;path&quot;:&quot;Zeppelin Tutorial&quot;,      &quot;id&quot;:&quot;2A94M5J1Z&quot;    }  ]}```      ### Create a new note                Description      This ```POST``` method creates a new note using the given name or default name if none given.          The body field of the returned JSON contains the new note id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook```              Success code      200               Fail code       500                sample JSON input (without paragraphs)       ```json{&quot;name&quot;: &quot;name of new note&quot;}```               sample JSON input (with initial paragraphs)       ```json{  &quot;name&quot;: &quot;name of new note&quot;,  &quot;paragraphs&quot;: [    {      &quot;title&quot;: &quot;paragraph title1&quot;,      &quot;text&quot;: &quot;paragraph text1&quot;    },    {      &quot;title&quot;: &quot;paragraph title2&quot;,      &quot;text&quot;: &quot;paragraph text2&quot;,      &quot;config&quot;: {        &quot;title&quot;: true,        &quot;colWidth&quot;: 6.0,        &quot;results&quot;: [          {            &quot;graph&quot;: {              &quot;mode&quot;: &quot;scatterChart&quot;,              &quot;optionOpen&quot;: true            }          }        ]      }    }  ]}```               sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: &quot;2AZPHY918&quot;}```      ### Get the status of all paragraphs                Description      This ```GET``` method gets the status of all paragraphs by the given note id.          The body field of the returned JSON contains of the array that compose of the paragraph id, paragraph status, paragraph finish date, paragraph started date.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;body&quot;: [    {      &quot;id&quot;:&quot;20151121-212654_766735423&quot;,      &quot;status&quot;:&quot;FINISHED&quot;,      &quot;finished&quot;:&quot;Tue Nov 24 14:21:40 KST 2015&quot;,      &quot;started&quot;:&quot;Tue Nov 24 14:21:39 KST 2015&quot;    },    {      &quot;progress&quot;:&quot;1&quot;,      &quot;id&quot;:&quot;20151121-212657_730976687&quot;,      &quot;status&quot;:&quot;RUNNING&quot;,      &quot;finished&quot;:&quot;Tue Nov 24 14:21:35 KST 2015&quot;,      &quot;started&quot;:&quot;Tue Nov 24 14:21:40 KST 2015&quot;    }  ]}```      ### Get an existing note information                Description      This ```GET``` method retrieves an existing note&#39;s information using the given id.          The body field of the returned JSON contain information about paragraphs in the note.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;paragraphs&quot;: [      {        &quot;text&quot;: &quot;%sql nselect age, count(1) valuenfrom bank nwhere age &lt; 30 ngroup by age norder by age&quot;,        &quot;config&quot;: {          &quot;colWidth&quot;: 4,          &quot;graph&quot;: {            &quot;mode&quot;: &quot;multiBarChart&quot;,            &quot;height&quot;: 300,            &quot;optionOpen&quot;: false,            &quot;keys&quot;: [              {                &quot;name&quot;: &quot;age&quot;,                &quot;index&quot;: 0,                &quot;aggr&quot;: &quot;sum&quot;              }            ],            &quot;values&quot;: [              {                &quot;name&quot;: &quot;value&quot;,                &quot;index&quot;: 1,                &quot;aggr&quot;: &quot;sum&quot;              }            ],            &quot;groups&quot;: [],            &quot;scatter&quot;: {              &quot;xAxis&quot;: {                &quot;name&quot;: &quot;age&quot;,                &quot;index&quot;: 0,                &quot;aggr&quot;: &quot;sum&quot;              },              &quot;yAxis&quot;: {                &quot;name&quot;: &quot;value&quot;,                &quot;index&quot;: 1,                &quot;aggr&quot;: &quot;sum&quot;              }            }          }        },        &quot;settings&quot;: {          &quot;params&quot;: {},          &quot;forms&quot;: {}        },        &quot;jobName&quot;: &quot;paragraph_1423500782552_-1439281894&quot;,        &quot;id&quot;: &quot;20150210-015302_1492795503&quot;,        &quot;results&quot;: {          &quot;code&quot;: &quot;SUCCESS&quot;,          &quot;msg&quot;: [            {              &quot;type&quot;: &quot;TABLE&quot;,              &quot;data&quot;: &quot;agetvaluen19t4n20t3n21t7n22t9n23t20n24t24n25t44n26t77n27t94n28t103n29t97n&quot;            }          ]        },        &quot;dateCreated&quot;: &quot;Feb 10, 2015 1:53:02 AM&quot;,        &quot;dateStarted&quot;: &quot;Jul 3, 2015 1:43:17 PM&quot;,        &quot;dateFinished&quot;: &quot;Jul 3, 2015 1:43:23 PM&quot;,        &quot;status&quot;: &quot;FINISHED&quot;,        &quot;progressUpdateIntervalMs&quot;: 500      }    ],    &quot;name&quot;: &quot;Zeppelin Tutorial&quot;,    &quot;id&quot;: &quot;2A94M5J1Z&quot;,    &quot;angularObjects&quot;: {},    &quot;config&quot;: {      &quot;looknfeel&quot;: &quot;default&quot;    },    &quot;info&quot;: {}  }}```      ### Delete a note                Description      This ```DELETE``` method deletes a note by the given note id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;,&quot;message&quot;: &quot;&quot;}```      ### Clone a note                Description      This ```POST``` method clones a note by the given id and create a new note using the given name          or default name if none given.          The body field of the returned JSON contains the new note id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]```              Success code      200               Fail code       500                sample JSON input       ```json{&quot;name&quot;: &quot;name of new note&quot;}```               sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: &quot;2AZPHY918&quot;}```      ### Rename a note                Description      This ```PUT``` method renames a note by the given id using the given name.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/rename```              Success code      200               Bad Request code       400               Fail code       500                sample JSON input       ```json{&quot;name&quot;: &quot;new name of a note&quot;}```               sample JSON response       ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Export a note                Description      This ```GET``` method exports a note by the given id and gernerates a JSON                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/export/[noteId]```              Success code      201               Fail code       500          sample JSON response       ```json{  &quot;paragraphs&quot;: [    {      &quot;text&quot;: &quot;%md This is my new paragraph in my new note&quot;,      &quot;dateUpdated&quot;: &quot;Jan 8, 2016 4:49:38 PM&quot;,      &quot;config&quot;: {        &quot;enabled&quot;: true      },      &quot;settings&quot;: {        &quot;params&quot;: {},        &quot;forms&quot;: {}      },      &quot;jobName&quot;: &quot;paragraph_1452300578795_1196072540&quot;,      &quot;id&quot;: &quot;20160108-164938_1685162144&quot;,      &quot;dateCreated&quot;: &quot;Jan 8, 2016 4:49:38 PM&quot;,      &quot;status&quot;: &quot;READY&quot;,      &quot;progressUpdateIntervalMs&quot;: 500    }  ],  &quot;name&quot;: &quot;source note for export&quot;,  &quot;id&quot;: &quot;2B82H3RR1&quot;,  &quot;angularObjects&quot;: {},  &quot;config&quot;: {},  &quot;info&quot;: {}}```      ### Import a note                Description      This ```POST``` method imports a note from the note JSON input                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/import```              Success code      201               Fail code       500               sample JSON input      ```json{  &quot;paragraphs&quot;: [    {      &quot;text&quot;: &quot;%md This is my new paragraph in my new note&quot;,      &quot;dateUpdated&quot;: &quot;Jan 8, 2016 4:49:38 PM&quot;,      &quot;config&quot;: {        &quot;enabled&quot;: true      },      &quot;settings&quot;: {        &quot;params&quot;: {},        &quot;forms&quot;: {}      },      &quot;jobName&quot;: &quot;paragraph_1452300578795_1196072540&quot;,      &quot;id&quot;: &quot;20160108-164938_1685162144&quot;,      &quot;dateCreated&quot;: &quot;Jan 8, 2016 4:49:38 PM&quot;,      &quot;status&quot;: &quot;READY&quot;,      &quot;progressUpdateIntervalMs&quot;: 500    }  ],  &quot;name&quot;: &quot;source note for export&quot;,  &quot;id&quot;: &quot;2B82H3RR1&quot;,  &quot;angularObjects&quot;: {},  &quot;config&quot;: {},  &quot;info&quot;: {}}```              sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: &quot;2AZPHY918&quot;}```      ### Run all paragraphs                Description            This ```POST``` method runs all paragraphs in the given note id.       If you can not find Note id 404 returns.      If there is a problem with the interpreter returns a 412 error.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]```              Success code      200               Fail code       404 or 412               sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```                sample JSON error response        ```json{  &quot;status&quot;: &quot;NOT_FOUND&quot;,  &quot;message&quot;: &quot;note not found.&quot;}``````json{  &quot;status&quot;: &quot;PRECONDITION_FAILED&quot;,  &quot;message&quot;: &quot;paragraph_1469771130099_-278315611 Not selected or Invalid Interpreter bind&quot;}```      ### Stop all paragraphs                Description      This ```DELETE``` method stops all paragraphs in the given note id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;:&quot;OK&quot;}```      ### Clear all paragraph result                Description      This ```PUT``` method clear all paragraph results from note of given id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/clear```              Success code      200              Forbidden code      401              Not Found code      404              Fail code      500              sample JSON response      ```json{&quot;status&quot;: &quot;OK&quot;}```          ## Paragraph operations ### Create a new paragraph                Description      This ```POST``` method create a new paragraph using JSON payload.          The body field of the returned JSON contain the new paragraph id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph```              Success code      201               Fail code       500                sample JSON input (add to the last)       ```json{  &quot;title&quot;: &quot;Paragraph insert revised&quot;,  &quot;text&quot;: &quot;%sparknprintln(&quot;Paragraph insert revised&quot;)&quot;}```               sample JSON input (add to specific index)       ```json{  &quot;title&quot;: &quot;Paragraph insert revised&quot;,  &quot;text&quot;: &quot;%sparknprintln(&quot;Paragraph insert revised&quot;)&quot;,  &quot;index&quot;: 0}```               sample JSON input (providing paragraph config)       ```json{  &quot;title&quot;: &quot;paragraph title2&quot;,  &quot;text&quot;: &quot;paragraph text2&quot;,  &quot;config&quot;: {    &quot;title&quot;: true,    &quot;colWidth&quot;: 6.0,    &quot;results&quot;: [      {        &quot;graph&quot;: {          &quot;mode&quot;: &quot;pieChart&quot;,          &quot;optionOpen&quot;: true        }      }    ]  }}```               sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: &quot;20151218-100330_1754029574&quot;}```      ### Get a paragraph information                Description      This ```GET``` method retrieves an existing paragraph&#39;s information using the given id.          The body field of the returned JSON contain information about paragraph.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph/[paragraphId]```              Success code      200               Fail code       500                sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;title&quot;: &quot;Paragraph2&quot;,    &quot;text&quot;: &quot;%sparknnprintln(&quot;it&#39;s paragraph2&quot;)&quot;,    &quot;dateUpdated&quot;: &quot;Dec 18, 2015 7:33:54 AM&quot;,    &quot;config&quot;: {      &quot;colWidth&quot;: 12,      &quot;graph&quot;: {        &quot;mode&quot;: &quot;table&quot;,        &quot;height&quot;: 300,        &quot;optionOpen&quot;: false,        &quot;keys&quot;: [],        &quot;values&quot;: [],        &quot;groups&quot;: [],        &quot;scatter&quot;: {}      },      &quot;enabled&quot;: true,      &quot;title&quot;: true,      &quot;editorMode&quot;: &quot;ace/mode/scala&quot;    },    &quot;settings&quot;: {      &quot;params&quot;: {},      &quot;forms&quot;: {}    },    &quot;jobName&quot;: &quot;paragraph_1450391574392_-1890856722&quot;,    &quot;id&quot;: &quot;20151218-073254_1105602047&quot;,    &quot;results&quot;: {      &quot;code&quot;: &quot;SUCCESS&quot;,      &quot;msg&quot;: [        {           &quot;type&quot;: &quot;TEXT&quot;,           &quot;data&quot;: &quot;it&#39;s paragraph2n&quot;        }      ]    },    &quot;dateCreated&quot;: &quot;Dec 18, 2015 7:32:54 AM&quot;,    &quot;dateStarted&quot;: &quot;Dec 18, 2015 7:33:55 AM&quot;,    &quot;dateFinished&quot;: &quot;Dec 18, 2015 7:33:55 AM&quot;,    &quot;status&quot;: &quot;FINISHED&quot;,    &quot;progressUpdateIntervalMs&quot;: 500  }}```      ### Get the status of a single paragraph                Description      This ```GET``` method gets the status of a single paragraph by the given note and paragraph id.          The body field of the returned JSON contains of the array that compose of the paragraph id, paragraph status, paragraph finish date, paragraph started date.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]/[paragraphId]```              Success code      200               Fail code       500                sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;body&quot;: {      &quot;id&quot;:&quot;20151121-212654_766735423&quot;,      &quot;status&quot;:&quot;FINISHED&quot;,      &quot;finished&quot;:&quot;Tue Nov 24 14:21:40 KST 2015&quot;,      &quot;started&quot;:&quot;Tue Nov 24 14:21:39 KST 2015&quot;    }}```      ### Update paragraph                Description      This ```PUT``` method update paragraph contents using given id, e.g. {&quot;text&quot;: &quot;hello&quot;}                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph/[paragraphId]```              Success code      200              Bad Request code      400              Forbidden code      403              Not Found code      404              Fail code      500              sample JSON input      ```json{  &quot;title&quot;: &quot;Hello world&quot;,  &quot;text&quot;: &quot;println(&quot;hello world&quot;)&quot;}```              sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;}```      ### Update paragraph configuration                Description      This ```PUT``` method update paragraph configuration using given id so that user can change paragraph setting such as graph type, show or hide editor/result and paragraph size, etc. You can update certain fields you want, for example you can update colWidth field only by sending request with payload {&quot;colWidth&quot;: 12.0}.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph/[paragraphId]/config```              Success code      200              Bad Request code      400              Forbidden code      403              Not Found code      404              Fail code      500              sample JSON input      ```json{  &quot;colWidth&quot;: 6.0,  &quot;graph&quot;: {    &quot;mode&quot;: &quot;lineChart&quot;,    &quot;height&quot;: 200.0,    &quot;optionOpen&quot;: false,    &quot;keys&quot;: [      {        &quot;name&quot;: &quot;age&quot;,        &quot;index&quot;: 0.0,        &quot;aggr&quot;: &quot;sum&quot;      }    ],    &quot;values&quot;: [      {        &quot;name&quot;: &quot;value&quot;,        &quot;index&quot;: 1.0,        &quot;aggr&quot;: &quot;sum&quot;      }    ],    &quot;groups&quot;: [],    &quot;scatter&quot;: {}  },  &quot;editorHide&quot;: true,  &quot;editorMode&quot;: &quot;ace/mode/markdown&quot;,  &quot;tableHide&quot;: false}```              sample JSON response      ```json{  &quot;status&quot;:&quot;OK&quot;,  &quot;message&quot;:&quot;&quot;,  &quot;body&quot;:{    &quot;text&quot;:&quot;%sql nselect age, count(1) valuenfrom bank nwhere age u003c 30 ngroup by age norder by age&quot;,    &quot;config&quot;:{      &quot;colWidth&quot;:6.0,      &quot;graph&quot;:{        &quot;mode&quot;:&quot;lineChart&quot;,        &quot;height&quot;:200.0,        &quot;optionOpen&quot;:false,        &quot;keys&quot;:[          {            &quot;name&quot;:&quot;age&quot;,            &quot;index&quot;:0.0,            &quot;aggr&quot;:&quot;sum&quot;          }        ],        &quot;values&quot;:[          {            &quot;name&quot;:&quot;value&quot;,            &quot;index&quot;:1.0,            &quot;aggr&quot;:&quot;sum&quot;          }        ],        &quot;groups&quot;:[],        &quot;scatter&quot;:{}      },      &quot;tableHide&quot;:false,      &quot;editorMode&quot;:&quot;ace/mode/markdown&quot;,      &quot;editorHide&quot;:true    },    &quot;settings&quot;:{      &quot;params&quot;:{},      &quot;forms&quot;:{}    },    &quot;apps&quot;:[],    &quot;jobName&quot;:&quot;paragraph_1423500782552_-1439281894&quot;,    &quot;id&quot;:&quot;20150210-015302_1492795503&quot;,    &quot;results&quot;:{      &quot;code&quot;:&quot;SUCCESS&quot;,      &quot;msg&quot;: [        {          &quot;type&quot;:&quot;TABLE&quot;,          &quot;data&quot;:&quot;agetvaluen19t4n20t3n21t7n22t9n23t20n24t24n25t44n26t77n27t94n28t103n29t97n&quot;        }      ]    },    &quot;dateCreated&quot;:&quot;Feb 10, 2015 1:53:02 AM&quot;,    &quot;dateStarted&quot;:&quot;Jul 3, 2015 1:43:17 PM&quot;,    &quot;dateFinished&quot;:&quot;Jul 3, 2015 1:43:23 PM&quot;,    &quot;status&quot;:&quot;FINISHED&quot;,    &quot;progressUpdateIntervalMs&quot;:500  }}```      ### Delete a paragraph                Description      This ```DELETE``` method deletes a paragraph by the given note and paragraph id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph/[paragraphId]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;,&quot;message&quot;: &quot;&quot;}```      ### Run a paragraph asynchronously                Description      This ```POST``` method runs the paragraph asynchronously by given note and paragraph id. This API always return SUCCESS even if the execution of the paragraph fails later because the API is asynchronous                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]/[paragraphId]```              Success code      200               Fail code       500                sample JSON input (optional, only needed when if you want to update dynamic form&#39;s value)       ```json{  &quot;name&quot;: &quot;name of new note&quot;,  &quot;params&quot;: {    &quot;formLabel1&quot;: &quot;value1&quot;,    &quot;formLabel2&quot;: &quot;value2&quot;  }}```               sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```      ### Run a paragraph synchronously                Description      This ```POST``` method runs the paragraph synchronously by given note and paragraph id. This API can return SUCCESS or ERROR depending on the outcome of the paragraph execution                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/run/[noteId]/[paragraphId]```              Success code      200               Fail code       500                sample JSON input (optional, only needed when if you want to update dynamic form&#39;s value)       ```json{  &quot;name&quot;: &quot;name of new note&quot;,  &quot;params&quot;: {    &quot;formLabel1&quot;: &quot;value1&quot;,    &quot;formLabel2&quot;: &quot;value2&quot;  }}```               sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```                   sample JSON error       ```json{   &quot;status&quot;: &quot;INTERNAL_SERVER_ERROR&quot;,   &quot;body&quot;: {       &quot;code&quot;: &quot;ERROR&quot;,       &quot;type&quot;: &quot;TEXT&quot;,       &quot;msg&quot;: &quot;bash: -c: line 0: unexpected EOF while looking for matching ``&#39;nbash: -c: line 1: syntax error: unexpected end of filenExitValue: 2&quot;   }}```      ### Stop a paragraph                Description      This ```DELETE``` method stops the paragraph by given note and paragraph id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/job/[noteId]/[paragraphId]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```      ### Move a paragraph to the specific index                Description      This ```POST``` method moves a paragraph to the specific index (order) from the note.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/paragraph/[paragraphId]/move/[newIndex]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;,&quot;message&quot;: &quot;&quot;}```      ### Full text search through the paragraphs in all notes                Description      ```GET``` request will return list of matching paragraphs                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/search?q=[query]```              Success code      200              Fail code       500               Sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;body&quot;: [    {      &quot;id&quot;: &quot;/paragraph/&quot;,      &quot;name&quot;:&quot;Note Name&quot;,       &quot;snippet&quot;:&quot;&quot;,      &quot;text&quot;:&quot;&quot;    }  ]}```      ## Cron jobs### Add Cron Job                Description      This ```POST``` method adds cron job by the given note id.           Default value of ```releaseResource``` is ```false```.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/cron/[noteId]```              Success code      200               Fail code       500                sample JSON input       ```json{&quot;cron&quot;: &quot;cron expression of note&quot;, &quot;releaseResource&quot;: &quot;false&quot;}```               sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```      ### Remove Cron Job                Description      This ```DELETE``` method removes cron job by the given note id.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/cron/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{&quot;status&quot;: &quot;OK&quot;}```      ### Get Cron Job                Description      This ```GET``` method gets cron job expression of given note id.          The body field of the returned JSON contains the cron expression and ```releaseResource``` flag.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/cron/[noteId]```              Success code      200               Fail code       500                sample JSON response       ```json{   &quot;status&quot;: &quot;OK&quot;,    &quot;body&quot;: {      &quot;cron&quot;: &quot;0 0/1 * * * ?&quot;,       &quot;releaseResource&quot;: true   }}```      ## Permission### Get a note permission information                Description      This ```GET``` method gets a note authorization information.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/permissions```              Success code      200              Forbidden code      403              Fail code      500               sample JSON response       ```json{     &quot;status&quot;:&quot;OK&quot;,   &quot;message&quot;:&quot;&quot;,   &quot;body&quot;:{        &quot;readers&quot;:[           &quot;user2&quot;      ],      &quot;owners&quot;:[           &quot;user1&quot;      ],      &quot;runners&quot;:[         &quot;user2&quot;      ],      &quot;writers&quot;:[           &quot;user2&quot;      ]   }}```      ### Set note permission                Description      This ```PUT``` method set note authorization information.                    URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook/[noteId]/permissions```              Success code      200              Forbidden code      403              Fail code      500               sample JSON input       ```json{  &quot;readers&quot;: [    &quot;user1&quot;  ],  &quot;owners&quot;: [    &quot;user2&quot;  ],  &quot;runners&quot;:[    &quot;user2&quot;  ],  &quot;writers&quot;: [    &quot;user1&quot;  ]}```               sample JSON response       ```json{  &quot;status&quot;: &quot;OK&quot;}```      ",
      "url": " /usage/rest_api/notebook.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Notebook REST API information."
    }
    ,



    "/usage/rest_api/notebook_repository.html": {
      "title": "Apache Zeppelin notebook repository REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Notebook Repository API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Notebook Repository REST API List### List all available notebook repositories                Description      This ```GET``` method returns all the available notebook repositories.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook-repositories```              Success code      200              Fail code      500              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: [    {      &quot;name&quot;: &quot;GitNotebookRepo&quot;,      &quot;className&quot;: &quot;org.apache.zeppelin.notebook.repo.GitNotebookRepo&quot;,      &quot;settings&quot;: [        {          &quot;type&quot;: &quot;INPUT&quot;,          &quot;value&quot;: [],          &quot;selected&quot;: &quot;ZEPPELIN_HOME/zeppelin/notebook/&quot;,          &quot;name&quot;: &quot;Notebook Path&quot;        }      ]    }  ]}```      ### Reload a notebook repository                Description      This ```GET``` method triggers reloading and broadcasting of the note list.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook-repositories/reload```              Success code      200              Fail code      500              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;}```      ### Update a specific notebook repository                Description      This ```PUT``` method updates a specific notebook repository.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/notebook-repositories```              Success code      200              Fail code              404 when the specified notebook repository doesn&#39;t exist          406 for invalid payload         500 for any other errors                    Sample JSON input      ```json{  &quot;name&quot;:&quot;org.apache.zeppelin.notebook.repo.GitNotebookRepo&quot;,  &quot;settings&quot;:{    &quot;Notebook Path&quot;:&quot;/tmp/notebook/&quot;  }}```              Sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;&quot;,  &quot;body&quot;: {    &quot;name&quot;: &quot;GitNotebookRepo&quot;,    &quot;className&quot;: &quot;org.apache.zeppelin.notebook.repo.GitNotebookRepo&quot;,    &quot;settings&quot;: [      {        &quot;type&quot;: &quot;INPUT&quot;,        &quot;value&quot;: [],        &quot;selected&quot;: &quot;/tmp/notebook/&quot;,        &quot;name&quot;: &quot;Notebook Path&quot;      }    ]  }}```      ",
      "url": " /usage/rest_api/notebook_repository.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin notebook repository REST API information."
    }
    ,



    "/usage/rest_api/zeppelin_server.html": {
      "title": "Apache Zeppelin Server REST API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin Server REST API## OverviewApache Zeppelin provides several REST APIs for interaction and remote activation of zeppelin functionality.All REST APIs are available starting with the following endpoint `http://[zeppelin-server]:[zeppelin-port]/api`. Note that Apache Zeppelin REST APIs receive or return JSON objects, it is recommended for you to install some JSON viewers such as [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc).If you work with Apache Zeppelin and find a need for an additional REST API, please [file an issue or send us an email](http://zeppelin.apache.org/community.html).## Zeppelin Server REST API list### Get Zeppelin version                Description      This ```GET``` method returns Zeppelin version              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/version```              Success code      200              Fail code      500              sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;,  &quot;message&quot;: &quot;Zeppelin version&quot;,  &quot;body&quot;: [    {      &quot;version&quot;: &quot;0.8.0&quot;,      &quot;git-commit-id&quot;: &quot;abc0123&quot;,      &quot;git-timestamp&quot;: &quot;2017-01-02 03:04:05&quot;    }  ]}```      ### Change the log level of Zeppelin Server                 Description      This ```PUT``` method is used to update the root logger&#39;s log level of the server.              URL      ```http://[zeppelin-server]:[zeppelin-port]/api/log/level/```              Success code      200              Fail code      406              sample JSON response      ```json{  &quot;status&quot;: &quot;OK&quot;}```              sample error JSON response      ```json{  &quot;status&quot;:&quot;NOT_ACCEPTABLE&quot;,  &quot;message&quot;:&quot;Please check LOG level specified. Valid values: DEBUG, ERROR, FATAL, INFO, TRACE, WARN&quot;}```      ",
      "url": " /usage/rest_api/zeppelin_server.html",
      "group": "usage/rest_api",
      "excerpt": "This page contains Apache Zeppelin Server REST API information."
    }
    ,



    "/usage/zeppelin_sdk/client_api.html": {
      "title": "Apache Zeppelin SDK - ZeppelinClient API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin SDK - Zeppelin Client API## OverviewZeppelin client api is a lower level java api which encapsulates Zeppelin&#39;s rest api so that you can easily integrate Zeppelinwith your system. You can use zeppelin client api to do most of the things in notebook ui in a programmatic way, such as create/delete note/paragraph, run note/paragraph and etc.## How to use Zeppelin Client APIThe entry point of zeppelin client api is class `ZeppelinClient`. All the operations is via this class, e.g. in the following example, we use `ZeppelinClient` to run the spark tutorial note programmatically.{% highlight java %}ClientConfig clientConfig = new ClientConfig(&quot;http://localhost:8080&quot;);ZeppelinClient zClient = new ZeppelinClient(clientConfig);String zeppelinVersion = zClient.getVersion();System.out.println(&quot;Zeppelin version: &quot; + zeppelinVersion);// execute note 2A94M5J1Z paragraph by paragraphtry {  ParagraphResult paragraphResult = zClient.executeParagraph(&quot;2A94M5J1Z&quot;, &quot;20150210-015259_1403135953&quot;);  System.out.println(&quot;Execute the 1st spark tutorial paragraph, paragraph result: &quot; + paragraphResult);  paragraphResult = zClient.executeParagraph(&quot;2A94M5J1Z&quot;, &quot;20150210-015302_1492795503&quot;);  System.out.println(&quot;Execute the 2nd spark tutorial paragraph, paragraph result: &quot; + paragraphResult);  Map parameters = new HashMap();  parameters.put(&quot;maxAge&quot;, &quot;40&quot;);  paragraphResult = zClient.executeParagraph(&quot;2A94M5J1Z&quot;, &quot;20150212-145404_867439529&quot;, parameters);  System.out.println(&quot;Execute the 3rd spark tutorial paragraph, paragraph result: &quot; + paragraphResult);  parameters = new HashMap();  parameters.put(&quot;marital&quot;, &quot;married&quot;);  paragraphResult = zClient.executeParagraph(&quot;2A94M5J1Z&quot;, &quot;20150213-230422_1600658137&quot;, parameters);  System.out.println(&quot;Execute the 4th spark tutorial paragraph, paragraph result: &quot; + paragraphResult);} finally {  // you need to stop interpreter explicitly if you are running paragraph separately.  zClient.stopInterpreter(&quot;2A94M5J1Z&quot;, &quot;spark&quot;);}{% endhighlight %}Here we list some importance apis of ZeppelinClient, for the completed api, please refer its javadoc.{% highlight java %}public String createNote(String notePath) throws Exception public void deleteNote(String noteId) throws Exception public NoteResult executeNote(String noteId) throws Exception public NoteResult executeNote(String noteId,                               Map parameters) throws Exception                              public NoteResult queryNoteResult(String noteId) throws Exception public NoteResult submitNote(String noteId) throws Exceptionpublic NoteResult submitNote(String noteId,                              Map parameters) throws Exception                              public NoteResult waitUntilNoteFinished(String noteId) throws Exceptionpublic String addParagraph(String noteId,                            String title,                            String text) throws Exception                           public void updateParagraph(String noteId,                             String paragraphId,                             String title,                             String text) throws Exception                            public ParagraphResult executeParagraph(String noteId,                                        String paragraphId,                                        String sessionId,                                        Map parameters) throws Exception                                        public ParagraphResult submitParagraph(String noteId,                                       String paragraphId,                                       String sessionId,                                       Map parameters) throws Exception                                       public void cancelParagraph(String noteId, String paragraphId)    public ParagraphResult queryParagraphResult(String noteId, String paragraphId)     public ParagraphResult waitUtilParagraphFinish(String noteId, String paragraphId){% endhighlight %}## ExamplesFor more detailed usage of zeppelin client api, you can check the examples in module `zeppelin-client-examples`* [ZeppelinClientExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/ZeppelinClientExample.java)* [ZeppelinClientExample2](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/ZeppelinClientExample2.java)",
      "url": " /usage/zeppelin_sdk/client_api.html",
      "group": "usage/zeppelin_sdk",
      "excerpt": "This page contains Apache Zeppelin Client API."
    }
    ,



    "/usage/zeppelin_sdk/session_api.html": {
      "title": "Apache Zeppelin SDK - Session API",
      "content"  : "&lt;!--Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;{% include JB/setup %}# Apache Zeppelin SDK - Session API## OverviewSession api is a high level api for zeppelin. There&#39;s no zeppelin concept (note, paragraph) in this api. The most important thing is a ZSession which represent a running interpreter process.It is pretty to create a ZSession and its api is very straightforward, we can see a concret examples below.## How to use ZSessionIt is very easy to create a ZSession, you need to provide ClientConfig, interpreter and also you can customize your ZSession by specificy its interpreter properties.After you can create ZSession, you need to start it before running any code. ZSession&#39;s lifecycle  is under your control, you need to call stop method expclitly, otherwise the interpreter processs will keep running.{% highlight java %}ZSession session = null;try {  ClientConfig clientConfig = new ClientConfig(&quot;http://localhost:8080&quot;);  Map intpProperties = new HashMap();  intpProperties.put(&quot;spark.master&quot;, &quot;local[*]&quot;);  session = ZSession.builder()          .setClientConfig(clientConfig)          .setInterpreter(&quot;spark&quot;)          .setIntpProperties(intpProperties)          .build();  session.start();  System.out.println(&quot;Spark Web UI: &quot; + session.getWeburl());  // scala (single result)  ExecuteResult result = session.execute(&quot;println(sc.version)&quot;);  System.out.println(&quot;Spark Version: &quot; + result.getResults().get(0).getData());  // scala (multiple result)  result = session.execute(&quot;println(sc.version)n&quot; +          &quot;val df = spark.createDataFrame(Seq((1,&quot;a&quot;), (2,&quot;b&quot;)))n&quot; +          &quot;z.show(df)&quot;);  // The first result is text output  System.out.println(&quot;Result 1: type: &quot; + result.getResults().get(0).getType() +          &quot;, data: &quot; + result.getResults().get(0).getData() );  // The second result is table output  System.out.println(&quot;Result 2: type: &quot; + result.getResults().get(1).getType() +          &quot;, data: &quot; + result.getResults().get(1).getData() );  System.out.println(&quot;Spark Job Urls:n&quot; + StringUtils.join(result.getJobUrls(), &quot;n&quot;));  // error output  result = session.execute(&quot;1/0&quot;);  System.out.println(&quot;Result status: &quot; + result.getStatus() +          &quot;, data: &quot; + result.getResults().get(0).getData());  // pyspark  result = session.execute(&quot;pyspark&quot;, &quot;df = spark.createDataFrame([(1,&#39;a&#39;),(2,&#39;b&#39;)])n&quot; +          &quot;df.registerTempTable(&#39;df&#39;)n&quot; +          &quot;df.show()&quot;);  System.out.println(&quot;PySpark dataframe: &quot; + result.getResults().get(0).getData());  // matplotlib  result = session.execute(&quot;ipyspark&quot;, &quot;%matplotlib inlinen&quot; +          &quot;import matplotlib.pyplot as pltn&quot; +          &quot;plt.plot([1,2,3,4])n&quot; +          &quot;plt.ylabel(&#39;some numbers&#39;)n&quot; +          &quot;plt.show()&quot;);  System.out.println(&quot;Matplotlib result, type: &quot; + result.getResults().get(0).getType() +          &quot;, data: &quot; + result.getResults().get(0).getData());  // sparkr  result = session.execute(&quot;r&quot;, &quot;df &lt;- as.DataFrame(faithful)nhead(df)&quot;);  System.out.println(&quot;Sparkr dataframe: &quot; + result.getResults().get(0).getData());  // spark sql  result = session.execute(&quot;sql&quot;, &quot;select * from df&quot;);  System.out.println(&quot;Spark Sql dataframe: &quot; + result.getResults().get(0).getData());  // spark invalid sql  result = session.execute(&quot;sql&quot;, &quot;select * from unknown_table&quot;);  System.out.println(&quot;Result status: &quot; + result.getStatus() +          &quot;, data: &quot; + result.getResults().get(0).getData());} catch (Exception e) {  e.printStackTrace();} finally {  if (session != null) {    try {      session.stop();    } catch (Exception e) {      e.printStackTrace();    }  }}{% endhighlight %}Here&#39;s a list of apis of `ZSession`.{% highlight java %}public void start() throws Exceptionpublic void start(MessageHandler messageHandler) throws Exceptionpublic void stop() throws Exceptionpublic ExecuteResult execute(String code) throws Exceptionpublic ExecuteResult execute(String subInterpreter,                             Map localProperties,                             String code,                             StatementMessageHandler messageHandler) throws Exception public ExecuteResult submit(String code) throws Exception public ExecuteResult submit(String subInterpreter,                            Map localProperties,                            String code,                            StatementMessageHandler messageHandler) throws Exception                            public void cancel(String statementId) throws Exception public ExecuteResult queryStatement(String statementId) throws Exceptionpublic ExecuteResult waitUntilFinished(String statementId) throws Exception{% endhighlight %}## ExamplesFor more detailed usage of session api, you can check the examples in module `zeppelin-client-examples`* [SparkExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/SparkExample.java)* [SparkAdvancedExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/SparkAdvancedExample.java)* [FlinkExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/FlinkExample.java)* [FlinkAdvancedExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/FlinkAdvancedExample.java)* [FlinkAdvancedExample2](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/FlinkAdvancedExample2.java)* [HiveExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/HiveExample.java)* [PythonExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/PythonExample.java)* [RExample](https://github.com/apache/zeppelin/blob/master/zeppelin-client-examples/src/main/java/org/apache/zeppelin/client/examples/RExample.java)",
      "url": " /usage/zeppelin_sdk/session_api.html",
      "group": "usage/zeppelin_sdk",
      "excerpt": "This page contains Apache Zeppelin SDK - Session API."
    }



}
